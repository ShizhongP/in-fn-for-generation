{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Setting And Data/Model Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "\n",
    "    ## common args\n",
    "    seed = 42\n",
    "    model_name = \"t5-small\"\n",
    "    model_cache_dir = \"./model\"\n",
    "\n",
    "    is_limit_num_of_tran_and_eval_samples = (\n",
    "        True  # if True, You should setting num_train_examples and num_evaluate_examples\n",
    "    )\n",
    "    ## training args\n",
    "    is_train = False\n",
    "    batch_size = 4\n",
    "    num_train_examples = 30000\n",
    "    epoch = 1\n",
    "\n",
    "    max_input_length = 1024\n",
    "    max_target_length = 128\n",
    "    output_dir = \"./result/t5-small-test-summarization\"\n",
    "    # output_dir = f\"./result/t5-small-test-summarization-{num_train_examples}\"\n",
    "\n",
    "    # evaluate args\n",
    "    is_eva = False\n",
    "    num_evaluate_examples = 3000\n",
    "    check_point = f\"result/t5-small-test-summarization/checkpoint-51012\"\n",
    "    # check_point = f\"./result/t5-small-test-summarization-{num_train_examples}/checkpoint-{(num_train_examples+batch_size-1)//batch_size*epoch}\"\n",
    "\n",
    "    ## influence args\n",
    "    damping = 3e-3\n",
    "    lissa_depth = 0.25\n",
    "    lissa_repeat = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psz/.conda/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f42c01d7110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since EdinburghNLP/xsum couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at data/EdinburghNLP___xsum/default/1.2.0/40db7604fedb616a9d2b0673d11838fa5be8451c (last modified on Tue Jan  7 22:02:03 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"EdinburghNLP/xsum\", cache_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.is_limit_num_of_tran_and_eval_samples:\n",
    "    raw_datasets[\"train\"] = (\n",
    "        raw_datasets[\"train\"].shuffle(seed=args.seed).select(range(args.num_train_examples))\n",
    "    )\n",
    "    raw_datasets[\"validation\"] = (\n",
    "        raw_datasets[\"validation\"]\n",
    "        .shuffle(args.seed)\n",
    "        .select(range(args.num_evaluate_examples))\n",
    "    )\n",
    "    raw_datasets[\"test\"] = (\n",
    "        raw_datasets[\"test\"].shuffle(args.seed).select(range(args.num_evaluate_examples))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_646060/1964450314.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(path=\"./metric/rouge.py\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each predictions\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_agregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load metric file\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(path=\"./metric/rouge.py\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Download and Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name, cache_dir=\"./model\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = args.max_input_length\n",
    "max_target_length = args.max_target_length\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"], max_length=max_target_length, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psz/.conda/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21603, 10, 86, 10256, 6, 6098, 7, 33, 1966, 21, 3135, 11, 12162, 53, 2061, 5, 299, 16, 2789, 6, 1363, 411, 7, 12940, 31, 7, 515, 56, 1243, 415, 5779, 56, 18682, 12, 43, 3, 9, 1075, 16, 1260, 1073, 5, 30358, 7, 33, 1461, 11264, 57, 2069, 789, 11, 819, 3081, 43, 72, 4333, 147, 7209, 7, 11, 12, 483, 8, 194, 8, 496, 930, 5, 94, 19, 3, 9, 1516, 606, 16, 8, 2925, 12355, 122, 1433, 13, 2061, 1002, 30, 893, 596, 13, 4395, 9, 31, 7, 12991, 1050, 5, 275, 2199, 8, 22982, 3141, 56, 129, 996, 1723, 12, 1588, 8, 540, 21, 1566, 2061, 12, 4285, 8, 496, 239, 6, 34, 54, 1492, 34, 30, 136, 20, 4571, 162, 26, 1291, 616, 5, 3271, 7, 43, 150, 1390, 12, 1130, 3237, 5, 486, 8, 798, 6, 3, 19585, 5678, 33, 1966, 21, 1898, 496, 716, 11, 79, 174, 6323, 23, 138, 6059, 12, 143, 1516, 1112, 5, 290, 33, 641, 72, 145, 3, 8630, 6980, 3, 9, 6615, 2720, 7, 16, 2789, 11, 165, 4924, 12, 66, 538, 2061, 19, 9909, 12, 8944, 8, 22982, 3141, 31, 7, 11352, 12, 125, 79, 580, 3, 9, 96, 18782, 485, 6, 3452, 825, 121, 21, 2061, 5, 94, 15092, 7, 3213, 24, 4333, 787, 12, 3, 9, 6615, 2720, 7, 54, 199, 1262, 95, 2443, 6, 11, 34, 979, 12, 25990, 18, 2113, 8288, 38, 8, 200, 5505, 496, 358, 16, 8, 1270, 5, 2855, 3271, 3455, 210, 9765, 243, 132, 47, 96, 8461, 385, 2084, 12, 3130, 121, 3, 9, 6615, 2720, 7, 43, 3, 9, 1465, 1113, 16, 2191, 95, 2443, 11, 10256, 133, 59, 36, 826, 8, 825, 5, 96, 634, 304, 2593, 43, 21133, 3986, 13, 4040, 13, 7051, 30, 3, 9, 6615, 2720, 7, 11, 339, 2061, 11, 38, 8, 3, 26767, 804, 23, 2260, 112, 1487, 1390, 12, 3, 7, 8058, 3362, 364, 237, 856, 6, 3, 88, 19, 3, 25345, 135, 12, 3, 26281, 237, 72, 30, 3, 9, 12385, 25654, 5, 96, 1326, 43, 150, 1390, 12, 4277, 8, 16856, 11, 2670, 13, 3, 9, 6615, 2720, 7, 11, 339, 2061, 270, 16, 10256, 535, 14794, 13, 8, 711, 2251, 16, 932, 31, 7, 11993, 4356, 3, 18, 379, 8, 22982, 23053, 7, 3, 18, 43, 243, 79, 241, 12, 4277, 3, 9, 6615, 2720, 7, 16, 10256, 5, 20412, 1626, 189, 1343, 6, 445, 6675, 6400, 51, 52, 76, 31, 7, 1291, 5502, 6, 718, 8, 25990, 1390, 21, 2789, 96, 7, 75, 232, 138, 1162, 535, 5, 96, 7238, 19, 150, 2084, 24, 3, 9, 6615, 2720, 7, 161, 6, 150, 2084, 24, 79, 3033, 2443, 6, 150, 2084, 24, 79, 462, 394, 463, 1073, 11, 150, 2084, 24, 79, 33, 125, 1362, 11, 2597, 241, 976, 3, 88, 243, 5, 96, 22243, 3, 9, 3148, 12, 3452, 1073, 19, 424, 62, 133, 241, 6, 11, 5071, 1672, 6, 66, 2251, 12, 1520, 1669, 12, 16, 70, 6571, 32, 7, 21, 8, 22316, 22982, 4356, 535, 299, 8, 22982, 11, 1566, 2061, 1002, 33, 341, 5229, 57, 3, 9, 4494, 8641, 21, 3081, 31, 726, 11, 1124, 5, 30358, 7, 33, 59, 10422, 12, 175, 726, 2643, 7, 78, 16, 1504, 2875, 31, 7, 7298, 56, 240, 66, 1566, 2061, 91, 13, 8, 358, 11, 3033, 746, 81, 8, 5931, 2020, 13, 46, 2789, 11, 10256, 726, 11, 1124, 1809, 5, 290, 19, 641, 1710, 15290, 21, 8, 20, 24817, 13, 3081, 31, 726, 11, 1124, 5, 3, 19440, 3, 7, 6873, 1950, 6, 8, 22982, 16117, 3141, 19, 230, 23971, 16, 14499, 5, 886, 2119, 7021, 7, 2367, 8560, 250, 13, 2410, 24, 22982, 3081, 133, 414, 95, 271, 1866, 705, 145, 273, 16, 2789, 5, 1363, 1626, 189, 1343, 243, 3081, 130, 4376, 34, 228, 991, 12, 3518, 726, 5, 96, 188, 17, 8, 337, 97, 62, 103, 13, 503, 16236, 24, 8, 962, 13, 726, 19, 641, 2852, 3, 9, 7592, 616, 788, 12, 8, 2841, 1112, 62, 217, 838, 286, 16, 2789, 976, 3, 88, 243, 5, 299, 46, 237, 4038, 1750, 344, 8, 2061, 3283, 30, 893, 596, 13, 8, 4947, 6, 3475, 12, 143, 2450, 8281, 21, 726, 5684, 952, 16, 647, 5, 1], [21603, 10, 3234, 12, 18829, 26068, 56, 36, 5573, 21, 985, 18, 102, 4920, 239, 3500, 338, 37, 5209, 5780, 7, 898, 6996, 895, 45, 1600, 5, 94, 47, 4686, 57, 8, 616, 31, 7, 18176, 12838, 10846, 1483, 11, 7608, 21, 15993, 9145, 6, 11, 3, 9, 1126, 5336, 19, 271, 1702, 21, 8, 10730, 4907, 5, 1626, 354, 52, 9, 107, 22928, 6, 12864, 243, 985, 18, 102, 4920, 2601, 3500, 96, 8894, 36, 224, 3, 9, 600, 199, 1280, 37, 9145, 1888, 268, 1236, 113, 1111, 7, 12, 2384, 15622, 45, 2556, 15, 17, 1483, 3588, 334, 239, 243, 160, 2027, 7, 33, 583, 53, 3996, 2915, 399, 847, 5, 96, 6306, 196, 17, 908, 19, 6865, 46, 16242, 418, 13, 540, 21, 841, 114, 140, 6, 113, 744, 31, 17, 43, 3, 9, 294, 18, 715, 613, 5, 96, 196, 54, 320, 2177, 12, 8, 416, 215, 42, 78, 406, 8, 3516, 13, 149, 231, 540, 27, 183, 2887, 30, 82, 2027, 976, 255, 243, 5, 37, 1154, 47, 4382, 57, 1363, 10846, 1483, 16, 112, 6571, 32, 21, 18176, 16, 1186, 5, 96, 196, 11130, 12, 199, 69, 1021, 151, 129, 30, 16, 280, 6, 11, 48, 19, 8, 166, 1147, 16, 3, 6930, 30, 24, 976, 1363, 10846, 1483, 243, 5, 15993, 9145, 4983, 6043, 7, 3937, 6, 84, 5475, 66, 2601, 688, 16, 8, 616, 6, 56, 4285, 165, 1249, 18, 32, 883, 1016, 5743, 19077, 898, 18, 232, 18, 7248, 4142, 5, 71, 9212, 239, 4142, 12, 1189, 898, 12, 507, 215, 625, 7, 56, 92, 36, 3665, 5, 262, 2825, 11102, 12, 169, 8, 4142, 56, 661, 95, 12, 2664, 1660, 227, 8, 1139, 31, 7, 507, 189, 3591, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[282, 3, 26767, 3080, 411, 7, 12940, 2162, 66, 1566, 538, 2061, 56, 582, 3, 9, 6615, 2720, 7, 6, 8, 22982, 3141, 3256, 12, 15092, 8, 825, 270, 5, 1], [16111, 15, 26, 2601, 3500, 21, 898, 12, 507, 215, 625, 7, 56, 36, 3, 10671, 91, 16, 15993, 9145, 6, 34, 65, 118, 2162, 5, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = args.batch_size\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=args.output_dir,\n",
    "    logging_dir=args.output_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=args.epoch,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22500 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "  2%|â–         | 500/22500 [01:59<1:37:53,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1649, 'grad_norm': 4.244799613952637, 'learning_rate': 1.9558222222222223e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1000/22500 [03:58<1:16:32,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.929, 'grad_norm': 3.5437469482421875, 'learning_rate': 1.911377777777778e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 1500/22500 [05:57<1:20:15,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8983, 'grad_norm': 3.4916257858276367, 'learning_rate': 1.8669333333333334e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 2000/22500 [07:55<1:29:37,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8832, 'grad_norm': 3.9176409244537354, 'learning_rate': 1.822488888888889e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 2500/22500 [09:54<1:11:59,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8741, 'grad_norm': 5.255237102508545, 'learning_rate': 1.7781333333333335e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 3000/22500 [11:53<1:28:11,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.83, 'grad_norm': 3.6619646549224854, 'learning_rate': 1.733688888888889e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 3500/22500 [13:52<1:17:58,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8311, 'grad_norm': 3.1096231937408447, 'learning_rate': 1.6892444444444447e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 4000/22500 [15:49<57:14,  5.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8156, 'grad_norm': 4.834927082061768, 'learning_rate': 1.6448000000000002e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 4500/22500 [17:48<1:08:35,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8241, 'grad_norm': 4.271838188171387, 'learning_rate': 1.6004444444444444e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 5000/22500 [19:49<1:02:31,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8166, 'grad_norm': 6.328220844268799, 'learning_rate': 1.556e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 5500/22500 [21:48<1:05:05,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7787, 'grad_norm': 3.903757333755493, 'learning_rate': 1.5116444444444445e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 6000/22500 [23:45<1:15:04,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.791, 'grad_norm': 2.9944801330566406, 'learning_rate': 1.4672000000000001e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 6500/22500 [25:48<1:13:35,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7827, 'grad_norm': 3.4611856937408447, 'learning_rate': 1.4227555555555557e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 7000/22500 [27:47<1:12:02,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7835, 'grad_norm': 4.128263473510742, 'learning_rate': 1.378311111111111e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7500/22500 [29:48<53:14,  4.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8058, 'grad_norm': 3.7389602661132812, 'learning_rate': 1.3338666666666668e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psz/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                    \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7500/22500 [33:24<53:14,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5254247188568115, 'eval_rouge1': 27.4092, 'eval_rouge2': 7.2048, 'eval_rougeL': 21.463, 'eval_rougeLsum': 21.468, 'eval_gen_len': 18.7957, 'eval_runtime': 215.9428, 'eval_samples_per_second': 13.893, 'eval_steps_per_second': 3.473, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 8000/22500 [35:25<1:02:42,  3.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7493, 'grad_norm': 3.330371141433716, 'learning_rate': 1.2894222222222224e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 8500/22500 [37:28<1:06:38,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7557, 'grad_norm': 3.736124038696289, 'learning_rate': 1.2449777777777778e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9000/22500 [39:28<52:28,  4.29it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7355, 'grad_norm': 3.215278387069702, 'learning_rate': 1.2005333333333333e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 9500/22500 [41:27<54:11,  4.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7344, 'grad_norm': 5.4099907875061035, 'learning_rate': 1.1561777777777779e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 10000/22500 [43:26<56:38,  3.68it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7492, 'grad_norm': 6.402969837188721, 'learning_rate': 1.1117333333333333e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 10500/22500 [45:25<51:51,  3.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7221, 'grad_norm': 3.665576934814453, 'learning_rate': 1.067288888888889e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 11000/22500 [47:25<42:55,  4.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7397, 'grad_norm': 2.9963107109069824, 'learning_rate': 1.0228444444444446e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11500/22500 [49:29<40:01,  4.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7295, 'grad_norm': 4.257570266723633, 'learning_rate': 9.78488888888889e-06, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 12000/22500 [51:29<38:57,  4.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7338, 'grad_norm': 3.348665237426758, 'learning_rate': 9.340444444444445e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 12500/22500 [53:29<36:58,  4.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7494, 'grad_norm': 4.473400115966797, 'learning_rate': 8.896000000000001e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13000/22500 [55:28<27:04,  5.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7647, 'grad_norm': 6.455132007598877, 'learning_rate': 8.451555555555557e-06, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 13500/22500 [57:29<27:36,  5.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7294, 'grad_norm': 3.945760488510132, 'learning_rate': 8.007111111111112e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14000/22500 [59:30<38:02,  3.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.721, 'grad_norm': 3.464590311050415, 'learning_rate': 7.563555555555556e-06, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14500/22500 [1:01:30<34:34,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7211, 'grad_norm': 3.765570640563965, 'learning_rate': 7.1191111111111124e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15000/22500 [1:03:30<28:06,  4.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6831, 'grad_norm': 3.893583059310913, 'learning_rate': 6.674666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psz/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 15001/22500 [1:07:02<132:34:21, 63.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4939675331115723, 'eval_rouge1': 28.1305, 'eval_rouge2': 7.5866, 'eval_rougeL': 22.0035, 'eval_rougeLsum': 22.0039, 'eval_gen_len': 18.8123, 'eval_runtime': 210.6905, 'eval_samples_per_second': 14.239, 'eval_steps_per_second': 3.56, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 15500/22500 [1:09:01<27:53,  4.18it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6889, 'grad_norm': 3.740978717803955, 'learning_rate': 6.230222222222223e-06, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16000/22500 [1:11:00<24:18,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7198, 'grad_norm': 3.1007449626922607, 'learning_rate': 5.785777777777778e-06, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 16500/22500 [1:13:00<21:12,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7154, 'grad_norm': 4.052583694458008, 'learning_rate': 5.342222222222223e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17000/22500 [1:14:59<23:42,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6686, 'grad_norm': 3.2006192207336426, 'learning_rate': 4.897777777777778e-06, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 17500/22500 [1:17:01<19:29,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6971, 'grad_norm': 3.740164279937744, 'learning_rate': 4.453333333333334e-06, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 18000/22500 [1:19:01<18:51,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6882, 'grad_norm': 3.7211720943450928, 'learning_rate': 4.008888888888889e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 18500/22500 [1:21:01<14:14,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7057, 'grad_norm': 4.675158500671387, 'learning_rate': 3.564444444444445e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19000/22500 [1:22:59<15:04,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6978, 'grad_norm': 4.1983561515808105, 'learning_rate': 3.12e-06, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 19500/22500 [1:24:59<12:21,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7346, 'grad_norm': 3.343820333480835, 'learning_rate': 2.675555555555556e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 20000/22500 [1:27:00<10:57,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6643, 'grad_norm': 3.5886309146881104, 'learning_rate': 2.2320000000000004e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 20500/22500 [1:28:56<09:31,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7, 'grad_norm': 3.9172167778015137, 'learning_rate': 1.7875555555555556e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 21000/22500 [1:30:56<05:21,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6812, 'grad_norm': 3.8187644481658936, 'learning_rate': 1.343111111111111e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 21500/22500 [1:32:55<03:17,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7033, 'grad_norm': 3.627049207687378, 'learning_rate': 8.986666666666667e-07, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 22000/22500 [1:34:53<01:51,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7053, 'grad_norm': 3.444369316101074, 'learning_rate': 4.542222222222223e-07, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22500/22500 [1:36:51<00:00,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7083, 'grad_norm': 3.530238389968872, 'learning_rate': 1.0666666666666668e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psz/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                       \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22500/22500 [1:40:21<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4836490154266357, 'eval_rouge1': 28.2465, 'eval_rouge2': 7.6712, 'eval_rougeL': 22.1983, 'eval_rougeLsum': 22.197, 'eval_gen_len': 18.8313, 'eval_runtime': 209.0429, 'eval_samples_per_second': 14.351, 'eval_steps_per_second': 3.588, 'epoch': 3.0}\n",
      "{'train_runtime': 6021.3342, 'train_samples_per_second': 14.947, 'train_steps_per_second': 3.737, 'train_loss': 2.7623339409722223, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# resume_from_checkpoint = \"result/t5-small-test-summarization-50000/checkpoint-2000\"\n",
    "resume_from_checkpoint = None\n",
    "if args.is_train:\n",
    "    if resume_from_checkpoint:\n",
    "        trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    else :\n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17439/363907053.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/psz/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [03:27<00:00,  3.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.5135397911071777,\n",
       " 'eval_model_preparation_time': 0.0018,\n",
       " 'eval_rouge1': 28.123,\n",
       " 'eval_rouge2': 7.7164,\n",
       " 'eval_rougeL': 22.0584,\n",
       " 'eval_rougeLsum': 22.0585,\n",
       " 'eval_gen_len': 18.8183,\n",
       " 'eval_runtime': 207.9819,\n",
       " 'eval_samples_per_second': 14.424,\n",
       " 'eval_steps_per_second': 3.606}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.is_eval:\n",
    "    batch_size = args.batch_size\n",
    "    train_args = Seq2SeqTrainingArguments(\n",
    "        args.check_point,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        train_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(args.check_point)\n",
    "    trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(args.check_point)\n",
    "# shuffle the test dataset and select 10 examples\n",
    "tokenized_datasets[\"test\"].shuffle(seed=args.seed)\n",
    "influence_fn_examples = tokenized_datasets[\"test\"].select(range(10))\n",
    "\n",
    "input_documents = influence_fn_examples[\"document\"]\n",
    "target_documents = influence_fn_examples[\"summary\"]\n",
    "model_inputs = tokenizer(\n",
    "    input_documents,\n",
    "    max_length=max_input_length,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "model_inputs = model_inputs.to(model.device)\n",
    "##  Generate Summary\n",
    "outputs = model.generate(**model_inputs, max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: 0\n",
      "Input Document: Sarah Johnson was one of 21 women heading to Liverpool when their minibus was hit by a lorry on the M62.\n",
      "Her friend Bethany Jones, 18, was killed while Ms Johnson and several others were badly hurt.\n",
      "Minibus driver James Johnson was jailed for more than six years for causing Bethany's death, in April 2013.\n",
      "Ms Johnson, who broke her shoulder, back and pelvis, said the help she received from a charity while in hospital led her to want to support others.\n",
      "Speaking publicly for the first time about the crash, Ms Johnson described how everyone was \"excited and giddy\" for the hen party.\n",
      "\"To me the impact was just a massive explosion,\" she said.  \"I thought the bus had blown up.\n",
      "\"I remember the bus dropping on its side. The next thing, I woke up on the roadside so I'd actually come out of the window.\"\n",
      "Ms Johnson was taken to Leeds General Infirmary where she, along with Bethany's sister Amy Firth, underwent major surgery and spent time in intensive care.\n",
      "Whilst she was there she got support from charity Day One, which helps victims of major trauma.\n",
      "She said: \"It's absolutely fantastic.\n",
      "\"It supports people by giving benefit advice, legal advice and peer support such as me and Amy, who have been in similar situations and who are now helping other people who've suffered from major trauma.\"\n",
      "Ms Johnson said the crash had made her realise how lucky she had been.\n",
      "\"Beth can't complain, she's not here,\" she added.  \"We just have to be grateful for what we've got.\"\n",
      "Generated Summary: Sarah Johnson, who was one of 21 women heading to Liverpool when their minibus was hit by a lorry on the M62, said she was lucky to be in hospital.\n",
      "Target Summary: A woman who was seriously hurt in a fatal hen party motorway crash is now helping other major trauma victims rebuild their lives.\n",
      "==================================================\n",
      "Example: 1\n",
      "Input Document: A total of 1,400 tickets have sold out for the opening weekend at Bramall Hall in Stockport, Greater Manchester after renovation work began in 2014.\n",
      "Stained glass windows and ceilings have been restored, while the public will be able to visit the dining room and butler's pantry for the first time.\n",
      "Councillor Kate Butler, from Stockport Council, called it the \"jewel in the crown\" of the town's heritage.\n",
      "The manor dates back to the reign of William the Conqueror when he bestowed the lands upon one of his followers, Hamon de Masci, who became the first Baron of Dunham Massey.\n",
      "Since then the estate has been under the ownership of just three families: the Davenports, De Bromales and Nevills.\n",
      "In 1936, the timber-framed hall and its surrounding parkland was handed over to the local council.\n",
      "The transformation followed a Â£1.6m grant from the Heritage Lottery Fund and Â£400,000 in council funding.\n",
      "Generated Summary: The opening weekend of the Bramall Hall in Stockport, Greater Manchester, has sold out for the first time.\n",
      "Target Summary: A Tudor manor house has reopened following a Â£2.2m makeover.\n",
      "==================================================\n",
      "Example: 2\n",
      "Input Document: Two-year-old Lane Thomas Graves had been playing in the sand near the resort's Seven Seas Lagoon when he was dragged underwater by the creature.\n",
      "His parents and older sister had been visiting the Grand Floridian resort in June 2016 from the state of Nebraska.\n",
      "The lighthouse has been installed near to where the attack occurred.\n",
      "Wildlife officials classified the killing as a predatory attack, saying the boy did nothing to provoke the alligator.\n",
      "\"He was in the water not more than ankle deep,\" the Florida Fish and Wildlife Conservation Commission said in a report, describing how the boy had been gathering water for a sandcastle.\n",
      "His father, Matt Graves, jumped in the water to try to pry open the creature's mouth, but \"the alligator thrashed and broke Matt's grasp and went under the water,\" according to the report.\n",
      "A Disney spokesperson said they hoped the monument would spread awareness for the Lane Thomas Foundation, which also uses the lighthouse as its logo.\n",
      "Who is liable for alligator boy's death?\n",
      "\"The lighthouse sculpture has been installed to help spread awareness of the Lane Thomas Foundation, which was established to provide assistance and support to families whose children need organ transplants,\" Walt Disney World said in a statement.\n",
      "After the death, Disney was criticised for not having posted signs warning of the danger along the man-made lagoon, which borders Magic Kingdom.\n",
      "Public notices have now been added to the area, Florida media report.\n",
      "The Lane family announced a month after the boy's death that they would not sue Disney, and would instead \"solely be focused on the future health of our family\".\n",
      "Generated Summary: Disney has installed a lighthouse sculpture to help spread awareness of the Lane Thomas Foundation, which has been used as its logo.\n",
      "Target Summary: Walt Disney World has unveiled a lighthouse memorial for a young boy who was killed by an alligator while on holiday at the Florida theme park.\n",
      "==================================================\n",
      "Example: 3\n",
      "Input Document: Michelle O'Neill hosted a departmental breakfast at the Balmoral Show on Thursday morning.\n",
      "In attendance was the Chinese consul general Madam Wang Shuying.\n",
      "Chinese inspectors visited Northern Ireland pork plants in April as part of a process to approve local pork for export.\n",
      "Ms O'Neill said she was also hoping to agree access to Chinese markets for beef and chicken.\n",
      "US consul general Greg Burton was also at the event.\n",
      "Northern Ireland is hoping to sell beef into the United States.\n",
      "Ms O'Neill said she was also working with Irish agriculture minister Simon Coveney to resolve labelling issues that have hit Northern Ireland lamb sales to meat plants in the Republic of Ireland.\n",
      "She told guests that the quality of Northern Ireland food was \"something that carries our reputation around the world.\"\n",
      "She said it was \"safe, traceable and sustainably produced\" and she would seek to use that reputation to open doors.\n",
      "It is the second day of the Balmoral Show, the biggest showcase for Northern Ireland's farming and food industries.\n",
      "Generated Summary: The second day of the Balmoral Show, the biggest showcase for Northern Ireland's farming and food industries, is the Balmoral Show.\n",
      "Target Summary: The agriculture minister is to return to China next month as attempts to access markets there continue.\n",
      "==================================================\n",
      "Example: 4\n",
      "Input Document: Allsop, 25, made his Premier League debut in November 2015 and has spent time on loan at Coventry and Wycombe.\n",
      "Cooke, 20, won the European Championship with England Under-17s in 2014 and scored four goals while on loan at Crewe last season.\n",
      "League One Blackpool have now signed nine players this summer.\n",
      "Find all the latest football transfers on our dedicated page.\n",
      "Generated Summary: Blackpool have signed Blackpool and Coventry Allsop on a two-year deal.\n",
      "Target Summary: Blackpool have signed Bournemouth goalkeeper Ryan Allsop and midfielder Callum Cooke from Middlesbrough on season-long loans.\n",
      "==================================================\n",
      "Example: 5\n",
      "Input Document: Media playback is not supported on this device\n",
      "The Scotland head coach's side sit second bottom of Group F ahead of the game at Hampden Park on 26 March.\n",
      "\"We must win that game,\" he said. \"There is no getting away from that.\"\n",
      "But Strachan did not wish to consider what a defeat would mean for his own future and said: \"Whatever happens after that, we will look at that.\"\n",
      "The 60-year-old, who has been in charge since 2013, signed a new two-year contract in October 2015.\n",
      "Following successive 3-0 defeats away to Slovakia and England, he took time to consider his position but decided in November to carry on after receiving the backing of the Scottish Football Association board.\n",
      "As he announced his squad for the 22 March warm-up friendly against Canada, followed by the qualifier against Slovenia, Strachan underlined his belief that the Scots could still reach the 2018 finals in Russia.\n",
      "\"If we win that game, we will only be one point behind Slovenia, who are in second at the moment,\" he said.\n",
      "Strachan, whose side lie fifth with four points from four matches, insisted he had \"no idea\" how many points they might require from their final six matches to qualify, while admitting that the Scots could not afford further setbacks.\n",
      "\"But we are looking forward to that challenge and I think the fans will be looking forward to that challenge of going to a game that you must win,\" he said.\n",
      "\"So, together, we can make it a right good night for us.\n",
      "\"The fans have always given us support, but we really need it on that night.\n",
      "\"The guys will be prepared, the fans will be prepared and we are looking to put in a right good performance.\"\n",
      "Strachan will again field questions about his future should Scotland fall short.\n",
      "\"I think fans, players, anybody who really wants us to do well, will be wishing us luck and dealing with anything after the game,\" he added.\n",
      "\"Everyone who wants to see us go to the World Cup will be backing us to win this game.\n",
      "\"The most important thing is three points and enjoying the build-up, which we will be doing.\n",
      "\"The best part of my job is coaching and I am looking forward to that and then looking forward to the challenge of the game.\"\n",
      "Generated Summary: Scotland head coach Mark Strachan has said he is looking forward to a \"right good night\" if he is to win the World Cup.\n",
      "Target Summary: Scotland must beat Slovenia if they are to resurrect their chances of reaching the World Cup finals, Gordon Strachan has acknowledged.\n",
      "==================================================\n",
      "Example: 6\n",
      "Input Document: Clive Weatherhogg's counsel told appeal judges that prosecutors now conceded there had been a miscarriage of justice over the coercion charge.\n",
      "Appeal judges also agreed that a jury was misdirected over charges relating to the footage sent to relatives.\n",
      "Mr Weatherhogg, 43, was jailed last May after being found guilty by a jury.\n",
      "It is understood he will now be released from prison.\n",
      "Sheriff George Way had remitted the case from Dundee Sheriff Court to the High Court in Edinburgh to be dealt with because of its greater sentencing powers.\n",
      "Defence counsel Shelagh McCall QC told the Court of Criminal Appeal in Edinburgh that the Crown conceded there was insufficient evidence to prove lack of consent on the coercion charge.\n",
      "The Lord Justice Clerk, Lady Dorrian, who heard the appeal with Lady Paton and Lord Turnbull, said: \"We are satisfied the concession is well made.\"\n",
      "Lord Turnbull said that information conveyed to the court seemed to show that even at an earlier preparatory marking stage in the case there were concerns over a lack of corroboration.\n",
      "Lady Dorrian said the appeal judges were also satisfied that misdirections given to jurors by Sheriff Way over the charges relating to the footage sent to relatives were \"material and constituted a miscarriage of justice\".\n",
      "Mr Weatherhogg, of Guthrie, near Forfar, Angus, had denied the charges during the trial, but admitted a separate charge of uploading the film to the internet.\n",
      "He was jailed for four-and-years by Lady Wolffe with an additional 18 months for the charge he admitted.\n",
      "The appeal hearing was told that Mr Weatherhogg had been in custody since March last year and had served the 18 month sentence, taking into account early release provisions.\n",
      "Generated Summary: A man jailed for four-and-a-half years for uploading a film to the internet has been released from prison.\n",
      "Target Summary: A man jailed for six years for coercing a woman into having sex with a stranger and sending \"revenge porn\" to her family has had the convictions quashed.\n",
      "==================================================\n",
      "Example: 7\n",
      "Input Document: John Morley, 29, of Deanstown, Finglas West, is alleged to have carried out a sustained weekend attack at the Chimney Corner Hotel in Mallusk.\n",
      "The victim, Derek White, was said to have been stabbed in the chest and leg.\n",
      "Mr Morley, is charged with the attempted wounding of Mr White and possession of an offensive weapon with intent to commit an offence.\n",
      "He is further accused of disorderly behaviour and possession of Class B drugs, namely cannabis.\n",
      "He will not be released from custody until all conditions are met - including the lodgement of a Ã‚Â£2,000 cash surety.\n",
      "Police detained him after being called to the hotel just outside north Belfast in the early hours of Sunday.\n",
      "Paramedics were already on the scene treating Mr White, who is believed to be aged in his 30s.\n",
      "At the defendant's first appearance before Belfast Magistrates' Court it was alleged that he chased the victim through the hotel with a flick knife.\n",
      "A police officer revealed the two men had travelled to Northern Ireland with their children for a football match.\n",
      "Generated Summary: A man accused of stabbing a man in the chest and leg at a hotel in north Belfast has been released from custody.\n",
      "Target Summary: A Dublin man accused of stabbing his friend in a County Antrim hotel has been granted bail.\n",
      "==================================================\n",
      "Example: 8\n",
      "Input Document: Aiton, 30, had scans to diagnose the injury on Monday.\n",
      "He joined the Dragons from Leeds at the end of 2015, and missed the end of the last campaign with an arm injury.\n",
      "The Papua New Guinea player played 94 games for both Wakefield and Leeds following his move from Cronulla-Sutherland Sharks in 2012.\n",
      "Generated Summary: Wakefield and Leeds Dragons player Aiton Aiton has had scans to diagnose a wrist injury.\n",
      "Target Summary: Catalans Dragons will be without hooker Paul Aiton for up to six weeks after he tore a pectoral muscle on his debut, in a defeat by Wigan Warriors on Friday.\n",
      "==================================================\n",
      "Example: 9\n",
      "Input Document: The Sky Blues currently play in Coventry's Ricoh Arena but had a long dispute with the stadium's previous owners.\n",
      "The council said the club wanted to meet to understand how it would deal with a planning application.\n",
      "The club said it was not commenting \"at this stage\".\n",
      "The club's owners, Sisu, have been involved in a long-running stand off with the previous operators of the Ricoh that saw the Sky Blues play the 2013/14 season in Northampton.\n",
      "The arena is now owned by Premiership rugby team Wasps.\n",
      "In a statement, Rugby Borough Council said its leader and the council's executive director and head of planning had met with Coventry City in March.\n",
      "\"The club requested the meeting to understand how the council would deal with any planning application for potential stadium sites in the borough of Rugby,\" it said.\n",
      "It said the plans would need to be finalised by September to be included in the council's local plan, but added that a site had yet to be identified.\n",
      "Peter Ward, from Sky Blues Supporters' Consultative Group, said he was pleased to hear that things were \"moving\" with the club's search for a new home.\n",
      "\"It's good that finally there is some evidence things are happening,\" he said.\n",
      "\"As we've heard from the club's directors, there is a need to take control of and have access to all the revenue streams.\"\n",
      "However, he said that fans could find a move out of Coventry difficult - particularly as they only moved from their previous home, Highfield Road, to the Ricoh in 2005.\n",
      "\"The move from Highfield Road to the Ricoh was a big emotional move for the fans,\" he said. \"This would be another big change.\"\n",
      "Generated Summary: Rugby Borough Council has met with Coventry City to discuss plans for potential stadium sites in the borough of Rugby.\n",
      "Target Summary: Planners in Rugby have revealed they have been in talks with Coventry City Football Club about building a stadium in the borough.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, input_documents in enumerate(input_documents):\n",
    "    print(\"Example:\", i)\n",
    "    print(\"Input Document:\", input_documents)\n",
    "    print(\"Generated Summary:\", tokenizer.decode(outputs[i], skip_special_tokens=True))\n",
    "    print(\"Target Summary:\", target_documents[i])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psz/.conda/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      " 20%|â–ˆâ–ˆ        | 2/10 [00:00<00:00, 15.23it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  7.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.421166181564331,\n",
       " 'eval_model_preparation_time': 0.0022,\n",
       " 'eval_rouge1': 27.1753,\n",
       " 'eval_rouge2': 5.412,\n",
       " 'eval_rougeL': 20.8743,\n",
       " 'eval_rougeLsum': 20.8016,\n",
       " 'eval_gen_len': 19.0,\n",
       " 'eval_runtime': 1.5798,\n",
       " 'eval_samples_per_second': 6.33,\n",
       " 'eval_steps_per_second': 6.33}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "    args.check_point,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=influence_fn_examples,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Influence Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the code implementation of this paper on [Influence Funciton](ACL2020 Explaining black box predictions and unveiling data artifacts through influence functions):\n",
    "\n",
    "1. è®¡ç®—æµ‹è¯•æ ·æœ¬çš„æ¢¯åº¦ï¼ˆL_TEST GRADIENTï¼‰ï¼š\n",
    "è¿™éƒ¨åˆ†ä»£ç é¦–å…ˆè®¡ç®—æµ‹è¯•æ ·æœ¬ç›¸å¯¹äºŽæ¨¡åž‹å‚æ•°çš„æ¢¯åº¦ã€‚è¿™æ˜¯é€šè¿‡åœ¨æµ‹è¯•æ ·æœ¬ä¸Šè¿è¡Œå‰å‘ä¼ æ’­ï¼Œç„¶åŽè®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºŽæ¨¡åž‹å‚æ•°çš„æ¢¯åº¦æ¥å®žçŽ°çš„ã€‚\n",
    "```python\n",
    "######## L_TEST GRADIENT ########\n",
    "model.zero_grad()\n",
    "test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "test_grads = autograd.grad(test_loss, param_influence)\n",
    "################\n",
    "````\n",
    "\n",
    "\n",
    "2. è®¡ç®—é€†Hessianå‘é‡ç§¯ï¼ˆIHVPï¼‰ï¼š\n",
    "è¿™éƒ¨åˆ†ä»£ç ä½¿ç”¨ Lissa ç®—æ³•ï¼ˆé€šè¿‡ get_inverse_hvp_lissa å‡½æ•°ï¼‰æ¥è¿‘ä¼¼è®¡ç®—é€†HessiançŸ©é˜µä¸Žæµ‹è¯•æ¢¯åº¦çš„ä¹˜ç§¯ã€‚è¿™æ˜¯ influence score è®¡ç®—çš„æ ¸å¿ƒæ­¥éª¤ï¼Œå› ä¸ºå®ƒå…è®¸æˆ‘ä»¬ä¼°è®¡å¦‚æžœè®­ç»ƒæ•°æ®ä¸­ç§»é™¤æˆ–ç¨å¾®ä¿®æ”¹æŸä¸ªæ ·æœ¬ï¼Œæ¨¡åž‹å‚æ•°å°†å¦‚ä½•å˜åŒ–ã€‚\n",
    "```python\n",
    "######## IHVP ########\n",
    "model.train()\n",
    "logger.info(\"######## START COMPUTING IHVP ########\")\n",
    "inverse_hvp = get_inverse_hvp_lissa(test_grads, model, device, param_influence, train_dataloader_lissa, damping=damping, num_samples=args.lissa_repeat, recursion_depth=int(len(train_examples)*args.lissa_depth))\n",
    "logger.info(\"######## FINISHED COMPUTING IHVP ########\")\n",
    "################\n",
    "```\n",
    "3. è®¡ç®—è®­ç»ƒæ ·æœ¬çš„å½±å“åŠ›ï¼ˆINFLUENCEï¼‰ï¼š\n",
    "è¿™éƒ¨åˆ†ä»£ç éåŽ†è®­ç»ƒæ•°æ®é›†ï¼Œå¯¹äºŽæ¯ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè®¡ç®—å…¶æ¢¯åº¦ï¼Œå¹¶ä½¿ç”¨ä¹‹å‰è®¡ç®—çš„é€†Hessianå‘é‡ç§¯æ¥ä¼°è®¡è¯¥æ ·æœ¬å¯¹æµ‹è¯•æ ·æœ¬é¢„æµ‹çš„å½±å“åŠ›ã€‚è¿™æ˜¯é€šè¿‡è®¡ç®—è®­ç»ƒæ ·æœ¬æ¢¯åº¦å’Œé€†Hessianå‘é‡ç§¯çš„ç‚¹ç§¯æ¥å®žçŽ°çš„ã€‚\n",
    "```python\n",
    "######## INFLUENCE ########\n",
    "influences = np.zeros(len(train_dataloader.dataset))\n",
    "for train_idx, (_input_ids, _input_mask, _segment_ids, _label_ids, _) in enumerate(tqdm(train_dataloader, desc=\"Train set index\")):\n",
    "    model.zero_grad()\n",
    "    train_loss = model(_input_ids, _segment_ids, _input_mask, _label_ids)\n",
    "    train_grads = autograd.grad(train_loss, param_influence)\n",
    "    influences[train_idx] = torch.dot(inverse_hvp, gather_flat_grad(train_grads)).item()\n",
    "################\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "encoder.block.0.layer.0.layer_norm.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.0.layer.1.layer_norm.weight\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "encoder.block.1.layer.0.layer_norm.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.1.layer.1.layer_norm.weight\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "encoder.block.2.layer.0.layer_norm.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.2.layer.1.layer_norm.weight\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "encoder.block.3.layer.0.layer_norm.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.3.layer.1.layer_norm.weight\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "encoder.block.4.layer.0.layer_norm.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.4.layer.1.layer_norm.weight\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "encoder.block.5.layer.0.layer_norm.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.5.layer.1.layer_norm.weight\n",
      "encoder.final_layer_norm.weight\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "decoder.block.0.layer.0.layer_norm.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.0.layer.1.layer_norm.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.0.layer.2.layer_norm.weight\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "decoder.block.1.layer.0.layer_norm.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.1.layer.1.layer_norm.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.1.layer.2.layer_norm.weight\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "decoder.block.2.layer.0.layer_norm.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.2.layer.1.layer_norm.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.2.layer.2.layer_norm.weight\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "decoder.block.3.layer.0.layer_norm.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.3.layer.1.layer_norm.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.3.layer.2.layer_norm.weight\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "decoder.block.4.layer.0.layer_norm.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.4.layer.1.layer_norm.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.4.layer.2.layer_norm.weight\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "decoder.block.5.layer.0.layer_norm.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.5.layer.1.layer_norm.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.5.layer.2.layer_norm.weight\n",
      "decoder.final_layer_norm.weight\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "for n, p in param_optimizer:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0135, -0.0746,  0.0004,  ...,  0.0210, -0.0337, -0.1187],\n",
       "         [-0.0306, -0.0240,  0.0503,  ..., -0.0444, -0.0494,  0.0637],\n",
       "         [ 0.0539, -0.0559,  0.0016,  ..., -0.0335, -0.0202, -0.0460],\n",
       "         ...,\n",
       "         [-0.0888,  0.0146, -0.1137,  ..., -0.0082, -0.0970,  0.0397],\n",
       "         [ 0.0002,  0.0165, -0.0426,  ..., -0.0972,  0.0247, -0.0032],\n",
       "         [-0.1095, -0.0024, -0.0035,  ...,  0.0542, -0.0421, -0.0520]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0696,  0.0976, -0.0691,  ..., -0.6685, -0.0591,  0.4304],\n",
       "         [ 0.0281,  0.0197, -0.2820,  ..., -0.6439,  0.0988,  0.0655],\n",
       "         [ 0.5398, -0.0160,  0.0962,  ..., -0.1355, -0.0318,  0.3481],\n",
       "         ...,\n",
       "         [ 0.0089,  0.2795,  0.0293,  ..., -0.1555,  0.4314, -0.1776],\n",
       "         [-0.3142, -0.0810, -0.1309,  ...,  0.1979,  0.3735, -0.4425],\n",
       "         [ 0.1962,  0.4015, -0.1885,  ..., -0.5950,  0.1754,  0.4051]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0199,  0.3905, -0.8143,  ...,  0.3506, -0.4459,  0.1500],\n",
       "         [-0.1380,  0.3248,  0.4421,  ...,  0.1107,  0.5369, -0.4156],\n",
       "         [-0.6492,  0.0948, -0.2353,  ..., -0.1134, -0.5793, -0.1099],\n",
       "         ...,\n",
       "         [ 0.2814, -0.0934,  0.5961,  ...,  0.1737, -0.1011,  0.0990],\n",
       "         [ 0.0402, -0.4613, -0.3344,  ...,  0.7520,  0.0190,  0.0441],\n",
       "         [-0.2180, -0.0212, -0.0270,  ..., -0.8183, -0.3843, -0.3745]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.8284,  0.1553, -0.9104,  ...,  0.5920, -0.3631,  0.2677],\n",
       "         [ 0.0337,  0.5223,  0.8258,  ..., -0.0516, -0.1518,  0.5644],\n",
       "         [-0.2180, -0.9626, -0.5867,  ...,  0.6237,  0.0190,  0.4365],\n",
       "         ...,\n",
       "         [ 0.4279, -0.8841,  0.4676,  ...,  0.1359,  0.3692, -0.6737],\n",
       "         [-0.2515, -0.8787, -0.5914,  ...,  0.9836,  1.0495, -0.1500],\n",
       "         [ 1.1546, -0.8952,  0.0223,  ..., -0.4222,  0.2428, -0.5945]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.5004e+00,  2.7461e-02, -1.0814e+01, -2.6687e-01, -6.8572e+00,\n",
       "          -3.7158e-02, -3.9676e+00,  8.5796e-01],\n",
       "         [ 5.4293e+00,  1.3166e+00,  1.1100e-01, -5.4371e-02,  2.8823e+00,\n",
       "           6.1208e+00, -3.1626e+00, -3.6741e+00],\n",
       "         [ 4.1235e+00,  1.0088e+00,  9.4434e-01,  5.3938e-01,  2.9096e+00,\n",
       "           3.2570e+00, -2.8108e+00, -3.6078e+00],\n",
       "         [ 3.2596e+00,  7.8139e-01,  1.2712e+00,  7.7051e-01,  2.7207e+00,\n",
       "           1.7799e+00, -2.6846e+00, -3.5840e+00],\n",
       "         [ 2.5866e+00,  6.1026e-01,  1.3623e+00,  8.5450e-01,  2.5485e+00,\n",
       "           7.5542e-01, -2.7641e+00, -3.5599e+00],\n",
       "         [ 2.0480e+00,  4.5539e-01,  1.4521e+00,  8.8357e-01,  2.4080e+00,\n",
       "           9.0103e-02, -2.7831e+00, -3.6835e+00],\n",
       "         [ 1.6033e+00,  4.0511e-01,  1.4901e+00,  8.4323e-01,  2.2390e+00,\n",
       "          -3.3963e-01, -2.7793e+00, -3.7003e+00],\n",
       "         [ 1.2209e+00,  3.4374e-01,  1.5356e+00,  8.2636e-01,  2.1108e+00,\n",
       "          -5.9986e-01, -2.8116e+00, -3.7267e+00],\n",
       "         [ 4.2210e-01,  1.8400e-01,  1.5564e+00,  8.7226e-01,  1.8048e+00,\n",
       "          -8.8328e-01, -2.9497e+00, -3.7916e+00],\n",
       "         [-6.1105e-01,  8.5245e-02,  1.5884e+00,  8.6943e-01,  1.3584e+00,\n",
       "          -1.1016e+00, -3.0560e+00, -3.9445e+00],\n",
       "         [-1.6146e+00, -2.7929e-02,  1.5869e+00,  8.4493e-01,  9.1820e-01,\n",
       "          -1.2683e+00, -3.2522e+00, -4.0286e+00],\n",
       "         [-2.7687e+00, -1.5325e-01,  1.5862e+00,  8.5451e-01,  3.7087e-01,\n",
       "          -1.3769e+00, -3.4349e+00, -4.1246e+00],\n",
       "         [-3.8536e+00, -3.4015e-01,  1.4839e+00,  8.9036e-01, -2.3494e-01,\n",
       "          -1.5413e+00, -3.5507e+00, -4.2168e+00],\n",
       "         [-5.1556e+00, -5.2488e-01,  1.4207e+00,  9.0965e-01, -9.2546e-01,\n",
       "          -1.6351e+00, -3.7842e+00, -4.3388e+00],\n",
       "         [-7.3061e+00, -7.4150e-01,  1.3143e+00,  9.2488e-01, -1.7578e+00,\n",
       "          -1.8631e+00, -4.0241e+00, -4.5299e+00],\n",
       "         [-9.9397e+00, -1.2724e+00,  1.1297e+00,  1.0530e+00, -4.9061e+00,\n",
       "          -2.3913e+00, -4.3812e+00, -5.1187e+00],\n",
       "         [ 2.1094e-01, -3.7500e-01, -6.6406e-02,  1.7578e-01,  2.2266e-01,\n",
       "          -3.5156e-01,  5.8594e-03, -2.3438e-02],\n",
       "         [-4.9985e+00,  4.4546e+00, -1.4421e+00,  5.7766e-01, -4.9991e+00,\n",
       "          -5.9167e-01,  3.6284e+00,  5.5594e+00],\n",
       "         [-4.7500e+00,  2.8763e+00,  6.5756e-01,  8.0103e-01, -4.2189e+00,\n",
       "          -6.7960e-01,  3.6784e+00,  3.8896e+00],\n",
       "         [-4.6835e+00,  2.1201e+00,  1.0658e+00,  8.5980e-01, -4.1216e+00,\n",
       "          -5.8672e-01,  3.4426e+00,  2.9186e+00],\n",
       "         [-4.7189e+00,  1.6724e+00,  1.2631e+00,  8.3790e-01, -4.0631e+00,\n",
       "          -6.7373e-01,  3.2017e+00,  2.2630e+00],\n",
       "         [-4.7495e+00,  1.3617e+00,  1.3897e+00,  8.5009e-01, -4.0928e+00,\n",
       "          -8.9078e-01,  3.0106e+00,  1.6766e+00],\n",
       "         [-4.8762e+00,  1.0808e+00,  1.4436e+00,  7.8970e-01, -4.0920e+00,\n",
       "          -9.6534e-01,  2.8661e+00,  1.2938e+00],\n",
       "         [-4.8720e+00,  9.7039e-01,  1.4870e+00,  7.6919e-01, -4.1175e+00,\n",
       "          -1.0580e+00,  2.6915e+00,  9.3018e-01],\n",
       "         [-4.9082e+00,  6.2774e-01,  1.5417e+00,  7.3667e-01, -4.1554e+00,\n",
       "          -1.1171e+00,  2.3691e+00,  9.5853e-02],\n",
       "         [-5.0640e+00,  3.8126e-01,  1.5746e+00,  6.0687e-01, -4.1546e+00,\n",
       "          -1.2360e+00,  1.9041e+00, -1.0045e+00],\n",
       "         [-5.0894e+00,  9.2984e-02,  1.5730e+00,  5.1348e-01, -4.1857e+00,\n",
       "          -1.3786e+00,  1.4465e+00, -1.9097e+00],\n",
       "         [-5.2156e+00, -1.0351e-01,  1.5477e+00,  4.6456e-01, -4.2190e+00,\n",
       "          -1.4606e+00,  8.5197e-01, -2.7448e+00],\n",
       "         [-5.2504e+00, -4.2605e-01,  1.5091e+00,  4.0085e-01, -4.2201e+00,\n",
       "          -1.6312e+00,  1.8653e-01, -3.3502e+00],\n",
       "         [-5.3723e+00, -7.7671e-01,  1.3852e+00,  2.8484e-01, -4.2794e+00,\n",
       "          -1.7050e+00, -5.4811e-01, -3.7312e+00],\n",
       "         [-5.3410e+00, -1.0833e+00,  1.3397e+00,  1.5353e-01, -4.3358e+00,\n",
       "          -1.8387e+00, -1.1999e+00, -4.0278e+00],\n",
       "         [-5.4352e+00, -1.5090e+00,  1.0984e+00, -4.3881e-02, -4.2825e+00,\n",
       "          -2.0164e+00, -2.4938e+00, -4.4366e+00]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0925, 0.1409, 0.0801, 0.0998, 0.1812, 0.0752, 0.1121, 0.1222, 0.1523,\n",
       "         0.0689, 0.0690, 0.0857, 0.0731, 0.0731, 0.1224, 0.0765, 0.0650, 0.0787,\n",
       "         0.0881, 0.0794, 0.0721, 0.0586, 0.1232, 0.1419, 0.0664, 0.0956, 0.0826,\n",
       "         0.1827, 0.0691, 0.1022, 0.0730, 0.1127, 0.1072, 0.0676, 0.1420, 0.1147,\n",
       "         0.0884, 0.0692, 0.0688, 0.0987, 0.0662, 0.0565, 0.0616, 0.1215, 0.1110,\n",
       "         0.0698, 0.2371, 0.0964, 0.0854, 0.0776, 0.0847, 0.1128, 0.1156, 0.1053,\n",
       "         0.1418, 0.0827, 0.0772, 0.0836, 0.0612, 0.1443, 0.0689, 0.1074, 0.1259,\n",
       "         0.0705, 0.0638, 0.1065, 0.0912, 0.0763, 0.2283, 0.1283, 0.1083, 0.0834,\n",
       "         0.0814, 0.0746, 0.0798, 0.0610, 0.0700, 0.0834, 0.1091, 0.1360, 0.1448,\n",
       "         0.0995, 0.0842, 0.1153, 0.0612, 0.1071, 0.0592, 0.0908, 0.0765, 0.3712,\n",
       "         0.0894, 0.1148, 0.0732, 0.0881, 0.0947, 0.0813, 0.0892, 0.0612, 0.0420,\n",
       "         0.0666, 0.0800, 0.0685, 0.1196, 0.1095, 0.0658, 0.0773, 0.0625, 0.0898,\n",
       "         0.0861, 0.0890, 0.0885, 0.1281, 0.0631, 0.0785, 0.0663, 0.1206, 0.0763,\n",
       "         0.0702, 0.0592, 0.0566, 0.0753, 0.0860, 0.0737, 0.0791, 0.0646, 0.2375,\n",
       "         0.0978, 0.0803, 0.0504, 0.0747, 0.0813, 0.0681, 0.1057, 0.1204, 0.1017,\n",
       "         0.0743, 0.1126, 0.0665, 0.0630, 0.0653, 0.0678, 0.1666, 0.1048, 0.0610,\n",
       "         0.0767, 0.0948, 0.1082, 0.0992, 0.1011, 0.0721, 0.0640, 0.0713, 0.1213,\n",
       "         0.0644, 0.0700, 0.0734, 0.0617, 0.1113, 0.0854, 0.2183, 0.0643, 0.1297,\n",
       "         0.0675, 0.0764, 0.1048, 0.0928, 0.0501, 0.1095, 0.0704, 0.0631, 0.1229,\n",
       "         0.0817, 0.0870, 0.0676, 0.0728, 0.0927, 0.0612, 0.0995, 0.0626, 0.0796,\n",
       "         0.0605, 0.1051, 0.1555, 0.1283, 0.1066, 0.0774, 0.0827, 0.1014, 0.0754,\n",
       "         0.1039, 0.1679, 0.0851, 0.0779, 0.1205, 0.0752, 0.0682, 0.0583, 0.1370,\n",
       "         0.0681, 0.0813, 0.1238, 0.1285, 0.1035, 0.0879, 0.0780, 0.0905, 0.1085,\n",
       "         0.1258, 0.0825, 0.0768, 0.0794, 0.0723, 0.1105, 0.1279, 0.0755, 0.0688,\n",
       "         0.0961, 0.1023, 0.1759, 0.0637, 0.0601, 0.0673, 0.1129, 0.0725, 0.1338,\n",
       "         0.0700, 0.0973, 0.0693, 0.1260, 0.1104, 0.0846, 0.0647, 0.0807, 0.0849,\n",
       "         0.1444, 0.0975, 0.0885, 0.0602, 0.0722, 0.1025, 0.1295, 0.1237, 0.0837,\n",
       "         0.0725, 0.1242, 0.2009, 0.1349, 0.1117, 0.0659, 0.1172, 0.0628, 0.0761,\n",
       "         0.1158, 0.0633, 0.0618, 0.0785, 0.0905, 0.0881, 0.0881, 0.1116, 0.0998,\n",
       "         0.0623, 0.0701, 0.1864, 0.1057, 0.0788, 0.0905, 0.0665, 0.0522, 0.0663,\n",
       "         0.0808, 0.1036, 0.1155, 0.1806, 0.0594, 0.1225, 0.0647, 0.0758, 0.0993,\n",
       "         0.1138, 0.1509, 0.0973, 0.0749, 0.1118, 0.0701, 0.0920, 0.1026, 0.1200,\n",
       "         0.0618, 0.1188, 0.1346, 0.0740, 0.0681, 0.0704, 0.1242, 0.1427, 0.0926,\n",
       "         0.0670, 0.0741, 0.0836, 0.0978, 0.1156, 0.1292, 0.0734, 0.1302, 0.0742,\n",
       "         0.0853, 0.1768, 0.1797, 0.1070, 0.0730, 0.0873, 0.0901, 0.0864, 0.0892,\n",
       "         0.0704, 0.0645, 0.0615, 0.1212, 0.1000, 0.0857, 0.0625, 0.0647, 0.1603,\n",
       "         0.1913, 0.0705, 0.1113, 0.0628, 0.0763, 0.1097, 0.1213, 0.1042, 0.1210,\n",
       "         0.0705, 0.0637, 0.0963, 0.1112, 0.0802, 0.0713, 0.1024, 0.1272, 0.0979,\n",
       "         0.0868, 0.1297, 0.0797, 0.1110, 0.1153, 0.0639, 0.0595, 0.1060, 0.1192,\n",
       "         0.0774, 0.0712, 0.0627, 0.0763, 0.1792, 0.0838, 0.0847, 0.0837, 0.1075,\n",
       "         0.0997, 0.0666, 0.0675, 0.0736, 0.0879, 0.1192, 0.0584, 0.1872, 0.0823,\n",
       "         0.0716, 0.0800, 0.1035, 0.0627, 0.0650, 0.1085, 0.0707, 0.0644, 0.0720,\n",
       "         0.0604, 0.1312, 0.0807, 0.1345, 0.1019, 0.0717, 0.0963, 0.0707, 0.0806,\n",
       "         0.0743, 0.1477, 0.0661, 0.0708, 0.0705, 0.0851, 0.0845, 0.0886, 0.0986,\n",
       "         0.0571, 0.0885, 0.0818, 0.1253, 0.1452, 0.1105, 0.0789, 0.2204, 0.0609,\n",
       "         0.0566, 0.0723, 0.0691, 0.0731, 0.0707, 0.0874, 0.1555, 0.1154, 0.1083,\n",
       "         0.0697, 0.1101, 0.0996, 0.1986, 0.0708, 0.0742, 0.0885, 0.0883, 0.0764,\n",
       "         0.0746, 0.1026, 0.0749, 0.0660, 0.0919, 0.0620, 0.1125, 0.0692, 0.0706,\n",
       "         0.0588, 0.1187, 0.0664, 0.0712, 0.0996, 0.0535, 0.1018, 0.1190, 0.0625,\n",
       "         0.0722, 0.0617, 0.0931, 0.0782, 0.0788, 0.0695, 0.1233, 0.0764, 0.0828,\n",
       "         0.0559, 0.0614, 0.0614, 0.1009, 0.1181, 0.0862, 0.0679, 0.0808, 0.0793,\n",
       "         0.0668, 0.0625, 0.0716, 0.0658, 0.0653, 0.0694, 0.0574, 0.0578, 0.1075,\n",
       "         0.1112, 0.0706, 0.0796, 0.0667, 0.0832, 0.0626, 0.0828, 0.0750, 0.0673,\n",
       "         0.1289, 0.0725, 0.0601, 0.1347, 0.0950, 0.0653, 0.0674, 0.0746, 0.0722,\n",
       "         0.0914, 0.0914, 0.0842, 0.0856, 0.1441, 0.0569, 0.0788, 0.0681, 0.0630,\n",
       "         0.1247, 0.0688, 0.0835, 0.0929, 0.1146, 0.0776, 0.1621, 0.1022, 0.0787,\n",
       "         0.0854, 0.1299, 0.1427, 0.0684, 0.1207, 0.0729, 0.0719, 0.0958],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.6740, -1.3454,  0.6439,  ..., -0.2120,  0.4771, -0.1525],\n",
       "         [-0.0667, -0.3078, -0.1464,  ...,  0.4139,  0.1812,  0.5146],\n",
       "         [ 0.6224,  0.2615,  0.4261,  ...,  0.4416, -0.1360,  0.0710],\n",
       "         ...,\n",
       "         [-0.5912,  0.0819, -0.6948,  ...,  0.8871, -0.1900,  0.4571],\n",
       "         [-0.5907, -0.0092, -0.1246,  ..., -0.3520,  0.2286,  0.4982],\n",
       "         [-0.2786, -0.3651,  0.2030,  ..., -0.0048,  0.6889, -0.0163]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0562, -0.1385, -0.2890,  ..., -0.0332,  0.0401, -0.3002],\n",
       "         [-0.3374,  0.2718,  0.0273,  ..., -0.1850, -0.0951, -0.2863],\n",
       "         [ 0.2515,  0.0136, -0.1988,  ..., -0.7565,  0.0220, -0.3096],\n",
       "         ...,\n",
       "         [-0.1910, -0.3032, -0.1861,  ...,  0.1998,  0.0190, -0.2781],\n",
       "         [-0.0956,  0.0586,  0.1320,  ...,  0.0198, -0.0224,  0.4407],\n",
       "         [ 0.4038,  0.1455,  0.0541,  ..., -0.2215,  0.0987,  0.0488]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2942, 0.4743, 0.2440, 0.3026, 0.4381, 0.2351, 0.4016, 0.3975, 0.4597,\n",
       "         0.1976, 0.2181, 0.3848, 0.2187, 0.2145, 0.9185, 0.2400, 0.2067, 0.3056,\n",
       "         0.2744, 0.2253, 0.2480, 0.1711, 0.3729, 0.8242, 0.2012, 0.2888, 0.2223,\n",
       "         0.4954, 0.2073, 0.2470, 0.2037, 0.3998, 0.3072, 0.2017, 0.4264, 0.3913,\n",
       "         0.2342, 0.2362, 0.2973, 0.3031, 0.2408, 0.2009, 0.2280, 0.3633, 0.3413,\n",
       "         0.2055, 0.7219, 0.3017, 0.2773, 0.2092, 0.2904, 0.3317, 0.3810, 0.4297,\n",
       "         0.4165, 0.2604, 0.2440, 0.2761, 0.1982, 0.4122, 0.2201, 0.3105, 0.3505,\n",
       "         0.2356, 0.3085, 0.3370, 0.2471, 0.1873, 0.5594, 0.3733, 0.3287, 0.2638,\n",
       "         0.2375, 0.2102, 0.2286, 0.2137, 0.2159, 0.2704, 0.3350, 0.3754, 0.3563,\n",
       "         0.3036, 0.2629, 0.3174, 0.1915, 0.3359, 0.2250, 0.3228, 0.2238, 0.9423,\n",
       "         0.2481, 0.3473, 0.2630, 0.2690, 0.3112, 0.2550, 0.3205, 0.1898, 0.2335,\n",
       "         0.2442, 0.2360, 0.2096, 0.3424, 0.4911, 0.1974, 0.2288, 0.2057, 0.2852,\n",
       "         0.2503, 0.2941, 0.2852, 0.3779, 0.1953, 0.2389, 0.1922, 0.3634, 0.2191,\n",
       "         0.2315, 0.2232, 0.1793, 0.2120, 0.2685, 0.1921, 0.2525, 0.2105, 0.7872,\n",
       "         0.2957, 0.2379, 0.2351, 0.2598, 0.2575, 0.2046, 0.3189, 0.3586, 0.3592,\n",
       "         0.2152, 0.3615, 0.3996, 0.2125, 0.2048, 0.2105, 0.5147, 0.3295, 0.2120,\n",
       "         0.2333, 0.2991, 0.3451, 0.3956, 0.2828, 0.2844, 0.1886, 0.1963, 0.3738,\n",
       "         0.1965, 0.2434, 0.2152, 0.2040, 0.3723, 0.3428, 3.6644, 0.2047, 0.2735,\n",
       "         0.2087, 0.2205, 0.3309, 0.2980, 0.4329, 0.3348, 0.2036, 0.2004, 0.3699,\n",
       "         0.2377, 0.2714, 0.1969, 0.2225, 0.2659, 0.2433, 0.3554, 0.1962, 0.2042,\n",
       "         0.2027, 0.2674, 0.3770, 0.4116, 0.5636, 0.2584, 0.2451, 0.3177, 0.2241,\n",
       "         0.3218, 2.3283, 0.1779, 0.2143, 0.4014, 0.2665, 0.1921, 0.2035, 0.3506,\n",
       "         0.1938, 0.2334, 0.3499, 0.3696, 0.3323, 0.3631, 0.2386, 0.2634, 0.2741,\n",
       "         0.3249, 0.2811, 0.2059, 0.2258, 0.2082, 0.3548, 0.3893, 0.2149, 0.1999,\n",
       "         0.2639, 0.3138, 0.4886, 0.1809, 0.2042, 0.2102, 0.3667, 0.2181, 0.4781,\n",
       "         0.2020, 0.3441, 0.1900, 0.3329, 0.3566, 0.2444, 0.2236, 0.3057, 0.2682,\n",
       "         0.4419, 0.3121, 0.2679, 0.2685, 0.2210, 0.3781, 0.4191, 0.3909, 0.2911,\n",
       "         0.2034, 0.4208, 0.4991, 0.1630, 0.3115, 0.3862, 0.3571, 0.1969, 0.2322,\n",
       "         0.3558, 0.2040, 0.1954, 0.2648, 0.2637, 0.3013, 0.3349, 0.3618, 0.0797,\n",
       "         0.2043, 0.2250, 0.8708, 0.1799, 0.2346, 0.2874, 0.2088, 0.2160, 0.2857,\n",
       "         0.2319, 0.3201, 0.3676, 0.6253, 0.1996, 0.1507, 0.2351, 0.1908, 0.3248,\n",
       "         0.3494, 0.5090, 0.3352, 0.2100, 0.3350, 0.2123, 0.2748, 0.3356, 0.3362,\n",
       "         0.2151, 0.3786, 0.5806, 0.2063, 0.2130, 0.2255, 0.5169, 0.4612, 0.2593,\n",
       "         0.2046, 0.2006, 0.4762, 0.3287, 0.3489, 0.5368, 0.2007, 0.4227, 0.2052,\n",
       "         0.2748, 0.5783, 0.7560, 0.3648, 0.2178, 0.2830, 0.4420, 0.2732, 0.2756,\n",
       "         0.2105, 0.1814, 0.2030, 0.3628, 0.3536, 0.2535, 0.2053, 0.1975, 0.3815,\n",
       "         1.2354, 0.1837, 0.3675, 0.2131, 0.2142, 0.3995, 0.4147, 0.3205, 0.3622,\n",
       "         0.1962, 0.1913, 0.3673, 0.3548, 0.2559, 0.2255, 0.2171, 0.4340, 0.3248,\n",
       "         0.2359, 0.3936, 0.2184, 0.3510, 0.3542, 0.2086, 0.2011, 0.3075, 0.3704,\n",
       "         0.2245, 0.2250, 0.1924, 0.2751, 0.5616, 0.2665, 0.2310, 0.2529, 0.3387,\n",
       "         0.2966, 0.2024, 0.2125, 0.2676, 0.2552, 0.3264, 0.1990, 1.7895, 0.2700,\n",
       "         0.2097, 0.2204, 0.3341, 0.2364, 0.2250, 0.4623, 0.1971, 0.2247, 0.2271,\n",
       "         0.1871, 0.4653, 0.2163, 0.6551, 0.2991, 0.2038, 0.4002, 0.2059, 0.3137,\n",
       "         0.2123, 0.4093, 0.2033, 0.2022, 0.2211, 0.2987, 0.2825, 0.2668, 0.3000,\n",
       "         0.2291, 0.1500, 0.2479, 0.3297, 0.4215, 0.4303, 0.2928, 0.5507, 0.1934,\n",
       "         0.3262, 0.2242, 0.2283, 0.2240, 0.2273, 0.2965, 0.4484, 0.3942, 0.3785,\n",
       "         0.2056, 0.3301, 0.2965, 0.5906, 0.2054, 0.1947, 0.2631, 0.2541, 0.1201,\n",
       "         0.2642, 0.2020, 0.2091, 0.2054, 0.2808, 0.1914, 0.3231, 0.1922, 0.2113,\n",
       "         0.1864, 0.1091, 0.2305, 0.2398, 0.3140, 0.1857, 0.2956, 0.3456, 0.2211,\n",
       "         0.2206, 0.2038, 0.3224, 0.2223, 0.2248, 0.1967, 0.3751, 0.2167, 0.2221,\n",
       "         0.1896, 0.1986, 0.2885, 0.2318, 0.3411, 0.2673, 0.1979, 0.2416, 0.2548,\n",
       "         0.1980, 0.1829, 0.2658, 0.2156, 0.2122, 0.2291, 0.2001, 0.2006, 0.3355,\n",
       "         0.3847, 0.2124, 0.2399, 0.1822, 0.2789, 0.2075, 0.2074, 0.5372, 0.2020,\n",
       "         0.4596, 0.2189, 0.1901, 0.4059, 0.2689, 0.2017, 0.1888, 0.2080, 0.2002,\n",
       "         0.2808, 0.2908, 0.2704, 0.2338, 0.3654, 0.2163, 0.0890, 0.2360, 0.1935,\n",
       "         0.3642, 0.2153, 0.3282, 0.2965, 0.3664, 0.3039, 0.5858, 0.3090, 0.2456,\n",
       "         0.2516, 0.4021, 0.3412, 0.2229, 0.3433, 0.2520, 0.2298, 0.3649],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.7158e-02,  2.6255e-02, -2.0745e-02,  ..., -9.2019e-03,\n",
       "           3.2113e-02, -9.4909e-03],\n",
       "         [-3.0870e-02,  1.7567e-02, -3.1235e-06,  ...,  2.4419e-02,\n",
       "          -3.3236e-02,  1.9277e-02],\n",
       "         [-4.6242e-02,  5.5458e-02,  4.4033e-02,  ...,  2.9463e-03,\n",
       "           1.5207e-02, -5.9996e-02],\n",
       "         ...,\n",
       "         [-7.8356e-02,  6.2782e-02,  5.7995e-02,  ..., -7.7721e-03,\n",
       "           7.3003e-02,  1.1527e-03],\n",
       "         [-6.6591e-02,  1.3906e-02,  1.5782e-02,  ...,  5.7555e-02,\n",
       "          -4.3479e-02,  5.7037e-02],\n",
       "         [-4.8092e-02, -4.7791e-02, -3.7732e-03,  ...,  4.6030e-02,\n",
       "           1.1595e-02,  6.6530e-02]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.3758e-01, -4.6715e-01, -1.9033e-05,  ...,  3.9842e-01,\n",
       "          -4.9566e-01,  4.5146e-01],\n",
       "         [-2.9123e-01, -3.0785e-01,  4.8027e-01,  ..., -4.7627e-02,\n",
       "           6.0119e-01,  4.9734e-01],\n",
       "         [-6.1902e-02, -3.1327e-01,  2.1264e-02,  ...,  2.7861e-01,\n",
       "          -4.8719e-01, -3.6679e-02],\n",
       "         ...,\n",
       "         [-1.6966e-01, -4.6442e-01,  1.6947e-01,  ..., -8.3669e-02,\n",
       "           1.7697e-02, -1.3633e-01],\n",
       "         [-5.1237e-01,  6.1055e-01, -8.7540e-01,  ...,  4.1402e-03,\n",
       "           3.4767e-01,  1.4572e-01],\n",
       "         [-8.6774e-01,  5.6959e-01,  5.3168e-02,  ..., -1.5589e-01,\n",
       "           9.4744e-02, -2.7975e-01]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.4602,  0.4496,  0.7601,  ...,  0.0294, -0.6860, -0.0682],\n",
       "         [-0.6367,  0.1718,  0.8350,  ...,  0.1772,  0.3610,  0.1503],\n",
       "         [ 0.3982, -0.8914,  0.3550,  ..., -1.0509,  0.1847,  0.3340],\n",
       "         ...,\n",
       "         [ 0.2824,  0.0277,  0.9267,  ..., -0.4588,  0.2749, -0.3080],\n",
       "         [-0.1015,  0.5105, -0.9491,  ..., -0.3441,  0.2843, -0.1243],\n",
       "         [ 0.1241, -0.2385,  0.2157,  ..., -0.9219, -0.0300, -0.4678]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1069, -0.0525, -0.1871,  ...,  0.4608, -0.7113, -0.1705],\n",
       "         [ 0.3230, -1.9336,  0.4764,  ...,  0.5630,  1.0137, -0.1572],\n",
       "         [-0.2502,  0.2749, -0.6688,  ..., -0.4601,  1.2428, -0.0033],\n",
       "         ...,\n",
       "         [-0.4636,  0.0189, -0.4568,  ...,  0.8515,  0.4570, -0.8534],\n",
       "         [-0.7504, -0.1617, -0.3050,  ..., -0.6178,  0.0219, -1.3428],\n",
       "         [-1.2946,  1.4476, -0.0019,  ...,  1.4883,  0.5667,  0.0099]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1139, 0.1428, 0.0942, 0.1110, 0.1540, 0.0959, 0.1330, 0.1344, 0.1239,\n",
       "         0.0850, 0.0872, 0.1089, 0.0852, 0.0930, 0.1559, 0.0973, 0.0899, 0.0994,\n",
       "         0.1073, 0.0943, 0.0989, 0.0690, 0.1315, 0.1760, 0.0771, 0.0995, 0.0953,\n",
       "         0.1271, 0.0894, 0.0432, 0.0831, 0.1277, 0.1117, 0.0857, 0.1293, 0.1279,\n",
       "         0.1072, 0.0902, 0.0708, 0.1149, 0.1026, 0.0853, 0.0653, 0.1226, 0.1126,\n",
       "         0.0855, 0.1832, 0.1137, 0.1093, 0.0962, 0.1146, 0.1169, 0.1269, 0.1076,\n",
       "         0.1167, 0.1106, 0.0972, 0.0972, 0.0837, 0.1276, 0.0895, 0.0451, 0.1315,\n",
       "         0.0912, 0.0881, 0.1223, 0.1043, 0.0804, 0.1602, 0.1312, 0.1331, 0.0936,\n",
       "         0.0993, 0.0966, 0.0935, 0.0954, 0.0929, 0.1100, 0.1159, 0.1115, 0.0974,\n",
       "         0.1015, 0.0970, 0.1253, 0.0854, 0.1164, 0.0848, 0.0995, 0.0904, 0.2181,\n",
       "         0.0995, 0.1270, 0.0989, 0.1017, 0.1169, 0.1003, 0.1056, 0.0780, 0.0674,\n",
       "         0.0934, 0.0963, 0.0841, 0.1308, 0.1192, 0.0800, 0.0936, 0.0785, 0.1071,\n",
       "         0.0467, 0.1097, 0.1125, 0.1260, 0.0843, 0.0910, 0.0876, 0.0605, 0.0927,\n",
       "         0.1031, 0.0902, 0.0752, 0.0876, 0.0984, 0.0870, 0.0903, 0.0896, 0.1827,\n",
       "         0.1123, 0.0864, 0.0583, 0.1118, 0.1011, 0.0811, 0.1024, 0.1080, 0.1114,\n",
       "         0.0870, 0.0449, 0.1113, 0.0774, 0.0773, 0.0927, 0.1618, 0.1043, 0.0814,\n",
       "         0.0955, 0.1164, 0.1158, 0.0978, 0.1119, 0.0579, 0.0824, 0.0871, 0.1219,\n",
       "         0.0802, 0.1069, 0.0797, 0.0844, 0.1195, 0.1104, 0.3575, 0.0864, 0.1073,\n",
       "         0.0868, 0.0927, 0.1220, 0.1148, 0.0563, 0.1228, 0.0795, 0.0794, 0.1361,\n",
       "         0.0963, 0.0969, 0.0871, 0.0895, 0.1007, 0.0928, 0.1193, 0.0837, 0.0904,\n",
       "         0.0802, 0.1047, 0.1317, 0.1379, 0.1230, 0.1068, 0.0974, 0.1151, 0.0924,\n",
       "         0.1201, 0.2532, 0.0396, 0.0975, 0.1256, 0.1028, 0.0877, 0.0800, 0.1128,\n",
       "         0.0812, 0.0976, 0.0978, 0.1308, 0.1183, 0.0416, 0.0899, 0.1010, 0.1239,\n",
       "         0.1342, 0.1037, 0.0877, 0.0902, 0.0906, 0.1081, 0.1244, 0.0803, 0.0804,\n",
       "         0.1206, 0.1262, 0.1388, 0.0768, 0.0844, 0.0827, 0.1375, 0.1005, 0.1196,\n",
       "         0.0880, 0.1005, 0.0766, 0.1265, 0.1160, 0.0945, 0.0843, 0.0839, 0.1059,\n",
       "         0.1394, 0.1164, 0.0977, 0.0891, 0.0910, 0.1190, 0.1221, 0.1351, 0.1064,\n",
       "         0.0880, 0.1272, 0.1351, 0.0359, 0.1072, 0.0774, 0.1285, 0.0776, 0.0875,\n",
       "         0.1118, 0.0800, 0.0764, 0.0986, 0.1051, 0.1094, 0.0955, 0.1286, 0.0423,\n",
       "         0.0871, 0.0821, 0.1768, 0.0359, 0.0935, 0.1083, 0.0808, 0.0786, 0.0966,\n",
       "         0.0930, 0.1218, 0.1237, 0.1766, 0.0775, 0.0590, 0.0882, 0.0743, 0.1118,\n",
       "         0.1164, 0.1338, 0.1174, 0.0940, 0.1185, 0.0884, 0.1051, 0.1249, 0.1322,\n",
       "         0.0786, 0.1283, 0.1343, 0.0902, 0.0897, 0.0888, 0.1384, 0.1602, 0.1086,\n",
       "         0.0898, 0.0870, 0.0820, 0.1139, 0.1288, 0.1269, 0.0891, 0.1116, 0.0781,\n",
       "         0.1128, 0.1522, 0.1670, 0.1233, 0.0984, 0.1119, 0.0724, 0.1091, 0.1073,\n",
       "         0.0904, 0.0755, 0.0770, 0.0902, 0.1274, 0.1031, 0.0864, 0.0764, 0.1020,\n",
       "         0.1912, 0.0736, 0.1241, 0.0798, 0.0916, 0.1104, 0.1137, 0.1130, 0.1347,\n",
       "         0.0879, 0.0811, 0.1255, 0.1154, 0.0982, 0.0917, 0.0788, 0.1159, 0.1137,\n",
       "         0.1018, 0.1266, 0.0839, 0.1208, 0.1305, 0.0860, 0.0813, 0.1171, 0.1419,\n",
       "         0.0996, 0.0847, 0.0866, 0.1063, 0.1620, 0.1008, 0.1001, 0.0972, 0.1245,\n",
       "         0.1120, 0.0788, 0.0891, 0.0962, 0.0975, 0.1154, 0.0859, 0.1729, 0.1010,\n",
       "         0.0925, 0.1004, 0.0834, 0.0773, 0.0940, 0.1156, 0.0848, 0.0879, 0.0956,\n",
       "         0.0747, 0.1186, 0.0974, 0.1440, 0.1222, 0.0937, 0.1260, 0.0913, 0.0962,\n",
       "         0.0919, 0.1437, 0.0803, 0.0872, 0.0851, 0.1076, 0.1000, 0.1000, 0.1241,\n",
       "         0.0820, 0.0272, 0.1057, 0.1028, 0.1332, 0.1154, 0.0993, 0.1534, 0.0772,\n",
       "         0.0832, 0.0906, 0.0810, 0.0867, 0.0879, 0.0762, 0.1142, 0.1423, 0.1422,\n",
       "         0.0922, 0.1097, 0.1174, 0.1584, 0.0906, 0.0806, 0.1084, 0.1159, 0.0296,\n",
       "         0.0962, 0.0562, 0.0957, 0.0850, 0.1077, 0.0835, 0.1251, 0.0853, 0.0884,\n",
       "         0.0732, 0.0300, 0.0806, 0.0865, 0.1125, 0.0797, 0.1091, 0.1320, 0.0765,\n",
       "         0.0845, 0.0825, 0.1165, 0.1007, 0.0959, 0.0789, 0.1321, 0.0860, 0.1014,\n",
       "         0.0717, 0.0797, 0.0886, 0.0583, 0.1199, 0.1036, 0.0787, 0.1050, 0.1015,\n",
       "         0.0856, 0.0792, 0.0688, 0.0877, 0.0920, 0.0956, 0.0839, 0.0718, 0.1184,\n",
       "         0.1270, 0.0802, 0.0913, 0.0800, 0.1044, 0.0943, 0.0953, 0.1017, 0.0906,\n",
       "         0.1151, 0.0914, 0.0843, 0.1385, 0.1070, 0.0817, 0.0769, 0.0856, 0.0883,\n",
       "         0.1137, 0.1137, 0.1122, 0.1062, 0.1198, 0.0771, 0.0338, 0.0919, 0.0862,\n",
       "         0.1256, 0.0842, 0.1086, 0.1200, 0.1181, 0.0779, 0.1683, 0.1225, 0.0940,\n",
       "         0.1035, 0.1268, 0.1151, 0.0934, 0.1267, 0.0966, 0.0859, 0.0987],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.3138,  0.9963,  0.0199,  ..., -0.2014, -0.1471, -0.8039],\n",
       "         [ 0.3205,  0.5901, -0.2924,  ...,  0.8860, -0.3949, -0.8318],\n",
       "         [-0.0437, -0.5701, -0.4577,  ..., -0.3332,  0.9856,  0.9766],\n",
       "         ...,\n",
       "         [ 0.6702,  0.8499,  1.7617,  ..., -1.0525, -0.1779, -0.7312],\n",
       "         [-0.1850,  0.6322,  0.0480,  ...,  0.9191, -0.0700, -0.8574],\n",
       "         [-0.7300, -0.5802, -0.8047,  ..., -0.5758,  0.4749, -0.2203]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0416,  0.1722, -0.0927,  ..., -0.1921, -0.0147, -0.0205],\n",
       "         [-0.2592, -0.0156, -0.1937,  ..., -0.6694, -0.2247, -0.3572],\n",
       "         [-0.0111,  0.5689,  0.6074,  ..., -0.4262, -0.0897, -0.0636],\n",
       "         ...,\n",
       "         [-0.2162,  0.2890, -0.1745,  ...,  0.1092, -0.3046, -0.2872],\n",
       "         [-0.5593, -0.0434, -0.3406,  ...,  0.3623,  0.1791, -0.2650],\n",
       "         [ 0.2909,  0.3340,  0.0135,  ..., -0.2847, -0.0910,  0.1337]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4373, 0.5217, 0.3591, 0.4052, 0.5622, 0.3660, 0.5225, 0.4626, 0.5049,\n",
       "         0.3223, 0.3460, 0.4339, 0.3314, 0.3339, 0.8357, 0.3546, 0.3378, 0.3782,\n",
       "         0.3965, 0.3692, 0.3641, 0.2870, 0.4983, 0.8494, 0.3256, 0.4191, 0.3641,\n",
       "         0.5765, 0.3373, 0.3375, 0.3328, 0.4758, 0.4383, 0.3298, 0.5219, 0.4887,\n",
       "         0.3656, 0.3639, 0.3428, 0.4434, 0.3606, 0.3243, 0.7024, 0.4514, 0.4191,\n",
       "         0.3318, 0.6700, 0.4227, 0.4289, 0.3655, 0.4352, 0.4659, 0.4883, 0.3956,\n",
       "         0.4326, 0.4112, 0.3634, 0.4051, 0.3203, 0.4968, 0.3485, 0.3043, 0.4562,\n",
       "         0.3585, 0.4011, 0.4631, 0.4291, 0.3026, 0.6843, 0.4738, 0.4506, 0.3741,\n",
       "         0.3695, 0.3345, 0.3619, 0.3838, 0.3166, 0.4081, 0.4473, 0.4148, 0.3809,\n",
       "         0.4641, 0.3883, 0.4487, 0.3088, 0.4703, 0.3366, 0.3615, 0.3533, 0.8849,\n",
       "         0.3759, 0.4474, 0.3903, 0.3898, 0.4363, 0.3852, 0.4197, 0.3148, 0.3541,\n",
       "         0.3718, 0.3779, 0.3302, 0.4398, 0.5028, 0.3121, 0.3566, 0.3159, 0.3875,\n",
       "         0.3243, 0.4141, 0.4141, 0.5414, 0.3228, 0.3846, 0.3120, 0.3622, 0.3610,\n",
       "         0.3725, 0.3630, 0.2982, 0.3324, 0.3787, 0.3166, 0.3692, 0.3204, 0.7084,\n",
       "         0.3449, 0.3573, 0.2967, 0.3873, 0.3953, 0.3400, 0.4458, 0.4311, 0.4723,\n",
       "         0.3341, 0.5094, 0.4196, 0.3310, 0.3486, 0.3394, 0.5428, 0.4002, 0.3246,\n",
       "         0.3770, 0.3791, 0.4727, 0.6833, 0.3661, 0.4337, 0.3180, 0.3159, 0.4600,\n",
       "         0.3080, 0.3724, 0.3505, 0.3068, 0.4745, 0.4979, 1.5658, 0.3105, 0.2940,\n",
       "         0.3494, 0.3735, 0.4281, 0.4310, 0.3965, 0.5128, 0.3323, 0.3094, 0.5142,\n",
       "         0.3736, 0.3885, 0.3257, 0.3385, 0.3809, 0.3441, 0.3987, 0.3289, 0.3287,\n",
       "         0.3248, 0.3978, 0.4806, 0.5047, 0.6741, 0.3906, 0.3666, 0.4207, 0.3550,\n",
       "         0.4297, 1.3280, 0.1791, 0.3469, 0.4912, 0.3826, 0.3323, 0.3265, 0.4460,\n",
       "         0.3141, 0.3663, 0.6577, 0.4401, 0.4287, 0.6090, 0.3837, 0.3946, 0.3907,\n",
       "         0.3543, 0.4015, 0.3355, 0.3362, 0.3510, 0.4291, 0.4729, 0.3389, 0.3304,\n",
       "         0.3958, 0.4120, 0.4953, 0.3049, 0.3207, 0.3243, 0.4842, 0.3540, 0.5293,\n",
       "         0.3366, 0.4070, 0.3069, 0.4429, 0.5886, 0.3640, 0.3374, 0.3525, 0.4003,\n",
       "         0.4936, 0.4040, 0.4629, 0.3578, 0.3512, 0.4614, 0.5049, 0.4644, 0.3890,\n",
       "         0.3240, 0.4727, 0.4982, 0.2011, 0.4511, 0.3785, 0.4540, 0.2988, 0.3735,\n",
       "         0.4587, 0.3492, 0.3082, 0.3895, 0.4072, 0.4314, 0.4715, 0.4746, 0.4014,\n",
       "         0.3113, 0.3607, 0.9670, 0.1853, 0.3691, 0.4001, 0.3242, 0.3361, 0.3864,\n",
       "         0.3582, 0.4116, 0.4370, 0.6658, 0.3333, 0.8412, 0.3672, 0.3032, 0.4133,\n",
       "         0.4637, 0.5265, 0.4337, 0.3213, 0.4532, 0.3296, 0.4194, 0.4600, 0.4256,\n",
       "         0.3182, 0.4394, 0.5590, 0.3300, 0.3318, 0.3279, 0.5060, 0.5114, 0.3904,\n",
       "         0.3303, 0.3340, 0.5206, 0.4220, 0.4753, 0.6323, 0.3228, 0.4702, 0.3254,\n",
       "         0.3624, 0.5248, 0.7908, 0.4679, 0.3378, 0.4067, 0.3855, 0.3999, 0.4158,\n",
       "         0.3286, 0.3049, 0.3177, 0.3737, 0.4572, 0.3776, 0.3230, 0.3090, 0.4048,\n",
       "         1.1392, 0.3102, 0.4921, 0.3192, 0.3269, 0.4180, 0.7394, 0.4231, 0.4635,\n",
       "         0.3142, 0.3068, 0.4502, 0.4636, 0.3851, 0.3593, 0.7222, 0.5029, 0.4380,\n",
       "         0.3600, 0.4469, 0.3323, 0.4341, 0.4694, 0.3040, 0.3317, 0.4340, 0.4459,\n",
       "         0.3696, 0.3478, 0.2999, 0.4199, 0.5856, 0.3961, 0.3670, 0.3650, 0.4104,\n",
       "         0.3910, 0.3272, 0.3326, 0.3823, 0.3776, 0.4418, 0.3192, 0.9040, 0.3884,\n",
       "         0.3268, 0.3371, 0.3646, 0.3048, 0.3599, 0.4849, 0.3259, 0.3315, 0.3507,\n",
       "         0.3196, 0.5938, 0.3372, 0.6189, 0.4335, 0.3374, 0.4874, 0.3264, 0.3569,\n",
       "         0.3480, 0.4906, 0.3224, 0.3194, 0.3513, 0.4036, 0.4189, 0.4057, 0.4063,\n",
       "         0.3231, 0.1567, 0.3774, 0.3989, 0.4830, 0.4940, 0.3882, 0.5366, 0.3120,\n",
       "         0.3393, 0.3609, 0.3360, 0.3658, 0.3475, 0.3734, 0.4899, 0.4896, 0.4168,\n",
       "         0.3210, 0.4068, 0.4119, 0.6320, 0.3476, 0.3211, 0.4186, 0.3924, 0.1401,\n",
       "         0.4035, 0.2014, 0.3228, 0.3314, 0.4134, 0.3144, 0.4366, 0.3281, 0.3361,\n",
       "         0.2871, 0.2512, 0.3434, 0.3595, 0.4366, 0.2992, 0.4319, 0.4986, 0.3295,\n",
       "         0.3657, 0.3423, 0.4257, 0.3537, 0.3498, 0.3245, 0.4474, 0.3418, 0.3585,\n",
       "         0.3002, 0.3039, 0.3669, 0.2179, 0.4653, 0.4151, 0.2994, 0.3770, 0.4116,\n",
       "         0.3263, 0.3022, 0.3138, 0.3477, 0.3334, 0.3490, 0.3156, 0.3213, 0.4588,\n",
       "         0.4843, 0.3320, 0.3601, 0.2991, 0.4006, 0.3378, 0.3665, 0.4819, 0.3366,\n",
       "         0.4899, 0.3272, 0.3217, 0.4529, 0.4103, 0.3362, 0.3093, 0.3425, 0.3205,\n",
       "         0.4272, 0.4090, 0.4157, 0.3440, 0.4786, 0.3239, 0.1472, 0.3495, 0.3061,\n",
       "         0.4389, 0.3709, 0.3991, 0.4422, 0.4666, 0.3700, 0.6220, 0.3935, 0.3795,\n",
       "         0.3748, 0.6985, 0.4007, 0.3548, 0.4493, 0.3871, 0.3382, 0.4239],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0175,  0.0578,  0.0542,  ...,  0.0020, -0.0478, -0.0321],\n",
       "         [-0.0464,  0.0487,  0.0637,  ..., -0.0645, -0.0765, -0.0467],\n",
       "         [ 0.1285,  0.0282, -0.0470,  ..., -0.0375,  0.0339, -0.0135],\n",
       "         ...,\n",
       "         [ 0.0515,  0.0507, -0.0062,  ...,  0.0103,  0.0337,  0.0279],\n",
       "         [ 0.0595,  0.0708,  0.0544,  ..., -0.0187, -0.0037, -0.0058],\n",
       "         [-0.0018,  0.0711, -0.0217,  ..., -0.0487,  0.0292,  0.0308]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.5270e-01, -1.8788e-02,  5.1851e-01,  ...,  9.9508e-02,\n",
       "           4.7318e-01, -4.8334e-01],\n",
       "         [-2.2972e-01,  7.2778e-01,  3.3662e-01,  ..., -4.0575e-01,\n",
       "           7.3181e-02,  1.4925e-01],\n",
       "         [ 6.7664e-01, -3.6870e-01,  2.2080e-01,  ..., -1.2206e-01,\n",
       "           2.6592e-01,  3.9681e-01],\n",
       "         ...,\n",
       "         [-3.0649e-01,  5.5945e-01,  2.0151e-01,  ...,  4.8690e-01,\n",
       "          -1.5777e-01, -1.7638e-01],\n",
       "         [-1.3140e-01,  4.3596e-04, -2.4287e-01,  ..., -3.6417e-01,\n",
       "           5.8922e-03, -3.1221e-02],\n",
       "         [-9.9702e-02,  4.3712e-01, -2.3900e-01,  ..., -3.7843e-02,\n",
       "           1.0184e-01,  3.2343e-01]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.0432e-02,  7.2879e-01, -4.2854e-01,  ..., -4.5677e-01,\n",
       "           4.1978e-01, -1.5109e-01],\n",
       "         [-6.3089e-01, -1.2756e-01,  2.1672e-01,  ...,  3.7027e-04,\n",
       "           3.9488e-01,  3.9537e-01],\n",
       "         [-1.6185e+00,  3.4385e-01,  3.9106e-01,  ...,  1.1992e-01,\n",
       "           3.2111e-02, -4.2556e-02],\n",
       "         ...,\n",
       "         [ 6.0349e-01,  8.3973e-01, -7.8655e-01,  ..., -4.0149e-01,\n",
       "          -5.3358e-01, -3.8982e-01],\n",
       "         [ 7.9171e-01, -9.8184e-02,  5.0993e-01,  ...,  1.5030e-01,\n",
       "           5.4023e-01,  2.4106e-01],\n",
       "         [-7.9833e-01,  2.8668e-01, -1.9289e-01,  ...,  3.3567e-01,\n",
       "          -5.9281e-02,  2.4762e-01]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2573, -0.4196,  0.0243,  ...,  0.0935,  0.6778, -0.3641],\n",
       "         [-0.5167, -0.1996,  0.3394,  ..., -0.0982, -0.2411, -0.2089],\n",
       "         [ 0.1229, -0.1733,  0.1599,  ...,  0.1563,  0.2511, -0.5743],\n",
       "         ...,\n",
       "         [ 0.1461,  0.1590,  0.2210,  ..., -0.5340, -1.0828, -0.1982],\n",
       "         [ 0.4080,  0.2480,  0.1983,  ..., -1.2613, -0.0811,  0.3516],\n",
       "         [-1.1237, -0.2916, -1.8939,  ...,  0.2845,  0.8320,  0.7357]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1514, 0.2003, 0.1440, 0.1352, 0.1871, 0.1376, 0.1890, 0.1760, 0.1561,\n",
       "         0.1248, 0.1173, 0.1600, 0.1248, 0.1344, 0.2254, 0.1319, 0.1254, 0.1052,\n",
       "         0.1502, 0.1310, 0.1395, 0.1016, 0.1646, 0.1973, 0.1175, 0.1348, 0.1443,\n",
       "         0.1782, 0.1107, 0.0524, 0.1127, 0.1650, 0.1422, 0.1245, 0.1756, 0.1706,\n",
       "         0.1485, 0.1428, 0.0866, 0.1594, 0.1379, 0.1275, 0.1045, 0.1558, 0.1392,\n",
       "         0.1160, 0.2111, 0.1513, 0.1617, 0.1224, 0.1563, 0.1486, 0.1603, 0.1409,\n",
       "         0.1350, 0.1443, 0.1310, 0.1511, 0.1225, 0.1579, 0.1299, 0.0439, 0.1735,\n",
       "         0.1307, 0.1268, 0.1716, 0.1549, 0.1275, 0.1998, 0.1566, 0.1553, 0.1398,\n",
       "         0.1381, 0.1323, 0.1297, 0.1330, 0.1276, 0.1389, 0.1624, 0.1248, 0.1260,\n",
       "         0.1244, 0.1391, 0.1580, 0.1230, 0.1488, 0.1229, 0.1266, 0.1368, 0.2375,\n",
       "         0.1376, 0.1710, 0.1362, 0.1433, 0.1449, 0.1420, 0.1428, 0.1160, 0.0967,\n",
       "         0.1318, 0.1389, 0.1325, 0.1737, 0.1580, 0.1169, 0.1403, 0.1198, 0.1484,\n",
       "         0.0579, 0.1387, 0.1541, 0.1451, 0.1229, 0.1315, 0.1129, 0.0574, 0.1449,\n",
       "         0.1410, 0.1328, 0.1064, 0.1299, 0.1456, 0.1164, 0.1307, 0.1282, 0.2094,\n",
       "         0.1519, 0.1292, 0.0757, 0.1443, 0.1536, 0.1289, 0.1109, 0.1357, 0.1428,\n",
       "         0.1279, 0.0467, 0.1214, 0.1117, 0.1270, 0.1226, 0.1991, 0.1269, 0.1184,\n",
       "         0.1440, 0.1358, 0.1426, 0.1211, 0.1538, 0.1106, 0.1211, 0.1091, 0.1492,\n",
       "         0.1288, 0.1430, 0.1270, 0.1136, 0.1606, 0.1499, 0.3033, 0.1249, 0.1194,\n",
       "         0.1234, 0.1269, 0.1448, 0.1461, 0.0971, 0.1719, 0.1231, 0.1175, 0.1676,\n",
       "         0.1360, 0.1403, 0.1199, 0.1328, 0.1303, 0.1179, 0.1511, 0.1265, 0.1372,\n",
       "         0.1213, 0.1449, 0.1697, 0.1809, 0.1701, 0.1433, 0.1425, 0.1535, 0.1268,\n",
       "         0.1500, 0.2675, 0.0647, 0.1365, 0.1690, 0.1442, 0.1158, 0.1176, 0.1369,\n",
       "         0.1166, 0.1412, 0.1203, 0.1770, 0.1514, 0.0431, 0.1405, 0.1426, 0.1523,\n",
       "         0.1593, 0.1272, 0.1418, 0.1322, 0.1306, 0.1406, 0.1596, 0.1251, 0.1275,\n",
       "         0.1557, 0.1433, 0.1622, 0.1124, 0.1108, 0.1224, 0.1647, 0.1358, 0.1596,\n",
       "         0.1206, 0.1375, 0.1226, 0.1495, 0.1395, 0.1330, 0.1290, 0.1113, 0.1390,\n",
       "         0.1670, 0.1485, 0.1247, 0.1232, 0.1462, 0.1571, 0.1719, 0.1623, 0.1565,\n",
       "         0.1226, 0.1648, 0.1702, 0.0643, 0.1305, 0.1024, 0.1530, 0.1155, 0.1312,\n",
       "         0.1411, 0.1187, 0.1136, 0.1302, 0.1392, 0.1509, 0.1478, 0.1512, 0.0426,\n",
       "         0.1171, 0.1317, 0.2156, 0.0438, 0.1314, 0.1482, 0.1316, 0.1111, 0.1332,\n",
       "         0.1365, 0.1653, 0.1551, 0.2120, 0.1169, 0.0910, 0.1323, 0.1163, 0.1387,\n",
       "         0.1661, 0.1689, 0.1596, 0.1170, 0.1597, 0.1273, 0.1541, 0.1543, 0.1699,\n",
       "         0.1257, 0.1596, 0.1743, 0.1298, 0.1191, 0.1332, 0.1882, 0.1826, 0.1475,\n",
       "         0.1249, 0.1316, 0.0904, 0.1441, 0.1813, 0.1624, 0.1260, 0.1617, 0.1273,\n",
       "         0.1398, 0.1849, 0.2270, 0.1436, 0.1406, 0.1484, 0.0814, 0.1486, 0.1466,\n",
       "         0.1151, 0.1250, 0.1199, 0.1207, 0.1641, 0.1365, 0.1236, 0.1157, 0.1370,\n",
       "         0.2503, 0.1136, 0.1625, 0.1228, 0.1369, 0.1324, 0.1282, 0.1508, 0.1718,\n",
       "         0.1182, 0.1153, 0.1544, 0.1442, 0.1385, 0.1290, 0.1019, 0.1568, 0.1588,\n",
       "         0.1354, 0.1715, 0.1234, 0.1543, 0.1650, 0.1181, 0.1133, 0.1369, 0.1696,\n",
       "         0.1307, 0.1298, 0.1256, 0.1579, 0.1944, 0.1345, 0.1324, 0.1248, 0.1634,\n",
       "         0.1397, 0.1221, 0.1253, 0.1262, 0.1369, 0.1629, 0.1153, 0.2042, 0.1359,\n",
       "         0.1271, 0.1369, 0.0908, 0.1019, 0.1308, 0.1653, 0.1230, 0.1260, 0.1266,\n",
       "         0.1167, 0.1587, 0.1363, 0.1814, 0.1568, 0.1235, 0.1629, 0.1266, 0.1315,\n",
       "         0.1312, 0.1658, 0.1232, 0.1323, 0.1325, 0.1448, 0.1485, 0.1346, 0.1533,\n",
       "         0.1146, 0.0518, 0.1309, 0.1220, 0.1721, 0.1473, 0.1297, 0.1741, 0.1204,\n",
       "         0.1055, 0.1397, 0.1175, 0.1281, 0.1368, 0.1033, 0.1295, 0.1663, 0.1626,\n",
       "         0.1270, 0.1410, 0.1525, 0.1934, 0.1344, 0.1189, 0.1440, 0.1439, 0.0543,\n",
       "         0.1352, 0.0694, 0.1306, 0.1233, 0.1429, 0.1236, 0.1588, 0.1187, 0.1214,\n",
       "         0.1068, 0.0329, 0.1306, 0.1324, 0.1558, 0.1151, 0.1535, 0.1526, 0.1122,\n",
       "         0.1300, 0.1263, 0.1587, 0.1393, 0.1283, 0.1318, 0.1552, 0.1243, 0.1406,\n",
       "         0.0983, 0.1224, 0.1354, 0.0719, 0.1583, 0.1342, 0.1130, 0.1396, 0.1388,\n",
       "         0.1204, 0.1264, 0.0996, 0.1320, 0.1255, 0.1342, 0.1169, 0.1053, 0.1621,\n",
       "         0.1730, 0.1240, 0.1312, 0.1225, 0.1493, 0.1274, 0.1266, 0.1353, 0.1245,\n",
       "         0.1528, 0.1258, 0.1200, 0.1798, 0.1382, 0.1302, 0.1163, 0.1247, 0.1279,\n",
       "         0.1566, 0.1515, 0.1515, 0.1314, 0.1329, 0.1276, 0.0372, 0.1316, 0.1146,\n",
       "         0.1602, 0.1264, 0.1313, 0.1607, 0.1657, 0.0978, 0.2020, 0.1502, 0.1347,\n",
       "         0.1421, 0.1400, 0.1235, 0.1192, 0.1659, 0.1393, 0.1239, 0.1476],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.3860,  0.0686, -0.6503,  ...,  0.3306, -0.1453, -1.1369],\n",
       "         [ 0.4448,  0.8457, -0.0029,  ..., -0.1687, -0.7736, -0.0848],\n",
       "         [-0.0973,  0.3034,  0.7044,  ...,  0.2974, -0.7951,  0.4070],\n",
       "         ...,\n",
       "         [ 1.9544,  0.2290,  0.0107,  ...,  1.2182,  0.2805,  0.9638],\n",
       "         [-1.1390, -0.1083,  0.5682,  ...,  1.3561,  0.9735, -0.9619],\n",
       "         [ 0.1820, -0.5329, -0.4642,  ...,  0.1840,  0.5028,  1.0019]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1566,  0.4882, -0.4553,  ...,  0.7128, -0.4569, -0.1256],\n",
       "         [-0.1388,  0.4376,  0.1300,  ...,  0.5277, -0.2874,  0.0274],\n",
       "         [ 0.2032,  0.3144,  0.1448,  ...,  0.2538, -0.0110,  0.0204],\n",
       "         ...,\n",
       "         [-0.2320,  0.0349,  0.5163,  ..., -0.3440,  0.1189,  0.0375],\n",
       "         [-0.1190, -0.4587,  1.0667,  ..., -0.4134, -1.0670,  0.1038],\n",
       "         [ 0.3068,  1.1531,  0.1885,  ...,  0.3371,  0.5766,  0.2789]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.6593, 0.6595, 0.5804, 0.5674, 0.7012, 0.6125, 0.7030, 0.6281, 0.6376,\n",
       "         0.5587, 0.6023, 0.5432, 0.5721, 0.5868, 0.8385, 0.5970, 0.5836, 0.5201,\n",
       "         0.6331, 0.6171, 0.6027, 0.4883, 0.6698, 1.0370, 0.5539, 0.6474, 0.6311,\n",
       "         0.6957, 0.5539, 0.5816, 0.5493, 0.5965, 0.6937, 0.5329, 0.7229, 0.6851,\n",
       "         0.5924, 0.6128, 0.4394, 0.6644, 0.6014, 0.5569, 1.4693, 0.6111, 0.5391,\n",
       "         0.5536, 0.7159, 0.6536, 0.6453, 0.5964, 0.6230, 0.6740, 0.6557, 0.4818,\n",
       "         0.5595, 0.6399, 0.5874, 0.6477, 0.5455, 0.6560, 0.5730, 0.4540, 0.6963,\n",
       "         0.5776, 0.5134, 0.6523, 0.6752, 0.5589, 0.7793, 0.6302, 0.6204, 0.6120,\n",
       "         0.5844, 0.5744, 0.5967, 0.6442, 0.5385, 0.6873, 0.6543, 0.5539, 0.5213,\n",
       "         0.7350, 0.6086, 0.6629, 0.5447, 0.6895, 0.5811, 0.4909, 0.5839, 0.9663,\n",
       "         0.6345, 0.6763, 0.6483, 0.6025, 0.6260, 0.6391, 0.6063, 0.5440, 0.5030,\n",
       "         0.5847, 0.6482, 0.5616, 0.5938, 0.5785, 0.5448, 0.6228, 0.5458, 0.5971,\n",
       "         0.4744, 0.6074, 0.6080, 0.7401, 0.5633, 0.5934, 0.5645, 0.6144, 0.6052,\n",
       "         0.6186, 0.5840, 0.5150, 0.5564, 0.5553, 0.5409, 0.5784, 0.5488, 0.8433,\n",
       "         0.4801, 0.5779, 0.5179, 0.5770, 0.6295, 0.5992, 0.6232, 0.5481, 0.6313,\n",
       "         0.5820, 0.8647, 0.5171, 0.5409, 0.5830, 0.5693, 0.8178, 0.5402, 0.5395,\n",
       "         0.6151, 0.5266, 0.6089, 1.1125, 0.5089, 0.6125, 0.5581, 0.5474, 0.5980,\n",
       "         0.5500, 0.6401, 0.5739, 0.5412, 0.6845, 0.6817, 1.2741, 0.5580, 0.4247,\n",
       "         0.5829, 0.6228, 0.6082, 0.6389, 0.4509, 0.7536, 0.5572, 0.5564, 0.6984,\n",
       "         0.5883, 0.6099, 0.5700, 0.5795, 0.5808, 0.5752, 0.5568, 0.5586, 0.5995,\n",
       "         0.5816, 0.6165, 0.6769, 0.6875, 0.7687, 0.6307, 0.6281, 0.5874, 0.5811,\n",
       "         0.6399, 1.0043, 0.2474, 0.5983, 0.6623, 0.6243, 0.5470, 0.5676, 0.6387,\n",
       "         0.5464, 0.6237, 1.0346, 0.6253, 0.5402, 1.0089, 0.6218, 0.5800, 0.5258,\n",
       "         0.5067, 0.6073, 0.5490, 0.5478, 0.5837, 0.6073, 0.6102, 0.5924, 0.5792,\n",
       "         0.6307, 0.6018, 0.6399, 0.5738, 0.5590, 0.5409, 0.6538, 0.5549, 0.7063,\n",
       "         0.5560, 0.6015, 0.5770, 0.6168, 0.9167, 0.6321, 0.5570, 0.4631, 0.5984,\n",
       "         0.6783, 0.5262, 0.7738, 0.5497, 0.5690, 0.5724, 0.6821, 0.6571, 0.5903,\n",
       "         0.5701, 0.5870, 0.6218, 0.2773, 0.6546, 0.4756, 0.6105, 0.5413, 0.6012,\n",
       "         0.5962, 0.5699, 0.5525, 0.6225, 0.6421, 0.6337, 0.6899, 0.6290, 0.9427,\n",
       "         0.5435, 0.5982, 1.0023, 0.2747, 0.5889, 0.6312, 0.5721, 0.5268, 0.5447,\n",
       "         0.6143, 0.6084, 0.5921, 0.8575, 0.5747, 2.5102, 0.6236, 0.5474, 0.6134,\n",
       "         0.6770, 0.6504, 0.5889, 0.5533, 0.6889, 0.6088, 0.6144, 0.6487, 0.5980,\n",
       "         0.5563, 0.5677, 0.6702, 0.5548, 0.5698, 0.5578, 0.6358, 0.6526, 0.6185,\n",
       "         0.5748, 0.5679, 0.5850, 0.5422, 0.6371, 0.7410, 0.5485, 0.5956, 0.5862,\n",
       "         0.6020, 0.6936, 0.8279, 0.6502, 0.5711, 0.6495, 0.4673, 0.6280, 0.6402,\n",
       "         0.5711, 0.5127, 0.5443, 0.4825, 0.6934, 0.6268, 0.5561, 0.5417, 0.5247,\n",
       "         1.0908, 0.5377, 0.6516, 0.5287, 0.5797, 0.5891, 1.1002, 0.5618, 0.7048,\n",
       "         0.5670, 0.5443, 0.6041, 0.5951, 0.6398, 0.5857, 1.8189, 0.6477, 0.6624,\n",
       "         0.6163, 0.6134, 0.5524, 0.6829, 0.6480, 0.5868, 0.5814, 0.5877, 0.6397,\n",
       "         0.6312, 0.5893, 0.5655, 0.6133, 0.6993, 0.6535, 0.6000, 0.6014, 0.5872,\n",
       "         0.5298, 0.5465, 0.5437, 0.6271, 0.6066, 0.6295, 0.5663, 0.8754, 0.6049,\n",
       "         0.5917, 0.6109, 0.4372, 0.4744, 0.5786, 0.6217, 0.5920, 0.5457, 0.5964,\n",
       "         0.5180, 0.7475, 0.5769, 0.7155, 0.6144, 0.5721, 0.6264, 0.5732, 0.5196,\n",
       "         0.5375, 0.6468, 0.5441, 0.5598, 0.6046, 0.6124, 0.6272, 0.6155, 0.5900,\n",
       "         0.5094, 0.1931, 0.5888, 0.5453, 0.6319, 0.6765, 0.5883, 0.6480, 0.5240,\n",
       "         0.5148, 0.5714, 0.5713, 0.6063, 0.5719, 0.5402, 0.6815, 0.6036, 0.6365,\n",
       "         0.5858, 0.6001, 0.6273, 0.7907, 0.5718, 0.5391, 0.6621, 0.6001, 0.2147,\n",
       "         0.6091, 0.2673, 0.6061, 0.5726, 0.6897, 0.5659, 0.5934, 0.5533, 0.5730,\n",
       "         0.5341, 0.3955, 0.5876, 0.5821, 0.6030, 0.5427, 0.6790, 0.7073, 0.5647,\n",
       "         0.5795, 0.5837, 0.6484, 0.5769, 0.5949, 0.5785, 0.5778, 0.5701, 0.5976,\n",
       "         0.5535, 0.5628, 0.5424, 0.3148, 0.6145, 0.6152, 0.5090, 0.5970, 0.5962,\n",
       "         0.6098, 0.5338, 0.4273, 0.6039, 0.5461, 0.5967, 0.5412, 0.5514, 0.7494,\n",
       "         0.6305, 0.5849, 0.6011, 0.5068, 0.6262, 0.5946, 0.6135, 0.5901, 0.5868,\n",
       "         0.6023, 0.5787, 0.5436, 0.6153, 0.6322, 0.5440, 0.5528, 0.5773, 0.5529,\n",
       "         0.6636, 0.6170, 0.6543, 0.5675, 0.5970, 0.5622, 0.1997, 0.5840, 0.5459,\n",
       "         0.6203, 0.6120, 0.5564, 0.6463, 0.6291, 0.5263, 0.7360, 0.5193, 0.5994,\n",
       "         0.5939, 1.2147, 0.4992, 0.5857, 0.6326, 0.5680, 0.5317, 0.5288],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0026, -0.0560,  0.0627,  ..., -0.0907,  0.0084,  0.0100],\n",
       "         [-0.0132,  0.0815, -0.1080,  ...,  0.1092,  0.0112, -0.1297],\n",
       "         [-0.0275,  0.0022, -0.0680,  ..., -0.0134,  0.0510, -0.0647],\n",
       "         ...,\n",
       "         [-0.0771,  0.0358, -0.0018,  ..., -0.0127, -0.0365,  0.0365],\n",
       "         [-0.0182,  0.0068, -0.0315,  ...,  0.0413, -0.0098, -0.0056],\n",
       "         [ 0.0394, -0.0557, -0.0530,  ..., -0.0587,  0.0641,  0.1569]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.0329e-03, -1.2681e-01,  5.0855e-01,  ...,  2.9244e-02,\n",
       "          -5.1212e-01,  6.2457e-01],\n",
       "         [-8.4181e-01, -1.0564e+00, -8.1337e-01,  ...,  8.5326e-01,\n",
       "           1.1125e-01,  9.1682e-01],\n",
       "         [-5.0063e-01, -9.0991e-03,  1.0742e-01,  ..., -9.4232e-01,\n",
       "          -8.8881e-01, -3.3847e-01],\n",
       "         ...,\n",
       "         [ 2.7020e-02, -5.5261e-01, -7.0932e-02,  ...,  2.7593e-01,\n",
       "           4.7561e-02,  7.1453e-01],\n",
       "         [ 8.7352e-01, -1.7737e-01, -8.2173e-01,  ..., -3.6387e-01,\n",
       "           1.0972e-01, -1.9644e-02],\n",
       "         [-1.4383e-01,  1.9887e-03, -4.5619e-01,  ...,  4.0085e-02,\n",
       "           1.0071e+00, -7.9146e-01]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.3726, -0.5529, -1.0782,  ...,  0.6002, -0.9488,  0.1055],\n",
       "         [ 1.4376,  0.0917,  0.2042,  ..., -0.4888, -0.7583,  0.0693],\n",
       "         [ 0.9823, -0.0912, -0.5127,  ...,  0.1326, -0.2793, -0.6042],\n",
       "         ...,\n",
       "         [ 0.9704, -0.1955,  0.1377,  ...,  0.2436, -0.9808,  0.2694],\n",
       "         [ 0.2785, -0.6140,  0.0719,  ..., -0.2411,  0.8584,  0.5159],\n",
       "         [ 0.6414,  0.2066,  0.9944,  ...,  0.2504,  1.5821, -0.1653]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2598,  1.3442, -0.4039,  ..., -0.1017, -0.7251,  0.0360],\n",
       "         [ 0.2601, -0.3850,  0.7874,  ..., -0.5118, -1.8808,  2.1465],\n",
       "         [-0.2008, -0.0335,  0.0131,  ...,  0.3609, -0.4927, -0.4433],\n",
       "         ...,\n",
       "         [ 0.3004, -0.6629, -0.2434,  ...,  1.3862, -0.0299,  0.5350],\n",
       "         [ 0.3019, -0.1143,  1.8312,  ..., -2.0654,  0.2339,  0.9958],\n",
       "         [-0.2624,  1.8985,  0.2650,  ..., -0.0766,  1.4209,  0.4760]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1318,  0.1494,  0.1316,  0.0937,  0.1582,  0.1349,  0.1538,  0.1365,\n",
       "          0.1203,  0.1220,  0.1233,  0.1144,  0.1187,  0.1284,  0.1523,  0.1171,\n",
       "          0.1179,  0.0882,  0.1422,  0.1315,  0.1436,  0.1009,  0.1350,  0.1297,\n",
       "          0.1125,  0.1282,  0.1298,  0.1468,  0.1173,  0.0591,  0.1172,  0.1056,\n",
       "          0.1184,  0.1103,  0.1458,  0.1439,  0.1382,  0.1227,  0.0748,  0.1270,\n",
       "          0.1388,  0.1227,  0.0945,  0.1160,  0.1051,  0.1181,  0.1387,  0.1471,\n",
       "          0.1339,  0.1219,  0.1393,  0.1106,  0.1460,  0.1141,  0.1089,  0.1363,\n",
       "          0.1331,  0.1466,  0.1203,  0.1206,  0.1276,  0.0455,  0.1626,  0.1246,\n",
       "          0.1129,  0.1402,  0.1409,  0.1082,  0.1510,  0.1318,  0.1274,  0.1208,\n",
       "          0.1182,  0.1166,  0.1261,  0.1230,  0.1239,  0.1317,  0.1556,  0.0903,\n",
       "          0.0938,  0.1003,  0.1455,  0.1414,  0.1116,  0.1452,  0.1247,  0.1059,\n",
       "          0.1212,  0.1479,  0.1337,  0.1379,  0.1252,  0.1310,  0.1193,  0.1346,\n",
       "          0.1282,  0.1099,  0.0987,  0.1087,  0.1180,  0.1173,  0.1334,  0.1176,\n",
       "          0.1168,  0.1353,  0.1210,  0.1302,  0.0560,  0.1197,  0.1443,  0.1031,\n",
       "          0.1129,  0.1274,  0.1154,  0.0525,  0.1373,  0.1250,  0.1174,  0.1120,\n",
       "          0.1225,  0.1230,  0.1121,  0.1145,  0.1244,  0.1518,  0.1194,  0.1381,\n",
       "          0.0838,  0.1331,  0.1283,  0.1257,  0.1089,  0.1117,  0.1217,  0.1184,\n",
       "          0.0452,  0.1047,  0.1053,  0.1128,  0.1181,  0.1687,  0.0915,  0.1116,\n",
       "          0.1393,  0.1252,  0.1111,  0.1032,  0.1400,  0.0996,  0.1171,  0.1074,\n",
       "          0.1155,  0.1158,  0.1390,  0.1170,  0.1160,  0.1370,  0.1418,  0.2085,\n",
       "          0.1160,  0.1105,  0.1151,  0.1247,  0.1268,  0.1289,  0.0917,  0.1535,\n",
       "          0.1085,  0.1162,  0.1379,  0.1343,  0.1225,  0.1114,  0.1166,  0.1281,\n",
       "          0.1212,  0.1249,  0.1150,  0.1275,  0.1154,  0.1313,  0.1261,  0.1397,\n",
       "          0.1314,  0.1315,  0.1418,  0.1240,  0.1154,  0.1355,  0.1803,  0.0489,\n",
       "          0.1253,  0.1535,  0.1349,  0.1115,  0.1129,  0.0955,  0.1121,  0.1409,\n",
       "          0.1041,  0.1324,  0.1212,  0.0419,  0.1248,  0.1206,  0.1315,  0.1399,\n",
       "          0.1208,  0.1298,  0.1208,  0.1265,  0.1117,  0.1335,  0.1198,  0.1159,\n",
       "          0.1372,  0.1348,  0.1211,  0.1217,  0.1193,  0.1168,  0.1334,  0.1182,\n",
       "          0.1362,  0.1139,  0.1273,  0.1082,  0.1254,  0.1086,  0.1287,  0.1141,\n",
       "          0.0940,  0.1177,  0.1194,  0.1154,  0.1066,  0.0994,  0.1153,  0.1124,\n",
       "          0.1372,  0.1283,  0.1357,  0.1231,  0.1319,  0.1189,  0.0511,  0.1025,\n",
       "          0.0817,  0.1229,  0.1080,  0.1186,  0.1139,  0.1105,  0.1009,  0.1212,\n",
       "          0.1333,  0.1332,  0.1299,  0.1329,  0.0373,  0.1132,  0.1325,  0.1623,\n",
       "          0.0453,  0.1350,  0.1269,  0.1035,  0.1115,  0.1147,  0.1279,  0.1414,\n",
       "          0.1167,  0.1775,  0.1042,  0.0813,  0.1277,  0.1113,  0.1342,  0.1417,\n",
       "          0.1400,  0.1290,  0.1108,  0.1369,  0.1155,  0.1248,  0.1297,  0.1541,\n",
       "          0.1091,  0.1191,  0.1339,  0.1121,  0.1175,  0.1296,  0.1259,  0.1324,\n",
       "          0.1133,  0.1229,  0.1214,  0.0937,  0.1145,  0.1377,  0.1191,  0.1136,\n",
       "          0.1234,  0.1193,  0.1257,  0.1407,  0.1494,  0.1271,  0.1311,  0.1382,\n",
       "          0.0743,  0.1229,  0.1398,  0.1146,  0.1037,  0.1059,  0.1040,  0.1378,\n",
       "          0.1429,  0.1103,  0.1149,  0.1060,  0.1727,  0.1055,  0.1272,  0.1155,\n",
       "          0.1304,  0.1205,  0.1173,  0.1171,  0.1370,  0.1210,  0.1050,  0.1151,\n",
       "          0.1162,  0.1236,  0.1241,  0.0873,  0.1285,  0.1313,  0.1351,  0.1295,\n",
       "          0.1345,  0.1478,  0.1267,  0.1166,  0.1095,  0.1097,  0.1243,  0.1236,\n",
       "          0.1253,  0.1126,  0.1363,  0.1359,  0.1326,  0.1303,  0.1199,  0.1104,\n",
       "          0.1129,  0.1202,  0.1192,  0.1266,  0.1408,  0.1394,  0.1148,  0.1408,\n",
       "          0.1408,  0.1191,  0.1362,  0.0800,  0.0990,  0.1135,  0.1216,  0.1188,\n",
       "          0.1239,  0.1261,  0.1040,  0.1353,  0.1303,  0.1393,  0.1335,  0.1230,\n",
       "          0.1280,  0.1276,  0.1099,  0.1247,  0.1247,  0.1169,  0.1221,  0.1189,\n",
       "          0.1324,  0.1338,  0.1320,  0.1298,  0.1098,  0.0475,  0.1175,  0.1103,\n",
       "          0.1246,  0.1135,  0.1268,  0.1195,  0.1083,  0.0935,  0.1241,  0.1165,\n",
       "          0.1179,  0.1171,  0.0808,  0.1041,  0.1342,  0.1490,  0.1134,  0.0836,\n",
       "          0.1263,  0.1409,  0.1245,  0.1147,  0.1316,  0.1153,  0.0509,  0.1290,\n",
       "          0.0585,  0.1178,  0.1145,  0.1285,  0.1157,  0.1223,  0.1248,  0.1207,\n",
       "          0.1202, -0.0326,  0.1253,  0.1324,  0.1258,  0.1126,  0.1394,  0.1247,\n",
       "          0.1195,  0.1205,  0.1257,  0.1268,  0.1365,  0.1164,  0.1101,  0.1207,\n",
       "          0.1206,  0.1296,  0.1003,  0.1094,  0.1262,  0.0567,  0.1439,  0.1154,\n",
       "          0.1083,  0.1280,  0.1201,  0.1164,  0.1105,  0.0709,  0.1268,  0.1219,\n",
       "          0.1259,  0.1104,  0.0966,  0.1540,  0.1394,  0.1284,  0.1302,  0.1083,\n",
       "          0.1373,  0.1279,  0.1240,  0.1034,  0.1196,  0.1081,  0.1246,  0.1176,\n",
       "          0.1398,  0.1345,  0.1227,  0.1166,  0.1126,  0.1220,  0.1426,  0.1461,\n",
       "          0.1447,  0.1303,  0.1122,  0.1179,  0.0314,  0.1279,  0.1176,  0.1165,\n",
       "          0.1205,  0.1154,  0.1506,  0.1346,  0.0758,  0.1527,  0.1233,  0.1266,\n",
       "          0.1369,  0.1163,  0.1044,  0.1297,  0.1413,  0.1305,  0.1094,  0.1054],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.7243,  0.9986,  0.7918,  ...,  0.0417,  0.3805, -0.1393],\n",
       "         [-0.3788,  0.5245,  0.0793,  ...,  0.1924,  0.2518,  0.0746],\n",
       "         [-0.2883, -0.8553,  0.1381,  ...,  0.1577,  0.8933,  0.7107],\n",
       "         ...,\n",
       "         [-1.2317, -0.8577, -1.2157,  ...,  1.2750,  1.6363,  0.5133],\n",
       "         [ 0.8853, -0.3281,  0.7005,  ...,  0.3913, -0.9009,  0.3669],\n",
       "         [ 0.3684, -0.4104,  0.5779,  ...,  0.7964, -0.1311,  0.2370]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.8991, -0.6731, -0.0073,  ...,  0.4855, -0.1897, -0.1615],\n",
       "         [ 0.1292, -0.1643,  0.1046,  ..., -0.4227,  0.0431, -0.7481],\n",
       "         [ 0.2259,  0.0277, -0.0215,  ...,  0.3815, -0.2714, -0.5727],\n",
       "         ...,\n",
       "         [-0.2323,  0.1718, -0.2546,  ..., -0.2788, -0.4458, -1.1717],\n",
       "         [ 0.0623, -0.2706, -0.1588,  ...,  1.4228, -0.2754,  0.7904],\n",
       "         [ 0.1494,  0.3270, -0.1452,  ..., -0.1955,  0.0896,  0.3204]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.7553, 0.7177, 0.7893, 0.6642, 0.7628, 0.8335, 0.8183, 0.6692, 0.6657,\n",
       "         0.7302, 0.8020, 0.7267, 0.7488, 0.7640, 0.7595, 0.7752, 0.7711, 0.5520,\n",
       "         0.8160, 0.8119, 0.8224, 0.6985, 0.7719, 0.9913, 0.7854, 0.8785, 0.8439,\n",
       "         0.7360, 0.7543, 0.6811, 0.7015, 0.6425, 0.9565, 0.7423, 0.7432, 0.8553,\n",
       "         0.7609, 0.8065, 0.4379, 0.8765, 0.7759, 0.7493, 1.9987, 0.6552, 0.5786,\n",
       "         0.7494, 0.6723, 0.7743, 0.7737, 0.7896, 0.7803, 0.7056, 0.7298, 0.5058,\n",
       "         0.6250, 0.7970, 0.7581, 0.8123, 0.7363, 0.6333, 0.7338, 0.5677, 0.8263,\n",
       "         0.7588, 0.5309, 0.7270, 0.8675, 0.7374, 0.7688, 0.6189, 0.6368, 0.8057,\n",
       "         0.7697, 0.7735, 0.7590, 0.8236, 0.7489, 0.8036, 0.7736, 0.6166, 0.5634,\n",
       "         0.8827, 0.7827, 0.7657, 0.7218, 0.8132, 0.7927, 0.5382, 0.8434, 0.9041,\n",
       "         0.8680, 0.7868, 0.7774, 0.7829, 0.7007, 0.7853, 0.7132, 0.7754, 0.5906,\n",
       "         0.6938, 0.8084, 0.7822, 0.7288, 0.6409, 0.7616, 0.7502, 0.7217, 0.6905,\n",
       "         0.5165, 0.7108, 0.8019, 0.7214, 0.7703, 0.8466, 0.7762, 0.8121, 0.7385,\n",
       "         0.8153, 0.8334, 0.7135, 0.7711, 0.6978, 0.7399, 0.7531, 0.7104, 0.7937,\n",
       "         0.5522, 0.7695, 0.6333, 0.6826, 0.7788, 0.7533, 0.7281, 0.7333, 0.7675,\n",
       "         0.7856, 1.0733, 0.5412, 0.7345, 0.7995, 0.7772, 1.0398, 0.5218, 0.7285,\n",
       "         0.7725, 0.6307, 0.6956, 1.2282, 0.6086, 0.6647, 0.7626, 0.7491, 0.6928,\n",
       "         0.7243, 0.8162, 0.7974, 0.7196, 0.7833, 0.7847, 1.1569, 0.7726, 0.5489,\n",
       "         0.7452, 0.8022, 0.6530, 0.8018, 0.6274, 0.8756, 0.7684, 0.7585, 0.7819,\n",
       "         0.7963, 0.7978, 0.7366, 0.7397, 0.6991, 0.7605, 0.6611, 0.7547, 0.7656,\n",
       "         0.8039, 0.7394, 0.7585, 0.7510, 0.7240, 0.8177, 0.8629, 0.7605, 0.7908,\n",
       "         0.6901, 0.9399, 0.3281, 0.7875, 0.7504, 0.7725, 0.7779, 0.7260, 0.7330,\n",
       "         0.7234, 0.8501, 1.1290, 0.6974, 0.5556, 1.0879, 0.7727, 0.7216, 0.5640,\n",
       "         0.5773, 0.7321, 0.8094, 0.7103, 0.7932, 0.7004, 0.6454, 0.7838, 0.7821,\n",
       "         0.7293, 0.7156, 0.6409, 0.7657, 0.7298, 0.7467, 0.7331, 0.7330, 0.7286,\n",
       "         0.7605, 0.8080, 0.7588, 0.6631, 0.9428, 0.8429, 0.7502, 0.5273, 0.7066,\n",
       "         0.8037, 0.5710, 1.0023, 0.6614, 0.7484, 0.7556, 0.7328, 0.7745, 0.7196,\n",
       "         0.7736, 0.6758, 0.6663, 0.4454, 0.8367, 0.5948, 0.7252, 0.7645, 0.7308,\n",
       "         0.6711, 0.7710, 0.7729, 0.7280, 0.7333, 0.7454, 0.7610, 0.7013, 1.3978,\n",
       "         0.7515, 0.8092, 0.9416, 0.3559, 0.8195, 0.7255, 0.7563, 0.6651, 0.6421,\n",
       "         0.8233, 0.7223, 0.6970, 0.9917, 0.7338, 4.0950, 0.7769, 0.7947, 0.7740,\n",
       "         0.7740, 0.6988, 0.7005, 0.7573, 0.7729, 0.8107, 0.7563, 0.7037, 0.6627,\n",
       "         0.7273, 0.6141, 0.6989, 0.7748, 0.7386, 0.8152, 0.6602, 0.6877, 0.7187,\n",
       "         0.7618, 0.7427, 0.5804, 0.5848, 0.7033, 0.6959, 0.7641, 0.6667, 0.7928,\n",
       "         0.7346, 0.7099, 0.8373, 0.7915, 0.7279, 0.8040, 0.5790, 0.7868, 0.7306,\n",
       "         0.7780, 0.7157, 0.7389, 0.5648, 0.7504, 0.7582, 0.7702, 0.7400, 0.6135,\n",
       "         1.0086, 0.7264, 0.7227, 0.7178, 0.8008, 0.6431, 1.2788, 0.6354, 0.8455,\n",
       "         0.7450, 0.7366, 0.6024, 0.5933, 0.8662, 0.7736, 2.6405, 0.7536, 0.7446,\n",
       "         0.8095, 0.7073, 0.7695, 0.7929, 0.7187, 0.7807, 0.7616, 0.5797, 0.9049,\n",
       "         0.8536, 0.8094, 0.7291, 0.7337, 0.7959, 0.8160, 0.7803, 0.7765, 0.6365,\n",
       "         0.6274, 0.7252, 0.7249, 0.7600, 0.7395, 0.7720, 0.7690, 0.8239, 0.8093,\n",
       "         0.7638, 0.7937, 0.4418, 0.6079, 0.7764, 0.7330, 0.7596, 0.7377, 0.7727,\n",
       "         0.7251, 0.8264, 0.7631, 0.7579, 0.6955, 0.7494, 0.6957, 0.7029, 0.6155,\n",
       "         0.7243, 0.6829, 0.7599, 0.7514, 0.8243, 0.7536, 0.7885, 0.7833, 0.6764,\n",
       "         0.6554, 0.2676, 0.7385, 0.6550, 0.6877, 0.7211, 0.7692, 0.6668, 0.6924,\n",
       "         0.5288, 0.7652, 0.7744, 0.7678, 0.7780, 0.6444, 0.7249, 0.6813, 0.6811,\n",
       "         0.7901, 0.6045, 0.6934, 0.8902, 0.7563, 0.7190, 0.7929, 0.6896, 0.2850,\n",
       "         0.7768, 0.3441, 0.8096, 0.7790, 0.8971, 0.7472, 0.6784, 0.7555, 0.7935,\n",
       "         0.7436, 0.4946, 0.7592, 0.7831, 0.6894, 0.7226, 0.8378, 0.8915, 0.7644,\n",
       "         0.7624, 0.7718, 0.7118, 0.7466, 0.7697, 0.7662, 0.6272, 0.7849, 0.8076,\n",
       "         0.7371, 0.6927, 0.6627, 0.3862, 0.6782, 0.7571, 0.6743, 0.7373, 0.7655,\n",
       "         0.7990, 0.7781, 0.4825, 0.7522, 0.7525, 0.8116, 0.7282, 0.7393, 0.9317,\n",
       "         0.7694, 0.7848, 0.8013, 0.7142, 0.8438, 0.7864, 0.7863, 0.6308, 0.7715,\n",
       "         0.6285, 0.7422, 0.7686, 0.6826, 0.7204, 0.7363, 0.7813, 0.7842, 0.7048,\n",
       "         0.8107, 0.7810, 0.8336, 0.7394, 0.6331, 0.7896, 0.2200, 0.7655, 0.7481,\n",
       "         0.6477, 0.8121, 0.7095, 0.7983, 0.7198, 0.6985, 0.7148, 0.6079, 0.8054,\n",
       "         0.7400, 1.4694, 0.5529, 0.7402, 0.6929, 0.7410, 0.6376, 0.5834],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0524, -0.0434,  0.0143,  ...,  0.0119, -0.0078,  0.0073],\n",
       "         [ 0.0058, -0.0184,  0.0028,  ..., -0.0385,  0.0051, -0.0959],\n",
       "         [ 0.0165, -0.0344, -0.0164,  ..., -0.0422,  0.0148,  0.0107],\n",
       "         ...,\n",
       "         [ 0.0202,  0.0720, -0.0339,  ...,  0.0605, -0.0149, -0.0444],\n",
       "         [-0.0093,  0.0605,  0.0299,  ...,  0.0144, -0.0068, -0.0209],\n",
       "         [-0.0022,  0.0200,  0.0427,  ..., -0.0432, -0.0432,  0.0088]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0552,  0.7921,  0.1433,  ..., -0.3012, -0.2536,  0.0357],\n",
       "         [ 0.2233,  0.2694,  0.1865,  ...,  0.1925, -0.3136, -0.1196],\n",
       "         [ 0.0947, -0.0613,  0.1293,  ..., -0.1924, -0.3116, -0.4761],\n",
       "         ...,\n",
       "         [ 0.1238, -0.2752,  0.0289,  ..., -0.7223,  0.1506, -0.1231],\n",
       "         [ 0.4635, -0.2166,  0.0097,  ...,  0.2986, -0.2926, -0.1265],\n",
       "         [ 0.0726, -0.3540,  0.0520,  ..., -0.2138,  0.3812, -0.4898]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.0970, -0.3443,  1.1889,  ..., -1.0937,  0.9033,  1.0970],\n",
       "         [-0.2591,  1.0441, -0.1598,  ..., -0.6350,  0.1636,  0.5455],\n",
       "         [-0.8565,  1.1293,  0.2206,  ...,  1.4529,  1.0450,  0.7629],\n",
       "         ...,\n",
       "         [-1.3315,  0.1574, -0.1964,  ...,  0.0696,  0.7314,  1.7138],\n",
       "         [ 0.9212,  0.4560, -0.4599,  ..., -0.5673, -0.0581,  0.1226],\n",
       "         [-1.2279,  0.0237, -0.3236,  ..., -0.5452, -0.9142, -0.2905]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1677,  0.4255,  0.2191,  ..., -1.4003,  0.1753,  0.3326],\n",
       "         [ 1.4664, -0.3782, -0.4887,  ..., -0.0071, -2.6886, -0.4595],\n",
       "         [ 0.2616,  0.5186, -0.3139,  ...,  0.6527, -0.5173, -0.0728],\n",
       "         ...,\n",
       "         [-0.3819, -0.5426, -1.1225,  ...,  0.1752,  1.7035, -0.0730],\n",
       "         [ 0.8641, -0.2245, -0.1791,  ...,  0.5717, -0.5647, -0.4347],\n",
       "         [-1.7996,  0.0209,  0.7186,  ..., -0.8753, -0.3818, -0.1988]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1243, 0.1320, 0.1344, 0.0910, 0.1316, 0.1209, 0.1540, 0.1240, 0.1186,\n",
       "         0.1211, 0.1261, 0.1063, 0.1240, 0.1374, 0.1330, 0.1314, 0.1290, 0.0736,\n",
       "         0.1239, 0.1160, 0.1322, 0.1030, 0.1250, 0.1216, 0.1161, 0.1288, 0.1334,\n",
       "         0.1310, 0.1172, 0.0534, 0.1102, 0.1073, 0.1042, 0.1064, 0.1371, 0.1260,\n",
       "         0.1274, 0.1254, 0.0713, 0.1278, 0.1437, 0.1277, 0.1035, 0.1190, 0.0960,\n",
       "         0.1111, 0.1220, 0.1411, 0.1319, 0.1161, 0.1538, 0.1101, 0.1408, 0.0976,\n",
       "         0.1040, 0.1292, 0.1433, 0.1439, 0.1226, 0.1109, 0.1311, 0.0434, 0.1545,\n",
       "         0.1198, 0.1100, 0.1353, 0.1475, 0.1096, 0.1424, 0.1236, 0.1176, 0.1279,\n",
       "         0.1293, 0.1156, 0.1301, 0.1280, 0.1208, 0.1370, 0.1368, 0.0889, 0.0811,\n",
       "         0.1098, 0.1474, 0.1496, 0.1131, 0.1277, 0.1250, 0.0995, 0.1214, 0.1355,\n",
       "         0.1231, 0.1319, 0.1251, 0.1433, 0.1199, 0.1347, 0.1303, 0.1123, 0.0983,\n",
       "         0.1130, 0.1189, 0.1159, 0.1177, 0.1115, 0.1277, 0.1260, 0.1229, 0.1209,\n",
       "         0.0502, 0.1187, 0.1388, 0.1048, 0.1193, 0.1302, 0.1089, 0.0592, 0.1312,\n",
       "         0.1294, 0.1311, 0.1113, 0.1305, 0.1328, 0.1068, 0.1240, 0.1245, 0.1301,\n",
       "         0.1169, 0.1302, 0.0732, 0.1230, 0.1351, 0.1159, 0.0996, 0.1062, 0.1154,\n",
       "         0.1245, 0.0410, 0.1170, 0.0946, 0.1008, 0.1270, 0.1470, 0.0811, 0.1169,\n",
       "         0.1276, 0.1139, 0.1178, 0.0946, 0.1400, 0.0826, 0.1120, 0.1094, 0.1092,\n",
       "         0.1186, 0.1359, 0.1211, 0.1079, 0.1283, 0.1360, 0.1767, 0.1173, 0.1054,\n",
       "         0.1332, 0.1318, 0.1161, 0.1345, 0.0899, 0.1457, 0.1146, 0.1130, 0.1193,\n",
       "         0.1385, 0.1252, 0.1130, 0.1260, 0.1272, 0.1234, 0.1020, 0.1197, 0.1267,\n",
       "         0.1232, 0.1318, 0.0999, 0.1221, 0.1222, 0.1282, 0.1309, 0.1043, 0.1181,\n",
       "         0.1353, 0.1538, 0.0558, 0.1276, 0.1526, 0.1226, 0.1144, 0.1174, 0.0852,\n",
       "         0.1122, 0.1338, 0.0893, 0.1240, 0.0892, 0.0369, 0.1369, 0.1217, 0.1274,\n",
       "         0.1057, 0.0996, 0.1195, 0.1208, 0.1207, 0.1106, 0.1183, 0.1125, 0.1157,\n",
       "         0.1383, 0.1284, 0.1039, 0.1160, 0.1239, 0.1117, 0.1268, 0.1143, 0.1174,\n",
       "         0.1191, 0.1245, 0.1097, 0.1239, 0.1063, 0.1228, 0.1177, 0.0850, 0.1266,\n",
       "         0.1252, 0.1028, 0.0990, 0.1171, 0.1186, 0.1027, 0.1340, 0.1342, 0.1444,\n",
       "         0.1193, 0.1238, 0.1169, 0.0533, 0.0930, 0.0835, 0.1261, 0.1227, 0.1267,\n",
       "         0.1117, 0.1158, 0.1237, 0.1299, 0.1336, 0.1369, 0.1248, 0.1116, 0.0314,\n",
       "         0.1165, 0.1227, 0.0895, 0.0435, 0.1304, 0.1253, 0.1177, 0.1065, 0.1118,\n",
       "         0.1249, 0.1365, 0.1117, 0.1603, 0.1099, 0.0686, 0.1261, 0.1190, 0.1259,\n",
       "         0.1351, 0.1383, 0.1171, 0.1175, 0.1488, 0.1287, 0.1425, 0.1218, 0.1278,\n",
       "         0.1154, 0.1155, 0.1112, 0.1139, 0.1147, 0.1285, 0.1222, 0.1249, 0.1075,\n",
       "         0.1168, 0.1192, 0.0882, 0.1011, 0.1518, 0.1122, 0.1157, 0.1301, 0.1199,\n",
       "         0.1247, 0.1439, 0.1441, 0.1235, 0.1281, 0.1362, 0.0777, 0.1318, 0.1371,\n",
       "         0.1171, 0.1111, 0.1225, 0.1132, 0.1300, 0.1299, 0.1140, 0.1123, 0.1105,\n",
       "         0.1484, 0.1092, 0.1266, 0.1198, 0.1325, 0.1112, 0.1108, 0.1067, 0.1452,\n",
       "         0.1266, 0.1111, 0.1038, 0.1062, 0.1354, 0.1263, 0.0873, 0.1236, 0.1360,\n",
       "         0.1410, 0.1260, 0.1194, 0.1390, 0.1170, 0.1169, 0.1072, 0.1008, 0.1086,\n",
       "         0.1282, 0.1248, 0.1250, 0.1372, 0.1143, 0.1361, 0.1337, 0.1227, 0.1125,\n",
       "         0.1180, 0.1086, 0.1250, 0.1233, 0.1345, 0.1374, 0.1258, 0.1204, 0.1287,\n",
       "         0.1300, 0.1261, 0.0698, 0.1069, 0.1271, 0.1217, 0.1227, 0.1269, 0.1254,\n",
       "         0.1166, 0.1270, 0.1264, 0.1393, 0.1262, 0.1314, 0.1193, 0.1173, 0.1251,\n",
       "         0.1214, 0.1081, 0.1166, 0.1246, 0.1214, 0.1263, 0.1278, 0.1419, 0.1285,\n",
       "         0.1191, 0.0547, 0.1169, 0.1033, 0.1258, 0.1147, 0.1368, 0.1237, 0.1092,\n",
       "         0.0974, 0.1262, 0.1166, 0.1212, 0.1335, 0.0842, 0.0833, 0.1268, 0.1213,\n",
       "         0.1134, 0.0689, 0.1187, 0.1405, 0.1355, 0.1158, 0.1370, 0.1230, 0.0629,\n",
       "         0.1240, 0.0672, 0.1122, 0.1118, 0.1263, 0.1138, 0.1193, 0.1161, 0.1182,\n",
       "         0.1109, 0.0261, 0.1188, 0.1232, 0.1211, 0.1130, 0.1489, 0.1235, 0.1124,\n",
       "         0.1270, 0.1102, 0.1302, 0.1373, 0.1190, 0.1062, 0.1172, 0.1209, 0.1281,\n",
       "         0.1007, 0.1120, 0.1194, 0.0620, 0.1222, 0.1156, 0.1143, 0.1293, 0.1381,\n",
       "         0.1188, 0.1124, 0.0797, 0.1237, 0.1268, 0.1262, 0.1136, 0.1156, 0.1508,\n",
       "         0.1518, 0.1223, 0.1411, 0.1082, 0.1333, 0.1277, 0.1440, 0.1000, 0.1273,\n",
       "         0.1091, 0.1201, 0.1215, 0.1298, 0.1397, 0.1075, 0.1124, 0.1082, 0.1163,\n",
       "         0.1387, 0.1338, 0.1439, 0.1294, 0.0989, 0.1200, 0.0281, 0.1394, 0.1202,\n",
       "         0.1094, 0.1192, 0.1138, 0.1462, 0.1172, 0.0678, 0.1314, 0.1084, 0.1326,\n",
       "         0.1235, 0.1056, 0.0838, 0.1299, 0.1421, 0.1299, 0.1108, 0.1100],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2651,  0.6322,  0.2563,  ..., -0.5595, -1.3406,  0.1039],\n",
       "         [-0.3413, -0.2831, -0.2071,  ...,  0.0747,  0.2037, -0.2777],\n",
       "         [ 1.2796, -0.3246, -0.9628,  ...,  0.7252, -0.5882,  0.2703],\n",
       "         ...,\n",
       "         [-1.0941,  0.3550, -0.0963,  ..., -0.9419,  0.8522,  0.1104],\n",
       "         [ 0.5472, -0.6645,  0.6100,  ..., -0.3692, -1.0576, -0.3043],\n",
       "         [-0.6170, -0.0989,  0.9976,  ..., -0.4377, -0.5241,  0.5273]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0859, -0.4869,  0.9303,  ...,  0.0716,  0.7151,  0.4276],\n",
       "         [-0.3918, -0.4296, -1.2188,  ..., -0.1212,  0.0736, -0.1465],\n",
       "         [-0.0593, -0.2094, -0.1684,  ..., -0.2052, -0.6137,  0.5682],\n",
       "         ...,\n",
       "         [ 0.4990,  0.3398, -0.2486,  ..., -0.3009,  0.7235,  0.4206],\n",
       "         [ 0.3029,  0.1940,  0.3458,  ..., -0.0779,  0.0060,  0.5957],\n",
       "         [-0.4367,  0.3734, -0.5485,  ...,  0.2143, -0.5004, -0.4899]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.7767, 0.6689, 0.7915, 0.5729, 0.6511, 0.8824, 0.7998, 0.6140, 0.6170,\n",
       "         0.7968, 0.8590, 0.5777, 0.8405, 0.8030, 0.6817, 0.7982, 0.7735, 0.4869,\n",
       "         0.7518, 0.8471, 0.8783, 0.7445, 0.6176, 0.7809, 0.7997, 0.8302, 0.8649,\n",
       "         0.6633, 0.8191, 0.5848, 0.8266, 0.5729, 0.9958, 0.8541, 0.6915, 0.6684,\n",
       "         0.8183, 0.8596, 0.4251, 0.8037, 0.8226, 0.8422, 2.2867, 0.5261, 0.5316,\n",
       "         0.8265, 0.5564, 0.7887, 0.8276, 0.8328, 0.8210, 0.6124, 0.7155, 0.4796,\n",
       "         0.6199, 0.8407, 0.7954, 0.7792, 0.8095, 0.5093, 0.7739, 0.5664, 0.7803,\n",
       "         0.7969, 0.5148, 0.7211, 0.8582, 0.8030, 0.6755, 0.4380, 0.5802, 0.8397,\n",
       "         0.8087, 0.8577, 0.7995, 0.8249, 0.8127, 0.7795, 0.7685, 0.5444, 0.5221,\n",
       "         0.8868, 0.8270, 0.7503, 0.7922, 0.7411, 0.8286, 0.5102, 0.8357, 0.8278,\n",
       "         0.9138, 0.7346, 0.8452, 0.7733, 0.6413, 0.8612, 0.6416, 0.8359, 0.5526,\n",
       "         0.7344, 0.8270, 0.8864, 0.7257, 0.5607, 0.8333, 0.7973, 0.8194, 0.6384,\n",
       "         0.4434, 0.7083, 0.7847, 0.6448, 0.8159, 0.7960, 0.8343, 0.7373, 0.8004,\n",
       "         0.8201, 0.8865, 0.7819, 0.8534, 0.7318, 0.7761, 0.7944, 0.8024, 0.6984,\n",
       "         0.4874, 0.8077, 0.5929, 0.7048, 0.7688, 0.8502, 0.5880, 0.5381, 0.7062,\n",
       "         0.8823, 0.8497, 0.5135, 0.7885, 0.8298, 0.8013, 0.9382, 0.4228, 0.7860,\n",
       "         0.7781, 0.6077, 0.7042, 1.2874, 0.6349, 1.0513, 0.8513, 0.8017, 0.5933,\n",
       "         0.8072, 0.8025, 0.7958, 0.8433, 0.7069, 0.7955, 0.8984, 0.8097, 0.5411,\n",
       "         0.8470, 0.8979, 0.5913, 0.8018, 0.5795, 0.8033, 0.8742, 0.8347, 0.7286,\n",
       "         0.8345, 0.7967, 0.7975, 0.7853, 0.6960, 0.8054, 0.6109, 0.8487, 0.7942,\n",
       "         0.8839, 0.7247, 0.6705, 0.6543, 0.6610, 0.8617, 0.8930, 0.6082, 0.8671,\n",
       "         0.6592, 0.7500, 0.3547, 0.8653, 0.7046, 0.8297, 0.8397, 0.8169, 0.6984,\n",
       "         0.8053, 0.8881, 0.9252, 0.5873, 0.4981, 0.9435, 0.8127, 0.7078, 0.5062,\n",
       "         0.6467, 0.6389, 0.8745, 0.7900, 0.8045, 0.5965, 0.6063, 0.8585, 0.8484,\n",
       "         0.7501, 0.7034, 0.5949, 0.8283, 0.8141, 0.8147, 0.6311, 0.7973, 0.6860,\n",
       "         0.8185, 0.8128, 0.8569, 0.5999, 0.7855, 0.8620, 0.7963, 0.4829, 0.7022,\n",
       "         0.7039, 0.5218, 0.9845, 0.6224, 0.8164, 0.5990, 0.7075, 0.6484, 0.6549,\n",
       "         0.8246, 0.6381, 0.5957, 0.4772, 0.6144, 0.5611, 0.6407, 0.8175, 0.7711,\n",
       "         0.5767, 0.8841, 0.8298, 0.7486, 0.7494, 0.7417, 0.6896, 0.6428, 1.6165,\n",
       "         0.8089, 0.8529, 0.8370, 0.3727, 0.8800, 0.7588, 0.8239, 0.7191, 0.6965,\n",
       "         0.8088, 0.6767, 0.5960, 1.0195, 0.8017, 4.7607, 0.8376, 0.8452, 0.7472,\n",
       "         0.7614, 0.7199, 0.6737, 0.7627, 0.7771, 0.8136, 0.7171, 0.6801, 0.6507,\n",
       "         0.8026, 0.5411, 0.6503, 0.8009, 0.8048, 0.8459, 0.6077, 0.5719, 0.6709,\n",
       "         0.8156, 0.7953, 0.4783, 0.4790, 0.6752, 0.5070, 0.7964, 0.6253, 0.8413,\n",
       "         0.7673, 0.6379, 0.6635, 0.6293, 0.7744, 0.8172, 0.5074, 0.7354, 0.7260,\n",
       "         0.8794, 0.7953, 0.8115, 0.5885, 0.7087, 0.8128, 0.8430, 0.7860, 0.5326,\n",
       "         0.7769, 0.7962, 0.6607, 0.7812, 0.8585, 0.6303, 1.0296, 0.5418, 0.7641,\n",
       "         0.7997, 0.7848, 0.4817, 0.5612, 0.7900, 0.8275, 2.9766, 0.6897, 0.7027,\n",
       "         0.9066, 0.5939, 0.7991, 0.7634, 0.6097, 0.8297, 0.8753, 0.4705, 0.6972,\n",
       "         0.8921, 0.8259, 0.8545, 0.7856, 0.6009, 0.8366, 0.8391, 0.7804, 0.5831,\n",
       "         0.5363, 0.8345, 0.8215, 0.8176, 0.8250, 0.7895, 0.8022, 0.6399, 0.8222,\n",
       "         0.8159, 0.8984, 0.4228, 0.6168, 0.8333, 0.6198, 0.8495, 0.8223, 0.8442,\n",
       "         0.7735, 0.7395, 0.7920, 0.6196, 0.6503, 0.8379, 0.6632, 0.7437, 0.6123,\n",
       "         0.7974, 0.6378, 0.7906, 0.8012, 0.8310, 0.7722, 0.8205, 0.7423, 0.6334,\n",
       "         0.6862, 0.2966, 0.7963, 0.6095, 0.6030, 0.6335, 0.7990, 0.5831, 0.7833,\n",
       "         0.4808, 0.7874, 0.7691, 0.8561, 0.8556, 0.6227, 0.6377, 0.6013, 0.6240,\n",
       "         0.8319, 0.4997, 0.6584, 0.7655, 0.8044, 0.8141, 0.8374, 0.6789, 0.3181,\n",
       "         0.7941, 0.3501, 0.8700, 0.8235, 0.8700, 0.8337, 0.6284, 0.8224, 0.8397,\n",
       "         0.8297, 0.4724, 0.8117, 0.8029, 0.6568, 0.7765, 0.8318, 0.6944, 0.8076,\n",
       "         0.8189, 0.8061, 0.6532, 0.8121, 0.8122, 0.8610, 0.6007, 0.8463, 0.7768,\n",
       "         0.7872, 0.7591, 0.6492, 0.3753, 0.6238, 0.7243, 0.7639, 0.7866, 0.7507,\n",
       "         0.8147, 0.8168, 0.4651, 0.8947, 0.8062, 0.8499, 0.7882, 0.8209, 0.9494,\n",
       "         0.7807, 0.8326, 0.8400, 0.7694, 0.7971, 0.8901, 0.8343, 0.5577, 0.7920,\n",
       "         0.5922, 0.8098, 0.8203, 0.6686, 0.7640, 0.8351, 0.8071, 0.8379, 0.8395,\n",
       "         0.8510, 0.7741, 0.8590, 0.7993, 0.7038, 0.8251, 0.2348, 0.8683, 0.8338,\n",
       "         0.5999, 0.8544, 0.6145, 0.8123, 0.6574, 0.4924, 0.6928, 0.5020, 0.8824,\n",
       "         0.7407, 1.7781, 0.5092, 0.7786, 0.6548, 0.7653, 0.6593, 0.5773],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0058,  0.0202, -0.0481,  ...,  0.0416, -0.0718, -0.0365],\n",
       "         [-0.0381, -0.0092,  0.0187,  ...,  0.0033, -0.0315, -0.0851],\n",
       "         [ 0.0088, -0.0429,  0.0939,  ...,  0.0065,  0.0178, -0.0483],\n",
       "         ...,\n",
       "         [-0.0010, -0.0361, -0.0488,  ..., -0.0045,  0.1628,  0.0385],\n",
       "         [ 0.0929, -0.0827, -0.0351,  ...,  0.0216,  0.0349, -0.0062],\n",
       "         [ 0.0501, -0.0819, -0.0277,  ...,  0.1180, -0.0318,  0.1060]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.3388,  0.2307,  0.5954,  ..., -0.0692, -0.1929, -0.1848],\n",
       "         [ 1.0009,  0.6013,  0.6180,  ..., -0.3921, -0.6978, -0.3835],\n",
       "         [-0.1793, -0.4470,  0.3180,  ..., -0.1809,  0.8654, -0.0018],\n",
       "         ...,\n",
       "         [-0.1075, -0.1979,  0.2458,  ..., -0.0162, -0.2362,  0.1880],\n",
       "         [-0.7208, -1.2150, -0.1492,  ..., -0.4539, -0.2563, -0.7628],\n",
       "         [-0.5544, -0.6433,  0.5127,  ..., -0.0992, -0.7974, -0.6239]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.4122, -0.4149,  0.7287,  ...,  0.6228,  0.1252, -0.6002],\n",
       "         [-0.1648,  0.3578, -0.0784,  ...,  1.6630,  0.4075, -0.6907],\n",
       "         [ 0.2532,  1.0258, -0.0082,  ..., -0.9621, -0.8213,  0.4798],\n",
       "         ...,\n",
       "         [ 0.8455, -0.4213,  0.9142,  ..., -1.5999,  0.2689,  0.0309],\n",
       "         [ 0.6330, -0.2131,  1.6511,  ...,  0.4269, -0.6175, -1.0265],\n",
       "         [ 0.4511, -0.2814, -0.2982,  ...,  0.7648, -0.9339, -0.3715]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.6724,  0.7003, -0.3438,  ...,  1.9462,  0.5659, -0.2322],\n",
       "         [-1.3051,  0.6025,  0.0423,  ..., -0.8326, -3.1166, -0.4471],\n",
       "         [-0.1705, -0.2448,  1.1132,  ...,  1.2362,  0.7762,  2.3385],\n",
       "         ...,\n",
       "         [ 0.3768,  0.4410,  0.1471,  ..., -0.9025,  1.5609,  1.5885],\n",
       "         [ 0.3513,  0.8435, -0.1047,  ..., -1.4496,  0.7176, -0.9240],\n",
       "         [-0.0812, -0.4878,  0.3421,  ..., -2.7993, -1.0165, -2.0574]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1300, 0.1266, 0.1360, 0.0938, 0.1362, 0.1338, 0.1581, 0.1175, 0.1079,\n",
       "         0.1294, 0.1249, 0.1077, 0.1429, 0.1464, 0.1384, 0.1299, 0.1259, 0.0590,\n",
       "         0.1456, 0.1273, 0.1479, 0.1126, 0.1121, 0.1096, 0.1148, 0.1343, 0.1388,\n",
       "         0.1310, 0.1361, 0.0409, 0.1250, 0.1145, 0.1049, 0.1390, 0.1383, 0.1201,\n",
       "         0.1314, 0.1329, 0.0595, 0.1358, 0.1425, 0.1426, 0.0984, 0.0910, 0.0932,\n",
       "         0.1156, 0.1139, 0.1349, 0.1370, 0.1482, 0.1496, 0.1098, 0.1374, 0.0970,\n",
       "         0.1119, 0.1371, 0.1443, 0.1469, 0.1208, 0.0828, 0.1444, 0.0366, 0.1303,\n",
       "         0.1334, 0.1124, 0.1296, 0.1521, 0.1164, 0.1440, 0.0994, 0.1119, 0.1381,\n",
       "         0.1231, 0.1391, 0.1319, 0.1288, 0.1318, 0.1320, 0.1380, 0.0929, 0.0883,\n",
       "         0.0909, 0.1415, 0.1346, 0.1172, 0.1196, 0.1240, 0.0868, 0.1224, 0.1215,\n",
       "         0.1419, 0.1306, 0.1188, 0.1262, 0.1150, 0.1479, 0.1204, 0.1089, 0.1001,\n",
       "         0.1169, 0.1318, 0.1330, 0.1202, 0.1179, 0.1324, 0.1355, 0.1242, 0.1227,\n",
       "         0.0537, 0.1198, 0.1583, 0.0956, 0.1252, 0.1358, 0.1313, 0.0555, 0.1400,\n",
       "         0.1402, 0.1367, 0.1243, 0.1247, 0.1285, 0.1250, 0.1173, 0.1355, 0.1296,\n",
       "         0.1016, 0.1386, 0.0566, 0.1293, 0.1367, 0.1392, 0.1223, 0.1150, 0.1180,\n",
       "         0.1366, 0.0311, 0.1185, 0.1043, 0.1249, 0.1321, 0.1226, 0.0855, 0.1174,\n",
       "         0.1308, 0.1067, 0.1122, 0.0852, 0.1336, 0.0776, 0.1303, 0.1114, 0.1089,\n",
       "         0.1258, 0.1298, 0.1291, 0.1208, 0.1260, 0.1427, 0.1565, 0.1198, 0.1020,\n",
       "         0.1370, 0.1341, 0.1100, 0.1342, 0.0940, 0.1487, 0.1115, 0.1144, 0.1419,\n",
       "         0.1393, 0.1313, 0.1138, 0.1316, 0.1291, 0.1262, 0.1159, 0.1203, 0.1324,\n",
       "         0.1295, 0.1298, 0.0989, 0.1213, 0.1043, 0.1246, 0.1486, 0.1105, 0.1355,\n",
       "         0.1231, 0.1448, 0.0538, 0.1347, 0.1392, 0.1391, 0.1319, 0.1183, 0.1021,\n",
       "         0.1218, 0.1505, 0.0911, 0.1141, 0.1049, 0.0302, 0.1309, 0.1338, 0.1088,\n",
       "         0.0948, 0.1210, 0.1315, 0.1282, 0.1199, 0.1077, 0.1171, 0.1300, 0.1270,\n",
       "         0.1320, 0.1323, 0.1061, 0.1189, 0.1294, 0.1193, 0.1237, 0.1217, 0.1131,\n",
       "         0.1229, 0.1295, 0.1202, 0.1231, 0.1177, 0.1233, 0.1211, 0.0846, 0.1112,\n",
       "         0.1145, 0.0987, 0.0894, 0.1231, 0.1162, 0.1039, 0.1299, 0.1226, 0.1314,\n",
       "         0.1252, 0.1205, 0.1062, 0.0551, 0.0825, 0.0961, 0.1168, 0.1105, 0.1224,\n",
       "         0.1057, 0.1316, 0.1341, 0.1240, 0.1423, 0.1370, 0.1079, 0.1138, 0.0273,\n",
       "         0.1292, 0.1401, 0.0787, 0.0501, 0.1427, 0.1245, 0.1358, 0.1166, 0.1111,\n",
       "         0.1350, 0.1374, 0.1210, 0.1672, 0.1212, 0.0519, 0.1430, 0.1363, 0.1181,\n",
       "         0.1364, 0.1222, 0.1306, 0.1277, 0.1412, 0.1358, 0.1315, 0.1235, 0.1201,\n",
       "         0.1270, 0.1133, 0.1220, 0.1400, 0.1361, 0.1205, 0.1206, 0.1207, 0.1173,\n",
       "         0.1252, 0.1358, 0.0857, 0.1010, 0.1292, 0.0963, 0.1362, 0.1102, 0.1290,\n",
       "         0.1388, 0.1275, 0.1232, 0.1220, 0.1350, 0.1419, 0.0709, 0.1167, 0.1323,\n",
       "         0.1309, 0.1201, 0.1243, 0.1018, 0.1335, 0.1482, 0.1195, 0.1191, 0.0948,\n",
       "         0.1405, 0.1210, 0.1257, 0.1387, 0.1264, 0.1113, 0.1098, 0.1074, 0.1308,\n",
       "         0.1316, 0.1203, 0.1089, 0.1064, 0.1454, 0.1266, 0.0879, 0.1154, 0.1242,\n",
       "         0.1558, 0.1319, 0.1211, 0.1263, 0.1109, 0.1160, 0.1262, 0.0871, 0.1164,\n",
       "         0.1334, 0.1280, 0.1252, 0.1386, 0.0947, 0.1399, 0.1354, 0.1238, 0.1072,\n",
       "         0.1037, 0.1225, 0.1291, 0.1364, 0.1371, 0.1293, 0.1317, 0.1032, 0.1399,\n",
       "         0.1299, 0.1490, 0.0666, 0.1108, 0.1245, 0.1133, 0.1320, 0.1362, 0.1241,\n",
       "         0.1105, 0.1243, 0.1493, 0.1370, 0.1291, 0.1309, 0.1202, 0.1261, 0.1161,\n",
       "         0.1240, 0.0997, 0.1307, 0.1240, 0.1241, 0.1387, 0.1287, 0.1382, 0.1262,\n",
       "         0.1277, 0.0470, 0.1318, 0.0757, 0.1203, 0.1145, 0.1347, 0.1038, 0.1263,\n",
       "         0.0752, 0.1365, 0.1286, 0.1308, 0.1331, 0.0899, 0.0897, 0.1275, 0.1005,\n",
       "         0.1337, 0.0706, 0.1177, 0.1396, 0.1351, 0.1297, 0.1346, 0.1275, 0.0531,\n",
       "         0.1315, 0.0613, 0.1298, 0.1240, 0.1309, 0.1245, 0.1241, 0.1157, 0.1228,\n",
       "         0.1200, 0.0250, 0.1255, 0.1396, 0.1257, 0.1267, 0.1364, 0.1095, 0.1344,\n",
       "         0.1357, 0.1316, 0.1295, 0.1409, 0.1166, 0.1179, 0.1068, 0.1270, 0.1401,\n",
       "         0.1037, 0.1150, 0.1116, 0.0554, 0.1139, 0.1285, 0.1324, 0.1515, 0.1263,\n",
       "         0.1295, 0.1037, 0.0672, 0.1236, 0.1338, 0.1305, 0.1189, 0.1135, 0.1589,\n",
       "         0.1416, 0.1326, 0.1498, 0.1208, 0.1411, 0.1352, 0.1402, 0.1107, 0.1366,\n",
       "         0.1198, 0.1325, 0.1276, 0.1288, 0.1322, 0.1146, 0.1305, 0.1227, 0.1281,\n",
       "         0.1465, 0.1413, 0.1542, 0.1251, 0.0905, 0.1287, 0.0269, 0.1452, 0.1295,\n",
       "         0.1016, 0.1237, 0.1102, 0.1451, 0.1326, 0.0678, 0.1292, 0.0938, 0.1352,\n",
       "         0.1319, 0.0801, 0.0914, 0.1368, 0.1248, 0.1240, 0.1053, 0.1029],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1768,  0.5953,  0.5887,  ...,  0.3946, -0.0545,  0.3563],\n",
       "         [ 0.3763,  0.9177,  0.5753,  ..., -1.4058, -0.7455,  1.0451],\n",
       "         [ 0.4532,  0.7508, -0.7072,  ..., -0.5422,  0.2709,  0.9863],\n",
       "         ...,\n",
       "         [-0.5058,  0.1501, -0.0105,  ..., -0.2164,  0.7773,  0.0760],\n",
       "         [ 0.3632, -1.1156,  1.2854,  ..., -1.4624,  1.6301,  1.1130],\n",
       "         [ 0.8171,  0.2208, -0.1087,  ..., -0.9137, -0.2161, -0.2714]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.4095, -0.4867,  0.1406,  ...,  0.0180,  0.3030,  0.8578],\n",
       "         [-0.4085,  0.0247,  0.0098,  ..., -0.2253, -1.0774,  0.7722],\n",
       "         [ 0.0853,  0.3183,  0.4024,  ..., -0.1882, -0.5082,  0.3786],\n",
       "         ...,\n",
       "         [ 0.1580, -0.1832,  0.2708,  ..., -0.4141,  0.0486, -0.2825],\n",
       "         [-0.1959,  0.1409, -0.0712,  ..., -0.4406,  0.4613, -0.3156],\n",
       "         [ 0.2556, -1.3947, -0.2929,  ...,  0.1776,  0.1640, -0.7635]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.6226,  0.4932,  0.6887,  0.4116,  0.4918,  0.7703,  0.6100,  0.4506,\n",
       "          0.4828,  0.6977,  0.7691,  0.4226,  0.7274,  0.7074,  0.5047,  0.6894,\n",
       "          0.6794,  0.2613,  0.6184,  0.7582,  0.7381,  0.6797,  0.4538,  0.5472,\n",
       "          0.6761,  0.6600,  0.7524,  0.5332,  0.7265,  0.2611,  0.7274,  0.4669,\n",
       "          0.6027,  0.7282,  0.5418,  0.4774,  0.7076,  0.7425,  0.2744,  0.5914,\n",
       "          0.7415,  0.6878,  1.8375,  0.3303,  0.3828,  0.6980,  0.3934,  0.6489,\n",
       "          0.7031,  0.7257,  0.6690,  0.4490,  0.5883,  0.3473,  0.4966,  0.7138,\n",
       "          0.7105,  0.6906,  0.7102,  0.3037,  0.6887,  0.4177,  0.3684,  0.7295,\n",
       "          0.3728,  0.5688,  0.7433,  0.7137,  0.5005,  0.2572,  0.4121,  0.6870,\n",
       "          0.6667,  0.7397,  0.6851,  0.7211,  0.6968,  0.6658,  0.6832,  0.3739,\n",
       "          0.3504,  0.6596,  0.6951,  0.6239,  0.6779,  0.5638,  0.7023,  0.3631,\n",
       "          0.7165,  0.5805,  0.7772,  0.5453,  0.6551,  0.6251,  0.5129,  0.7199,\n",
       "          0.5441,  0.7268,  0.4786,  0.6165,  0.7033,  0.7056,  0.6017,  0.4432,\n",
       "          0.7274,  0.7096,  0.7418,  0.5326,  0.2866,  0.5721,  0.6182,  0.4761,\n",
       "          0.7435,  0.6692,  0.7495,  0.5375,  0.6995,  0.6724,  0.7763,  0.7168,\n",
       "          0.7467,  0.5709,  0.6859,  0.6785,  0.6894,  0.5092,  0.2809,  0.6892,\n",
       "          0.3011,  0.5487,  0.6995,  0.7535,  0.4329,  0.3886,  0.5489,  0.8089,\n",
       "          0.4222,  0.3919,  0.6804,  0.7449,  0.7630,  0.5980,  0.2504,  0.6781,\n",
       "          0.6958,  0.4691,  0.5176,  0.9013,  0.4623,  1.0359,  0.7365,  0.7449,\n",
       "          0.4709,  0.7076,  0.7197,  0.7505,  0.7272,  0.5712,  0.6410,  0.5488,\n",
       "          0.7152,  0.3934,  0.7128,  0.7597,  0.4628,  0.6398,  0.4019,  0.6436,\n",
       "          0.7626,  0.6848,  0.5308,  0.6848,  0.6730,  0.6929,  0.6863,  0.6063,\n",
       "          0.7081,  0.4452,  0.7383,  0.6905,  0.7429,  0.5677,  0.5300,  0.5562,\n",
       "          0.4452,  0.7492,  0.8056,  0.4209,  0.7123,  0.5345,  0.5297,  0.2811,\n",
       "          0.7758,  0.5641,  0.7212,  0.6851,  0.7096,  0.5051,  0.7273,  0.7511,\n",
       "          0.3894,  0.4822,  0.3723,  0.3069,  0.6962,  0.5987,  0.3201,  0.4077,\n",
       "          0.4685,  0.7714,  0.6413,  0.6873,  0.4111,  0.4874,  0.7458,  0.7284,\n",
       "          0.5923,  0.5835,  0.4297,  0.7793,  0.7162,  0.6860,  0.4534,  0.7319,\n",
       "          0.5231,  0.7400,  0.6333,  0.7251,  0.4342,  0.6059,  0.7412,  0.6982,\n",
       "          0.3658,  0.5847,  0.5095,  0.3674,  0.6897,  0.5470,  0.6780,  0.4574,\n",
       "          0.6048,  0.4640,  0.5374,  0.7125,  0.5199,  0.4677,  0.3753,  0.4197,\n",
       "          0.4309,  0.5011,  0.7136,  0.6838,  0.3936,  0.7961,  0.7154,  0.6056,\n",
       "          0.6251,  0.6073,  0.5269,  0.4506,  1.2478,  0.6935,  0.7552,  0.5374,\n",
       "         -0.2519,  0.7555,  0.6102,  0.7255,  0.6414,  0.5974,  0.6892,  0.5404,\n",
       "          0.4426,  0.8013,  0.7089,  3.5468,  0.6850,  0.7299,  0.6612,  0.6673,\n",
       "          0.5416,  0.5033,  0.6828,  0.6495,  0.6907,  0.5779,  0.5530,  0.3779,\n",
       "          0.7426,  0.3784,  0.4596,  0.7027,  0.6702,  0.7391,  0.4593,  0.4344,\n",
       "          0.5641,  0.7152,  0.6871,  0.3033,  0.3038,  0.4948,  0.3102,  0.7192,\n",
       "          0.5428,  0.7238,  0.6745,  0.4655,  0.4699,  0.4703,  0.6519,  0.6935,\n",
       "          0.3206,  0.5923,  0.6115,  0.7535,  0.7315,  0.6971,  0.4917,  0.5590,\n",
       "          0.6991,  0.7503,  0.7223,  0.3653,  0.5429,  0.7184,  0.5423,  0.6703,\n",
       "          0.7150,  0.5091,  0.3145,  0.4345,  0.5228,  0.7047,  0.6816,  0.3290,\n",
       "          0.4128,  0.7085,  0.7572,  2.2281,  0.5222,  0.5672,  0.7379,  0.4826,\n",
       "          0.6897,  0.5997,  0.4168,  0.7235,  0.7643,  0.3151,  0.4545,  0.7755,\n",
       "          0.6814,  0.6992,  0.6570,  0.4362,  0.7407,  0.7134,  0.6998,  0.3865,\n",
       "          0.4007,  0.7325,  0.7147,  0.6892,  0.7210,  0.6166,  0.7326,  0.4562,\n",
       "          0.6946,  0.6995,  0.7489,  0.2979,  0.4883,  0.7155,  0.4632,  0.7167,\n",
       "          0.6756,  0.7186,  0.6814,  0.5513,  0.7170,  0.5014,  0.5205,  0.7281,\n",
       "          0.5282,  0.6699,  0.4906,  0.6628,  0.4525,  0.6749,  0.7128,  0.7495,\n",
       "          0.6594,  0.6485,  0.6218,  0.5146,  0.6173,  0.2414,  0.6998,  0.4048,\n",
       "          0.4596,  0.5543,  0.6934,  0.4433,  0.6890,  0.2583,  0.6417,  0.6625,\n",
       "          0.7249,  0.7521,  0.5490,  0.4047,  0.4614,  0.3494,  0.7461,  0.2760,\n",
       "          0.4784,  0.5711,  0.6906,  0.7241,  0.6674,  0.6178,  0.2448,  0.7109,\n",
       "          0.2590,  0.7432,  0.7375,  0.7331,  0.6996,  0.4760,  0.7453,  0.7348,\n",
       "          0.7396,  0.3083,  0.7360,  0.7046,  0.5664,  0.7102,  0.7299,  0.5054,\n",
       "          0.6940,  0.7031,  0.7093,  0.5572,  0.6965,  0.7028,  0.7299,  0.3499,\n",
       "          0.7439,  0.7011,  0.6848,  0.7320,  0.5358,  0.2730,  0.4953,  0.5719,\n",
       "          0.6749,  0.6183,  0.6520,  0.7068,  0.7155,  0.3307,  0.7443,  0.7079,\n",
       "          0.7492,  0.7235,  0.7088,  0.8304,  0.6549,  0.7242,  0.7544,  0.6779,\n",
       "          0.6470,  0.7510,  0.7301,  0.3781,  0.7656,  0.4341,  0.7127,  0.7418,\n",
       "          0.4822,  0.6148,  0.7600,  0.7352,  0.7230,  0.7100,  0.7068,  0.6390,\n",
       "          0.7557,  0.7453,  0.3905,  0.6971,  0.2134,  0.7412,  0.7027,  0.4628,\n",
       "          0.7774,  0.4248,  0.6860,  0.4953,  0.3307,  0.5298,  0.2677,  0.7609,\n",
       "          0.6551,  1.2349,  0.3555,  0.7084,  0.4817,  0.6508,  0.5565,  0.4451],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2375, 0.1788, 0.2832, 0.1360, 0.1884, 0.3344, 0.2353, 0.1616, 0.1789,\n",
       "         0.3174, 0.3302, 0.1418, 0.3258, 0.2948, 0.1813, 0.2696, 0.2899, 0.0905,\n",
       "         0.2455, 0.3047, 0.3065, 0.3177, 0.1664, 0.1465, 0.3224, 0.2684, 0.3339,\n",
       "         0.1957, 0.3204, 0.0354, 0.3145, 0.1709, 0.1792, 0.3212, 0.1942, 0.1857,\n",
       "         0.3139, 0.3259, 0.0750, 0.2479, 0.3060, 0.3273, 0.1660, 0.1362, 0.1313,\n",
       "         0.3111, 0.1593, 0.2507, 0.2897, 0.3334, 0.2628, 0.1774, 0.2270, 0.1291,\n",
       "         0.1485, 0.2897, 0.2906, 0.2806, 0.3124, 0.1301, 0.3084, 0.0560, 0.1236,\n",
       "         0.3118, 0.1527, 0.2160, 0.3060, 0.3218, 0.1875, 0.1201, 0.1492, 0.2875,\n",
       "         0.2974, 0.3209, 0.3075, 0.3081, 0.3175, 0.2481, 0.2629, 0.1285, 0.1340,\n",
       "         0.1394, 0.2956, 0.2347, 0.3007, 0.2229, 0.3059, 0.1160, 0.3210, 0.1935,\n",
       "         0.3332, 0.2074, 0.2503, 0.2685, 0.2037, 0.2959, 0.2120, 0.3123, 0.1758,\n",
       "         0.2335, 0.3099, 0.3237, 0.2179, 0.1764, 0.3379, 0.3091, 0.3104, 0.2054,\n",
       "         0.0533, 0.2001, 0.2461, 0.1234, 0.3335, 0.2711, 0.3297, 0.0671, 0.2890,\n",
       "         0.2949, 0.3385, 0.3005, 0.3265, 0.2518, 0.3130, 0.2600, 0.3121, 0.1658,\n",
       "         0.1327, 0.3079, 0.0639, 0.2289, 0.2981, 0.3322, 0.1474, 0.1287, 0.1832,\n",
       "         0.3473, 0.0719, 0.1511, 0.2857, 0.3465, 0.3275, 0.1858, 0.0953, 0.2825,\n",
       "         0.2759, 0.1739, 0.1821, 0.1443, 0.1527, 0.2259, 0.3316, 0.3213, 0.1671,\n",
       "         0.3072, 0.3038, 0.2980, 0.3320, 0.1973, 0.2421, 0.2169, 0.3382, 0.1089,\n",
       "         0.3355, 0.3250, 0.1836, 0.2532, 0.1185, 0.2668, 0.3463, 0.3011, 0.1761,\n",
       "         0.3023, 0.2739, 0.3175, 0.2852, 0.2215, 0.3006, 0.1542, 0.3364, 0.3000,\n",
       "         0.3335, 0.2398, 0.1520, 0.2160, 0.1531, 0.2723, 0.3185, 0.1606, 0.3109,\n",
       "         0.1907, 0.2070, 0.0671, 0.3132, 0.2159, 0.2950, 0.3225, 0.3205, 0.1309,\n",
       "         0.3235, 0.3096, 0.1360, 0.1878, 0.1419, 0.0203, 0.2799, 0.2535, 0.1244,\n",
       "         0.1279, 0.1650, 0.3379, 0.2957, 0.2940, 0.1545, 0.1819, 0.3257, 0.3168,\n",
       "         0.2484, 0.2069, 0.1517, 0.3243, 0.3199, 0.3187, 0.1736, 0.3165, 0.1805,\n",
       "         0.3382, 0.2316, 0.3274, 0.1433, 0.1885, 0.3051, 0.3170, 0.1070, 0.2230,\n",
       "         0.1918, 0.1253, 0.1477, 0.1976, 0.3201, 0.1651, 0.2249, 0.1716, 0.2071,\n",
       "         0.3146, 0.1946, 0.1469, 0.0591, 0.1345, 0.1343, 0.1824, 0.3129, 0.2711,\n",
       "         0.1557, 0.3225, 0.3144, 0.2323, 0.2644, 0.2386, 0.1855, 0.1575, 0.0216,\n",
       "         0.3322, 0.3182, 0.1149, 0.0407, 0.3444, 0.2346, 0.3157, 0.2817, 0.2052,\n",
       "         0.3002, 0.2076, 0.1543, 0.1908, 0.3167, 0.0492, 0.3012, 0.3159, 0.2541,\n",
       "         0.2457, 0.2028, 0.1825, 0.3009, 0.2379, 0.3102, 0.2295, 0.2183, 0.1197,\n",
       "         0.3190, 0.1299, 0.1736, 0.3001, 0.2902, 0.3290, 0.1747, 0.1740, 0.2172,\n",
       "         0.3165, 0.3256, 0.0922, 0.1118, 0.2130, 0.1356, 0.3215, 0.1895, 0.3435,\n",
       "         0.2547, 0.1822, 0.1843, 0.1615, 0.2876, 0.2654, 0.0802, 0.2662, 0.2694,\n",
       "         0.3296, 0.3349, 0.3219, 0.1514, 0.2131, 0.3000, 0.3307, 0.3101, 0.1178,\n",
       "         0.1951, 0.3413, 0.2110, 0.2849, 0.3220, 0.1880, 0.1372, 0.1538, 0.1892,\n",
       "         0.3075, 0.3181, 0.1313, 0.1414, 0.2935, 0.3135, 0.1529, 0.1805, 0.2126,\n",
       "         0.3293, 0.1878, 0.2908, 0.2317, 0.1562, 0.3100, 0.3222, 0.1206, 0.1656,\n",
       "         0.3510, 0.2943, 0.3389, 0.2795, 0.1579, 0.2924, 0.3136, 0.2988, 0.1563,\n",
       "         0.1418, 0.3153, 0.3225, 0.2569, 0.2890, 0.2317, 0.3209, 0.1602, 0.2894,\n",
       "         0.3183, 0.3494, 0.0682, 0.1869, 0.3234, 0.1688, 0.3184, 0.2899, 0.3091,\n",
       "         0.3073, 0.1857, 0.3052, 0.1762, 0.1968, 0.3114, 0.1843, 0.2888, 0.2013,\n",
       "         0.2863, 0.1561, 0.3250, 0.3108, 0.3086, 0.2425, 0.2713, 0.2476, 0.2037,\n",
       "         0.1984, 0.0489, 0.2809, 0.1269, 0.1742, 0.1981, 0.2706, 0.1592, 0.3219,\n",
       "         0.0953, 0.2658, 0.2999, 0.3243, 0.3088, 0.1498, 0.1267, 0.1760, 0.1289,\n",
       "         0.3132, 0.0798, 0.1871, 0.2170, 0.3221, 0.3302, 0.2496, 0.2463, 0.0472,\n",
       "         0.3008, 0.0732, 0.3380, 0.3191, 0.2575, 0.3188, 0.1703, 0.3247, 0.3160,\n",
       "         0.3244, 0.0116, 0.3094, 0.2833, 0.2068, 0.3188, 0.2848, 0.1566, 0.2998,\n",
       "         0.3094, 0.3066, 0.1933, 0.3014, 0.2839, 0.3207, 0.1306, 0.3200, 0.3080,\n",
       "         0.2942, 0.3066, 0.1865, 0.0724, 0.1840, 0.2249, 0.3132, 0.2468, 0.2499,\n",
       "         0.3068, 0.3193, 0.0887, 0.3125, 0.2885, 0.2968, 0.3045, 0.3146, 0.3350,\n",
       "         0.2469, 0.3036, 0.3236, 0.3179, 0.2526, 0.3323, 0.3155, 0.1291, 0.3043,\n",
       "         0.1560, 0.3171, 0.3354, 0.1658, 0.2382, 0.3145, 0.3385, 0.3157, 0.3343,\n",
       "         0.2825, 0.2578, 0.3019, 0.3005, 0.1262, 0.3176, 0.0311, 0.3138, 0.3104,\n",
       "         0.1429, 0.3204, 0.1433, 0.2733, 0.1857, 0.0855, 0.1967, 0.1133, 0.3392,\n",
       "         0.2428, 0.1365, 0.1184, 0.3021, 0.1930, 0.2643, 0.2169, 0.1498],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0334,  0.0702, -0.0449,  ...,  0.0335, -0.0739, -0.0074],\n",
       "         [-0.0639,  0.0912,  0.0061,  ..., -0.1432,  0.0246,  0.0536],\n",
       "         [ 0.0814,  0.1074, -0.0439,  ...,  0.0207,  0.0112, -0.0574],\n",
       "         ...,\n",
       "         [ 0.0031, -0.0052, -0.0171,  ...,  0.0154, -0.0070,  0.0044],\n",
       "         [ 0.0438, -0.0110, -0.0125,  ...,  0.0097, -0.0113,  0.0881],\n",
       "         [ 0.0091,  0.0712,  0.0191,  ..., -0.0317,  0.0563, -0.0408]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.2948, -0.5507, -0.8665,  ..., -0.4369, -1.1824, -0.1927],\n",
       "         [ 0.4025,  1.0283, -0.0236,  ..., -0.6924,  0.0335,  0.4483],\n",
       "         [ 0.1920, -0.6576,  0.1017,  ...,  0.1270,  0.3005,  0.6386],\n",
       "         ...,\n",
       "         [ 0.5634,  1.0474,  0.0598,  ...,  0.3133, -0.1880,  0.3536],\n",
       "         [ 0.4048,  0.5342,  0.3080,  ...,  0.3718,  0.4244, -0.2130],\n",
       "         [-0.2207, -0.1779, -0.0922,  ...,  0.3677, -0.1952,  0.1167]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.5133,  0.1521, -0.2289,  ..., -0.7616,  0.7872,  0.7899],\n",
       "         [ 0.7708,  0.0922, -0.5297,  ..., -0.1196, -0.3503,  0.3664],\n",
       "         [-0.0219, -0.1886,  0.3858,  ..., -0.8549,  0.1413,  0.1109],\n",
       "         ...,\n",
       "         [-1.2255, -0.1182, -0.4810,  ..., -0.2499, -0.5912, -0.0739],\n",
       "         [ 0.4112, -0.2279, -0.4472,  ...,  0.4612,  0.1416,  0.7631],\n",
       "         [-0.2315, -0.3842, -0.3811,  ..., -0.4026,  0.5246, -0.1230]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.4839,  0.5764,  0.1609,  ..., -0.0764,  0.4604, -0.1007],\n",
       "         [-0.3476,  0.2037,  0.1299,  ...,  0.2126, -0.1616, -0.4045],\n",
       "         [ 1.2223,  0.7106, -0.0680,  ..., -0.7041,  1.0229,  0.5053],\n",
       "         ...,\n",
       "         [-0.2056,  0.5603, -0.0755,  ..., -0.3437,  0.2833, -0.1749],\n",
       "         [-0.3315,  0.6727, -0.6381,  ...,  0.9523,  0.6630, -0.1329],\n",
       "         [-0.2270,  0.6518,  0.3616,  ...,  0.1442, -1.0036,  0.5781]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.5023e+00,  4.4399e-01,  3.1688e+00,  9.8330e-01, -5.4761e+00,\n",
       "           5.1908e+00,  2.1551e+00,  5.1753e-01],\n",
       "         [ 3.9766e+00,  1.2230e+00,  4.3384e+00,  2.0128e+00,  7.7737e-01,\n",
       "           4.9232e+00,  4.7324e+00,  4.4739e+00],\n",
       "         [ 1.4370e+00,  7.2636e-01,  2.9361e+00,  1.5558e+00,  1.0709e+00,\n",
       "           3.2694e+00,  2.3018e+00,  3.5495e+00],\n",
       "         [-2.7024e-01,  2.3678e-01,  2.0351e+00,  1.2915e+00,  1.1591e+00,\n",
       "           2.3123e+00,  1.1901e+00,  2.9390e+00],\n",
       "         [-1.5405e+00, -1.9768e-01,  1.2941e+00,  1.1876e+00,  1.1762e+00,\n",
       "           1.5803e+00,  5.3310e-01,  2.4621e+00],\n",
       "         [-2.6444e+00, -5.5446e-01,  6.2861e-01,  1.0587e+00,  1.1775e+00,\n",
       "           9.6888e-01,  6.0842e-02,  2.0311e+00],\n",
       "         [-3.3680e+00, -9.1466e-01,  1.5533e-01,  9.5481e-01,  1.1451e+00,\n",
       "           5.4734e-01, -1.9281e-01,  1.6591e+00],\n",
       "         [-3.9326e+00, -1.2584e+00, -3.7502e-01,  9.0362e-01,  1.0360e+00,\n",
       "           7.3151e-02, -4.9643e-01,  1.4384e+00],\n",
       "         [-4.3680e+00, -1.5479e+00, -7.5266e-01,  8.1447e-01,  1.0053e+00,\n",
       "          -2.2903e-01, -6.9518e-01,  1.1496e+00],\n",
       "         [-4.7151e+00, -1.8003e+00, -1.0946e+00,  7.6001e-01,  9.7079e-01,\n",
       "          -4.7195e-01, -8.5698e-01,  9.3990e-01],\n",
       "         [-4.9355e+00, -2.0986e+00, -1.3622e+00,  6.7407e-01,  9.0814e-01,\n",
       "          -6.7101e-01, -9.8900e-01,  7.2749e-01],\n",
       "         [-5.2135e+00, -2.3864e+00, -1.6043e+00,  6.1808e-01,  8.7607e-01,\n",
       "          -9.0177e-01, -1.1585e+00,  6.2345e-01],\n",
       "         [-5.3132e+00, -2.6615e+00, -1.8075e+00,  5.8090e-01,  8.1096e-01,\n",
       "          -1.0340e+00, -1.2031e+00,  3.6789e-01],\n",
       "         [-5.4710e+00, -2.9293e+00, -1.9912e+00,  5.3785e-01,  7.5883e-01,\n",
       "          -1.1973e+00, -1.2780e+00,  2.5503e-01],\n",
       "         [-5.6254e+00, -3.1145e+00, -2.1533e+00,  4.9085e-01,  7.0112e-01,\n",
       "          -1.2525e+00, -1.4005e+00,  1.4504e-01],\n",
       "         [-5.6862e+00, -3.3778e+00, -2.3405e+00,  4.5332e-01,  6.6295e-01,\n",
       "          -1.3235e+00, -1.4163e+00, -3.3957e-02],\n",
       "         [-5.8407e+00, -3.8204e+00, -2.5302e+00,  3.5879e-01,  5.6386e-01,\n",
       "          -1.5452e+00, -1.5017e+00, -1.9085e-01],\n",
       "         [-5.9694e+00, -4.2966e+00, -2.8254e+00,  2.8539e-01,  4.1548e-01,\n",
       "          -1.7282e+00, -1.6813e+00, -4.1271e-01],\n",
       "         [-6.0913e+00, -4.6471e+00, -2.9519e+00,  1.9925e-01,  3.1181e-01,\n",
       "          -1.7594e+00, -1.7224e+00, -6.1250e-01],\n",
       "         [-6.1264e+00, -4.9994e+00, -3.1383e+00,  1.3498e-01,  1.3773e-01,\n",
       "          -1.9009e+00, -1.7605e+00, -7.7214e-01],\n",
       "         [-6.2516e+00, -5.3240e+00, -3.3227e+00,  2.1288e-02, -1.4813e-02,\n",
       "          -1.9881e+00, -1.8367e+00, -9.5874e-01],\n",
       "         [-6.2115e+00, -5.5318e+00, -3.3736e+00, -6.4359e-02, -1.7722e-01,\n",
       "          -2.0390e+00, -1.8952e+00, -1.2014e+00],\n",
       "         [-6.2402e+00, -5.7459e+00, -3.4116e+00, -1.8700e-01, -3.1469e-01,\n",
       "          -2.0706e+00, -1.9069e+00, -1.3104e+00],\n",
       "         [-6.2802e+00, -6.0032e+00, -3.5008e+00, -2.5624e-01, -5.0596e-01,\n",
       "          -2.1293e+00, -1.9316e+00, -1.3874e+00],\n",
       "         [-6.2237e+00, -6.2099e+00, -3.5600e+00, -3.8662e-01, -7.1192e-01,\n",
       "          -2.0619e+00, -1.9455e+00, -1.5814e+00],\n",
       "         [-6.1923e+00, -6.3335e+00, -3.5813e+00, -4.9289e-01, -9.2136e-01,\n",
       "          -2.0775e+00, -1.9058e+00, -1.7458e+00],\n",
       "         [-6.1312e+00, -6.3374e+00, -3.5930e+00, -6.3565e-01, -1.1944e+00,\n",
       "          -2.1369e+00, -1.9575e+00, -1.8795e+00],\n",
       "         [-6.0023e+00, -5.9066e+00, -3.5457e+00, -7.7403e-01, -1.4950e+00,\n",
       "          -2.0453e+00, -1.9600e+00, -2.0653e+00],\n",
       "         [-5.8416e+00, -5.0429e+00, -3.4052e+00, -9.6811e-01, -1.8421e+00,\n",
       "          -2.0062e+00, -1.9099e+00, -2.1865e+00],\n",
       "         [-5.6903e+00, -3.6741e+00, -3.2499e+00, -1.1651e+00, -2.1841e+00,\n",
       "          -1.9617e+00, -1.8882e+00, -2.6194e+00],\n",
       "         [-5.5004e+00, -2.6762e+00, -3.0594e+00, -1.3489e+00, -2.3569e+00,\n",
       "          -1.8503e+00, -1.7110e+00, -3.3750e+01],\n",
       "         [ 4.8001e+01,  2.1248e+01,  4.5752e+01, -3.3250e+01,  2.6749e+01,\n",
       "          -3.4500e+01, -3.4000e+01, -3.3250e+01]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0962, 0.1454, 0.0724, 0.0936, 0.2446, 0.0817, 0.1409, 0.1126, 0.1841,\n",
       "         0.0667, 0.0665, 0.0764, 0.0789, 0.0762, 0.1808, 0.0692, 0.0628, 0.0900,\n",
       "         0.0884, 0.0702, 0.0735, 0.0530, 0.1133, 0.1055, 0.0692, 0.0895, 0.0860,\n",
       "         0.1529, 0.0710, 0.1175, 0.0683, 0.1178, 0.0996, 0.0639, 0.1949, 0.1171,\n",
       "         0.0810, 0.0820, 0.0870, 0.0861, 0.0784, 0.0666, 0.0660, 0.1042, 0.1012,\n",
       "         0.0630, 0.0468, 0.0852, 0.0838, 0.0695, 0.0928, 0.0909, 0.1148, 0.1011,\n",
       "         0.1366, 0.0830, 0.0830, 0.0887, 0.0616, 0.1374, 0.0696, 0.1164, 0.1233,\n",
       "         0.0737, 0.0696, 0.0980, 0.0948, 0.0718, 0.1670, 0.1217, 0.1105, 0.0833,\n",
       "         0.0845, 0.0689, 0.0814, 0.0722, 0.0884, 0.0843, 0.1020, 0.1027, 0.1224,\n",
       "         0.0924, 0.0803, 0.1032, 0.0592, 0.0931, 0.0635, 0.0829, 0.0749, 0.1062,\n",
       "         0.0817, 0.1154, 0.0718, 0.0875, 0.0995, 0.0839, 0.0889, 0.0598, 0.0401,\n",
       "         0.0629, 0.0776, 0.0651, 0.1012, 0.1784, 0.0618, 0.0794, 0.0696, 0.0894,\n",
       "         0.0796, 0.0853, 0.0766, 0.1391, 0.0636, 0.0881, 0.0695, 0.1005, 0.0757,\n",
       "         0.0809, 0.0621, 0.0554, 0.0686, 0.0741, 0.0677, 0.0416, 0.0613, 0.1412,\n",
       "         0.0872, 0.0795, 0.0756, 0.0740, 0.0775, 0.0610, 0.0907, 0.1161, 0.0833,\n",
       "         0.0788, 0.1392, 0.0672, 0.0605, 0.0668, 0.0746, 0.1507, 0.0950, 0.0609,\n",
       "         0.0861, 0.0889, 0.1053, 0.1008, 0.0937, 0.0828, 0.0609, 0.0676, 0.0969,\n",
       "         0.0753, 0.0776, 0.0653, 0.0633, 0.1100, 0.0808, 0.0849, 0.0573, 0.1153,\n",
       "         0.0806, 0.0779, 0.0910, 0.0864, 0.0896, 0.1143, 0.0719, 0.0634, 0.1110,\n",
       "         0.0851, 0.0736, 0.0570, 0.0789, 0.0740, 0.0613, 0.0996, 0.0678, 0.0824,\n",
       "         0.0644, 0.0916, 0.2127, 0.1317, 0.1172, 0.0833, 0.0840, 0.0985, 0.0727,\n",
       "         0.1000, 0.1549, 0.1009, 0.0727, 0.1304, 0.0762, 0.0674, 0.0615, 0.1052,\n",
       "         0.0652, 0.0777, 0.1002, 0.1155, 0.1012, 0.0911, 0.0834, 0.0833, 0.1171,\n",
       "         0.1118, 0.0975, 0.0730, 0.0774, 0.0696, 0.0935, 0.1466, 0.0716, 0.0706,\n",
       "         0.0892, 0.0944, 0.1442, 0.0645, 0.0705, 0.0652, 0.1094, 0.0791, 0.1347,\n",
       "         0.0682, 0.1047, 0.0617, 0.1042, 0.1006, 0.0841, 0.0664, 0.0823, 0.0813,\n",
       "         0.1213, 0.1050, 0.0948, 0.0681, 0.0618, 0.0980, 0.1220, 0.1173, 0.0884,\n",
       "         0.0721, 0.1984, 0.1412, 0.1265, 0.1022, 0.0809, 0.1023, 0.0630, 0.0717,\n",
       "         0.0960, 0.0648, 0.0672, 0.0835, 0.0931, 0.0780, 0.0978, 0.1066, 0.0847,\n",
       "         0.0701, 0.0668, 0.1742, 0.0921, 0.0756, 0.0910, 0.0698, 0.0551, 0.0623,\n",
       "         0.0778, 0.1037, 0.1147, 0.1022, 0.0696, 0.1097, 0.0684, 0.0699, 0.1069,\n",
       "         0.1015, 0.1397, 0.0972, 0.0770, 0.1025, 0.0757, 0.0826, 0.0988, 0.1134,\n",
       "         0.0693, 0.1178, 0.1380, 0.0670, 0.0744, 0.0649, 0.1290, 0.1334, 0.0854,\n",
       "         0.0716, 0.0745, 0.1150, 0.1061, 0.1034, 0.1571, 0.0669, 0.1170, 0.0727,\n",
       "         0.0794, 0.1508, 0.1757, 0.1002, 0.0802, 0.0918, 0.0843, 0.0829, 0.0882,\n",
       "         0.0638, 0.0591, 0.0649, 0.0794, 0.1180, 0.0721, 0.0678, 0.0613, 0.1467,\n",
       "         0.1323, 0.0679, 0.1136, 0.0659, 0.0698, 0.1469, 0.1237, 0.1072, 0.1162,\n",
       "         0.0699, 0.0653, 0.1020, 0.0966, 0.0834, 0.0672, 0.1059, 0.1212, 0.0883,\n",
       "         0.0852, 0.1315, 0.0693, 0.1045, 0.1070, 0.0606, 0.0508, 0.1056, 0.1251,\n",
       "         0.0813, 0.0686, 0.0666, 0.0744, 0.0942, 0.0673, 0.0799, 0.0853, 0.0992,\n",
       "         0.0908, 0.0622, 0.0700, 0.0671, 0.0786, 0.1057, 0.0735, 0.1239, 0.0991,\n",
       "         0.0697, 0.0812, 0.0972, 0.0932, 0.0714, 0.0873, 0.0726, 0.0710, 0.0692,\n",
       "         0.0581, 0.1465, 0.0791, 0.1098, 0.0970, 0.0664, 0.1042, 0.0722, 0.0920,\n",
       "         0.0695, 0.1062, 0.0681, 0.0702, 0.0798, 0.0434, 0.0836, 0.0667, 0.0821,\n",
       "         0.0656, 0.1014, 0.0844, 0.1023, 0.1281, 0.1254, 0.0719, 0.4686, 0.0613,\n",
       "         0.0612, 0.0793, 0.0641, 0.0729, 0.0681, 0.0892, 0.1273, 0.1215, 0.1227,\n",
       "         0.0720, 0.1120, 0.0980, 0.1427, 0.0756, 0.0676, 0.0883, 0.0824, 0.0769,\n",
       "         0.0703, 0.1149, 0.0736, 0.0724, 0.0889, 0.0680, 0.1197, 0.0734, 0.0680,\n",
       "         0.0506, 0.1208, 0.0664, 0.0750, 0.0909, 0.0624, 0.1023, 0.1151, 0.0732,\n",
       "         0.0747, 0.0688, 0.0870, 0.0838, 0.0791, 0.0677, 0.1181, 0.0793, 0.0828,\n",
       "         0.0630, 0.0657, 0.0513, 0.1133, 0.1103, 0.0725, 0.0634, 0.0770, 0.0803,\n",
       "         0.0695, 0.0655, 0.0696, 0.0709, 0.0614, 0.0794, 0.0585, 0.0612, 0.0562,\n",
       "         0.1382, 0.0646, 0.0817, 0.0617, 0.0843, 0.0717, 0.0814, 0.0798, 0.0643,\n",
       "         0.1449, 0.0773, 0.0355, 0.1212, 0.0879, 0.0592, 0.0677, 0.0706, 0.0678,\n",
       "         0.0997, 0.0855, 0.0960, 0.0813, 0.1569, 0.0686, 0.0889, 0.0755, 0.0709,\n",
       "         0.1194, 0.0720, 0.0798, 0.0812, 0.1115, 0.0880, 0.1138, 0.0997, 0.0822,\n",
       "         0.0855, 0.1295, 0.1244, 0.0671, 0.1237, 0.0731, 0.0984, 0.1187],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0496,  0.0364,  0.0603,  ..., -0.0356, -0.0219, -0.0458],\n",
       "         [ 0.0799,  0.0089, -0.0262,  ...,  0.0333,  0.0564, -0.0311],\n",
       "         [-0.1226,  0.0080, -0.0841,  ...,  0.1753,  0.0064, -0.0149],\n",
       "         ...,\n",
       "         [ 0.0688,  0.0131,  0.1122,  ..., -0.0214, -0.0206, -0.0048],\n",
       "         [ 0.0630, -0.0384,  0.0456,  ...,  0.0562,  0.0268, -0.0466],\n",
       "         [-0.0270, -0.0668, -0.0529,  ...,  0.0308, -0.0344,  0.0201]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0879, -0.0018, -0.0485,  ..., -0.2322, -0.3322,  0.1296],\n",
       "         [ 0.7345,  0.0908, -0.1707,  ...,  0.1567, -0.1776, -0.1381],\n",
       "         [-0.8172,  0.0208, -0.3380,  ...,  0.1576, -0.4329,  0.0620],\n",
       "         ...,\n",
       "         [-0.1037,  0.2527, -0.3175,  ..., -0.3336, -0.2805,  0.6199],\n",
       "         [-0.1535,  0.0177,  0.6037,  ..., -0.6889,  0.5444,  0.4996],\n",
       "         [-0.1932,  0.5680,  0.5430,  ..., -0.1428, -0.3930, -0.8219]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2148,  0.1634,  0.5303,  ..., -0.3402,  0.6348,  0.2537],\n",
       "         [ 0.4620,  0.8085, -0.1071,  ...,  0.5840, -0.9003,  0.1686],\n",
       "         [ 0.8434,  1.0483, -0.0436,  ..., -0.8440,  0.4547,  0.0258],\n",
       "         ...,\n",
       "         [-0.6410, -0.8101, -0.1569,  ...,  0.1477,  0.2412,  0.1132],\n",
       "         [ 0.0369, -0.4636,  0.1021,  ..., -0.1295,  0.3058,  0.1399],\n",
       "         [ 0.0818,  0.3990, -0.1581,  ..., -0.2853,  0.1281,  0.6886]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0285, -0.4632, -0.1648,  ...,  0.1853,  0.0029,  0.1580],\n",
       "         [-0.0656, -0.6917, -0.8068,  ..., -0.2789,  0.3334, -0.2655],\n",
       "         [ 0.6807, -0.1758,  1.2748,  ..., -0.7369,  1.2693,  0.4003],\n",
       "         ...,\n",
       "         [-0.0889, -0.2921, -0.0781,  ..., -0.3795,  0.0178,  0.6474],\n",
       "         [ 0.5243,  0.8087,  1.0101,  ...,  0.3294,  0.9951, -0.5115],\n",
       "         [-0.7984, -0.2637, -0.3617,  ..., -0.0889,  0.2310,  0.2827]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0800, 0.1826, 0.0747, 0.0822, 0.0323, 0.0802, 0.1250, 0.0675, 0.0585,\n",
       "         0.0621, 0.0792, 0.0327, 0.0767, 0.0773, 0.6999, 0.0768, 0.0709, 0.0731,\n",
       "         0.0794, 0.0700, 0.0858, 0.0608, 0.1193, 0.0282, 0.0742, 0.0705, 0.0833,\n",
       "         0.0688, 0.0728, 0.0973, 0.0766, 0.0830, 0.0907, 0.0648, 0.1330, 0.0750,\n",
       "         0.0802, 0.0903, 0.0844, 0.0886, 0.0759, 0.0769, 0.0462, 0.1064, 0.0830,\n",
       "         0.0761, 0.0213, 0.0845, 0.0918, 0.0804, 0.0953, 0.0853, 0.0851, 0.0359,\n",
       "         0.0714, 0.0851, 0.0783, 0.0821, 0.0682, 0.0756, 0.0746, 0.0827, 0.1135,\n",
       "         0.0820, 0.0985, 0.0837, 0.0823, 0.0701, 0.0306, 0.1161, 0.1001, 0.0715,\n",
       "         0.0792, 0.0690, 0.0732, 0.0793, 0.0752, 0.0740, 0.1223, 0.0823, 0.0967,\n",
       "         0.0825, 0.0851, 0.0857, 0.0707, 0.0851, 0.0772, 0.0595, 0.0818, 0.2887,\n",
       "         0.0734, 0.0790, 0.0829, 0.0826, 0.0847, 0.0834, 0.0894, 0.0583, 0.0817,\n",
       "         0.0686, 0.0812, 0.0670, 0.0856, 0.0858, 0.0737, 0.0792, 0.0662, 0.0920,\n",
       "         0.0861, 0.0809, 0.0779, 0.1067, 0.0724, 0.0812, 0.0781, 0.0916, 0.0781,\n",
       "         0.0873, 0.0698, 0.0651, 0.0687, 0.0776, 0.0698, 0.0791, 0.0712, 0.0516,\n",
       "         0.0952, 0.0789, 0.0837, 0.0942, 0.0811, 0.0756, 0.0902, 0.0638, 0.0796,\n",
       "         0.0818, 0.1096, 0.0786, 0.0578, 0.0766, 0.0801, 0.0404, 0.0894, 0.0648,\n",
       "         0.0808, 0.0826, 0.1003, 0.1232, 0.0908, 0.0238, 0.0779, 0.0754, 0.0788,\n",
       "         0.0678, 0.0774, 0.0784, 0.0715, 0.0702, 0.0966, 0.0636, 0.0690, 0.1075,\n",
       "         0.0703, 0.0705, 0.0841, 0.0954, 0.2277, 0.1082, 0.0772, 0.0733, 0.0817,\n",
       "         0.0811, 0.0801, 0.0820, 0.0798, 0.0729, 0.0910, 0.0704, 0.0738, 0.0766,\n",
       "         0.0704, 0.0753, 0.0945, 0.1548, 0.0395, 0.0808, 0.0770, 0.0876, 0.0763,\n",
       "         0.0900, 0.7168, 0.0757, 0.0777, 0.1501, 0.0898, 0.0756, 0.0716, 0.0939,\n",
       "         0.0655, 0.0801, 0.0869, 0.0932, 0.0897, 0.0853, 0.0731, 0.0816, 0.1127,\n",
       "         0.1043, 0.0772, 0.0741, 0.0750, 0.0789, 0.1031, 0.2386, 0.0761, 0.0773,\n",
       "         0.0889, 0.0808, 0.1173, 0.0738, 0.0696, 0.0741, 0.0909, 0.0720, 0.1004,\n",
       "         0.0699, 0.0755, 0.0751, 0.0974, 0.0923, 0.0866, 0.0783, 0.0346, 0.0767,\n",
       "         0.1472, 0.0849, 0.0780, 0.0674, 0.0663, 0.0797, 0.0911, 0.0940, 0.0848,\n",
       "         0.0660, 0.0885, 0.0587, 0.1074, 0.0790, 0.0814, 0.0987, 0.0725, 0.0793,\n",
       "         0.0909, 0.0781, 0.0752, 0.0792, 0.0872, 0.0660, 0.0669, 0.0928, 0.0777,\n",
       "         0.0689, 0.0678, 0.1416, 0.0897, 0.0841, 0.0871, 0.0741, 0.0849, 0.0694,\n",
       "         0.0785, 0.0951, 0.0922, 0.0365, 0.0719, 0.0741, 0.0864, 0.0679, 0.0855,\n",
       "         0.0891, 0.0647, 0.0830, 0.0699, 0.0991, 0.0757, 0.0824, 0.1194, 0.1014,\n",
       "         0.0655, 0.0985, 0.1450, 0.0733, 0.0747, 0.0809, 0.3686, 0.0935, 0.0793,\n",
       "         0.0721, 0.0682, 0.0752, 0.1034, 0.0945, 0.1933, 0.0791, 0.0440, 0.0776,\n",
       "         0.0699, 0.0360, 0.4298, 0.0958, 0.0750, 0.0764, 0.0557, 0.0810, 0.0943,\n",
       "         0.0721, 0.0694, 0.0744, 0.1216, 0.1002, 0.0768, 0.0797, 0.0647, 0.0324,\n",
       "         0.0479, 0.0778, 0.0959, 0.0679, 0.0688, 0.0748, 0.1143, 0.0850, 0.0864,\n",
       "         0.0602, 0.0714, 0.1082, 0.0875, 0.0926, 0.0722, 0.1047, 0.0513, 0.0808,\n",
       "         0.0837, 0.0783, 0.0749, 0.0556, 0.0914, 0.0619, 0.0605, 0.0757, 0.1182,\n",
       "         0.0880, 0.0793, 0.0802, 0.0862, 0.1400, 0.0719, 0.0872, 0.0776, 0.0970,\n",
       "         0.0904, 0.0699, 0.0798, 0.0521, 0.0813, 0.0814, 0.0760, 0.0381, 0.0876,\n",
       "         0.0730, 0.0832, 0.1010, 0.0902, 0.0811, 0.1400, 0.0737, 0.0770, 0.0713,\n",
       "         0.0701, 0.0647, 0.0722, 0.0550, 0.0944, 0.0704, 0.1145, 0.0732, 0.1291,\n",
       "         0.0749, 0.0987, 0.0744, 0.0751, 0.0773, 0.0544, 0.0840, 0.0731, 0.0922,\n",
       "         0.0712, 0.1016, 0.0788, 0.0965, 0.0812, 0.0639, 0.0868, 0.0381, 0.0699,\n",
       "         0.1016, 0.0705, 0.0854, 0.0713, 0.0696, 0.0754, 0.0847, 0.0950, 0.0495,\n",
       "         0.0787, 0.1012, 0.0882, 0.0534, 0.0694, 0.0672, 0.0850, 0.0763, 0.0647,\n",
       "         0.1146, 0.1061, 0.0782, 0.0739, 0.0888, 0.0687, 0.1071, 0.0683, 0.0649,\n",
       "         0.0708, 0.0868, 0.0704, 0.0668, 0.0920, 0.0741, 0.0771, 0.0951, 0.0860,\n",
       "         0.0768, 0.0698, 0.0934, 0.0740, 0.0795, 0.0622, 0.1185, 0.0768, 0.0778,\n",
       "         0.0675, 0.0726, 0.0335, 0.0605, 0.0936, 0.0720, 0.0707, 0.0845, 0.0696,\n",
       "         0.0722, 0.0708, 0.0433, 0.0689, 0.0762, 0.0732, 0.0653, 0.0815, 0.1080,\n",
       "         0.1385, 0.0649, 0.0735, 0.0724, 0.0831, 0.0855, 0.0774, 0.0396, 0.0727,\n",
       "         0.0693, 0.0784, 0.0924, 0.1255, 0.0793, 0.0744, 0.0671, 0.0726, 0.0798,\n",
       "         0.0959, 0.0904, 0.0927, 0.0805, 0.1232, 0.0668, 0.0775, 0.0811, 0.0687,\n",
       "         0.1039, 0.0776, 0.0974, 0.0944, 0.1082, 0.0835, 0.0453, 0.0933, 0.0851,\n",
       "         0.0701, 0.1141, 0.1078, 0.0846, 0.1036, 0.0766, 0.0828, 0.1265],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.7405, -0.2097,  0.4578,  ...,  0.4198, -0.2548,  0.3892],\n",
       "         [-0.0941, -0.4380,  0.4522,  ...,  0.0229,  0.2637, -0.4811],\n",
       "         [ 1.1070,  0.4963, -0.9766,  ..., -0.5712, -0.3164, -0.9737],\n",
       "         ...,\n",
       "         [ 0.0823, -0.0117, -0.1019,  ..., -0.8668,  0.1069, -0.0361],\n",
       "         [-0.8073, -0.1756, -0.5049,  ...,  0.4933,  0.1237,  0.8181],\n",
       "         [ 0.1086, -1.0159,  0.3178,  ..., -0.3388,  0.1074,  0.1799]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1939,  0.1790, -0.1469,  ...,  0.1964, -0.2783,  0.1420],\n",
       "         [-0.0322,  0.1240, -0.0121,  ..., -0.0709, -0.0636,  0.2117],\n",
       "         [ 0.0236, -0.2165, -0.2616,  ...,  0.1101, -0.2870, -0.1731],\n",
       "         ...,\n",
       "         [-1.2800, -0.1186,  0.0502,  ...,  0.3910,  0.2271, -0.0883],\n",
       "         [-0.2494,  0.1034, -0.0946,  ...,  0.1354,  0.1084, -0.1493],\n",
       "         [-0.2432,  0.3604, -4.5071,  ..., -0.2516,  0.0686,  0.4091]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.6658, 0.9661, 0.5934, 0.8251, 0.2930, 0.6002, 0.7835, 0.6029, 0.4040,\n",
       "         0.6176, 0.5884, 0.3372, 0.5821, 0.6886, 5.2324, 0.6468, 0.5289, 0.6920,\n",
       "         0.6684, 0.5875, 0.5919, 0.4929, 0.9645, 0.2659, 0.5644, 0.5555, 0.7012,\n",
       "         0.4827, 0.5823, 0.8039, 0.5640, 0.6321, 0.8483, 0.5502, 0.7802, 0.6571,\n",
       "         0.6172, 0.6889, 0.7547, 0.6313, 0.6279, 0.5932, 0.2640, 0.8344, 0.8303,\n",
       "         0.5916, 0.1519, 0.6108, 0.6629, 0.6074, 0.7029, 0.7638, 0.6402, 0.3052,\n",
       "         0.5744, 0.6272, 0.6009, 0.6914, 0.5581, 0.5925, 0.7195, 0.6471, 0.9526,\n",
       "         0.6652, 1.1138, 0.7069, 0.6795, 0.5712, 0.2602, 0.8793, 0.8566, 0.5831,\n",
       "         0.6349, 0.5499, 0.6530, 0.5983, 0.6390, 0.5529, 0.9078, 0.5279, 0.7060,\n",
       "         0.8810, 0.6484, 0.6913, 0.5420, 0.6888, 0.6143, 0.5650, 0.6239, 1.2919,\n",
       "         0.5918, 0.5532, 0.7297, 0.6658, 0.7587, 0.7206, 0.6733, 0.4705, 0.8523,\n",
       "         0.6039, 0.6984, 0.5577, 0.6241, 0.3970, 0.4851, 0.6154, 0.5394, 0.9005,\n",
       "         0.6952, 0.5993, 0.7294, 0.8884, 0.6101, 0.7927, 0.6289, 0.7793, 0.7044,\n",
       "         0.6565, 0.5764, 0.5646, 0.5446, 0.6307, 0.5820, 0.5548, 0.5836, 0.3965,\n",
       "         0.6692, 0.7398, 0.6540, 0.7965, 0.7106, 0.5196, 0.7565, 0.4669, 0.7020,\n",
       "         0.6130, 0.8065, 0.4633, 0.4971, 0.5269, 0.5783, 0.2595, 0.8736, 0.4770,\n",
       "         0.6879, 0.6833, 0.8214, 1.0297, 0.7409, 0.1029, 0.5242, 0.5952, 0.5890,\n",
       "         0.5715, 0.5507, 0.5793, 0.5407, 0.4251, 0.7496, 0.3455, 0.5497, 0.7149,\n",
       "         0.5785, 0.5894, 0.7651, 0.6982, 1.8460, 0.8244, 0.5818, 0.5632, 0.5744,\n",
       "         0.7165, 0.6440, 0.5822, 0.6676, 0.5817, 0.6484, 0.4821, 0.5944, 0.6079,\n",
       "         0.5656, 0.7194, 0.4665, 1.0436, 0.2753, 0.6567, 0.6311, 0.8189, 0.5732,\n",
       "         0.6714, 3.5877, 0.4649, 0.5776, 1.0951, 0.6946, 0.5418, 0.5000, 0.8040,\n",
       "         0.5065, 0.6445, 0.7050, 0.7378, 0.8026, 0.7211, 0.7040, 0.7288, 0.6450,\n",
       "         0.7184, 0.8603, 0.5750, 0.6270, 0.6264, 0.8703, 1.6468, 0.6241, 0.5615,\n",
       "         0.7829, 0.7135, 0.8029, 0.5277, 0.5123, 0.5490, 0.7842, 0.6129, 0.5937,\n",
       "         0.5487, 0.5752, 0.5167, 0.8617, 0.8353, 0.6815, 0.5669, 0.3447, 0.6683,\n",
       "         0.8805, 0.8588, 0.7862, 0.6000, 0.5367, 0.8919, 0.5666, 0.7535, 0.7512,\n",
       "         0.5267, 0.6143, 0.3950, 0.8897, 0.7653, 0.4358, 0.8010, 0.5686, 0.7297,\n",
       "         0.8579, 0.5243, 0.5164, 0.6294, 0.7948, 0.5028, 0.5250, 0.7505, 0.5906,\n",
       "         0.5604, 0.6230, 0.7690, 0.8633, 0.6608, 0.6826, 0.5293, 0.7053, 0.5964,\n",
       "         0.5696, 0.9530, 0.7609, 0.2639, 0.6221, 0.6547, 0.6310, 0.5733, 0.6507,\n",
       "         0.6774, 0.5351, 0.6395, 0.6762, 0.7976, 0.6082, 0.6875, 0.9062, 0.8513,\n",
       "         0.5840, 0.7439, 0.8780, 0.5523, 0.5571, 0.6221, 2.4390, 0.4938, 0.7201,\n",
       "         0.6438, 0.6162, 0.3980, 0.8006, 0.7773, 1.7913, 0.5784, 0.3501, 0.5639,\n",
       "         0.5484, 0.3119, 3.1192, 0.8115, 0.6099, 0.5843, 0.3380, 0.7466, 0.7096,\n",
       "         0.5660, 0.4955, 0.5995, 0.6325, 0.7602, 0.6063, 0.6178, 0.5228, 0.2842,\n",
       "         0.3627, 0.5482, 0.7598, 0.6506, 0.5936, 0.8078, 0.8179, 0.8468, 0.5961,\n",
       "         0.6238, 0.5286, 0.8846, 0.7160, 0.7107, 0.4996, 0.9132, 0.3781, 0.7385,\n",
       "         0.6294, 0.5134, 0.5790, 0.4502, 0.7691, 0.5093, 0.4395, 0.8370, 0.7839,\n",
       "         0.6168, 0.6715, 0.5600, 0.7007, 0.7986, 0.5924, 0.6686, 0.6341, 0.8116,\n",
       "         0.7938, 0.5458, 0.5605, 0.5416, 0.6534, 0.6499, 0.6531, 0.2160, 0.5838,\n",
       "         0.6293, 0.6537, 0.8390, 0.8748, 0.6863, 0.8791, 0.5647, 0.6716, 0.5615,\n",
       "         0.5567, 0.4708, 0.6657, 0.3342, 0.7206, 0.5809, 0.8061, 0.5558, 0.9394,\n",
       "         0.5579, 0.6141, 0.5993, 0.5750, 0.6657, 0.4896, 0.6012, 0.6688, 0.6469,\n",
       "         0.5132, 0.9432, 0.7289, 0.9454, 0.5051, 0.4474, 0.6520, 0.1281, 0.5646,\n",
       "         0.9987, 0.7525, 0.5757, 0.5484, 0.5843, 0.6827, 0.6025, 0.6675, 0.3596,\n",
       "         0.5952, 0.8588, 0.8951, 0.3670, 0.6358, 0.6071, 0.6361, 0.6918, 0.6346,\n",
       "         1.0264, 0.7837, 0.5887, 0.5132, 0.8036, 0.5819, 0.9829, 0.5437, 0.5826,\n",
       "         0.5553, 0.7032, 0.6130, 0.5159, 0.7086, 0.5677, 0.7511, 0.7684, 0.6100,\n",
       "         0.6563, 0.6252, 0.7011, 0.6339, 0.6395, 0.5804, 0.9236, 0.6249, 0.6576,\n",
       "         0.4960, 0.5808, 0.3664, 0.4829, 0.7802, 0.7497, 0.5738, 0.6534, 0.6210,\n",
       "         0.6450, 0.5607, 0.5794, 0.5796, 0.6210, 0.6276, 0.5715, 0.6367, 0.9488,\n",
       "         0.9847, 0.5888, 0.6822, 0.5742, 0.7841, 0.6085, 0.5990, 0.3590, 0.5028,\n",
       "         0.5130, 0.5797, 0.6978, 0.8407, 0.7293, 0.5385, 0.5562, 0.5905, 0.5759,\n",
       "         0.8045, 0.6403, 0.6526, 0.6083, 1.3029, 0.5917, 0.5516, 0.6995, 0.5660,\n",
       "         0.7513, 0.5651, 0.8764, 0.7217, 0.7983, 0.7796, 0.3472, 0.8697, 0.6434,\n",
       "         0.6753, 0.8193, 0.8783, 0.6265, 0.9726, 0.5869, 0.9207, 0.9322],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.2005e-02,  6.3732e-02, -1.6742e-02,  ...,  1.1150e-02,\n",
       "           8.1455e-02,  3.9890e-05],\n",
       "         [-6.6680e-03, -8.2777e-03, -5.2524e-02,  ..., -2.6035e-02,\n",
       "           4.7096e-02, -1.0076e-01],\n",
       "         [ 3.2190e-02, -7.3618e-02,  6.6170e-02,  ...,  4.2703e-02,\n",
       "           2.5812e-02, -1.2562e-02],\n",
       "         ...,\n",
       "         [-3.3605e-02,  4.7233e-02, -3.3413e-02,  ..., -1.4123e-02,\n",
       "           6.1619e-02,  1.1212e-02],\n",
       "         [-7.3825e-02,  1.2113e-01, -2.0895e-02,  ...,  7.3742e-04,\n",
       "          -4.1990e-03,  4.7413e-02],\n",
       "         [ 1.8459e-02,  1.5993e-02, -1.1371e-02,  ...,  1.3145e-02,\n",
       "           4.9214e-02,  4.6718e-02]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.5521, -0.3190,  0.3036,  ...,  0.0425,  0.1958, -0.5894],\n",
       "         [ 0.8850, -0.2300, -0.0316,  ...,  0.5962, -0.3229,  0.2752],\n",
       "         [ 0.3193, -0.5830, -0.7326,  ...,  0.0898, -0.4674, -0.4010],\n",
       "         ...,\n",
       "         [ 0.0521,  0.0787,  0.2260,  ...,  0.0324,  0.0116,  0.1336],\n",
       "         [ 0.2926, -0.1743,  0.0196,  ...,  0.5717,  0.1238,  0.1426],\n",
       "         [-0.0799,  0.4771,  0.1507,  ..., -0.0345,  0.2010, -0.2111]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0206, -0.1178, -0.4513,  ..., -0.5594,  0.2599, -0.4945],\n",
       "         [-1.3315,  1.0077,  0.4315,  ..., -0.0659, -0.5458,  1.8073],\n",
       "         [-1.1325,  0.3948, -0.0061,  ..., -0.4120, -0.5580,  0.3444],\n",
       "         ...,\n",
       "         [-0.5240,  0.3070,  0.0205,  ...,  0.9175,  0.6958, -1.1929],\n",
       "         [ 0.5396, -0.0353,  0.4786,  ...,  0.2669,  0.1573,  0.1904],\n",
       "         [ 0.3927, -0.0396, -0.8246,  ..., -0.0479, -0.1090, -1.2519]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0877,  0.1659, -0.1843,  ..., -1.1802, -0.5592,  0.2612],\n",
       "         [ 0.6112, -0.0375,  0.1331,  ...,  0.1704,  0.8184, -0.3970],\n",
       "         [-0.1181,  0.3922, -0.7358,  ...,  0.7659,  0.5450, -0.2928],\n",
       "         ...,\n",
       "         [-0.0406, -0.2667,  0.5189,  ...,  1.4981,  1.1648, -0.1131],\n",
       "         [-0.0315,  0.1357, -0.4053,  ...,  1.2667,  0.5401, -0.6525],\n",
       "         [-0.2788,  0.0906, -1.2093,  ...,  0.2230, -1.9167,  0.9861]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1176, 0.2193, 0.1062, 0.1732, 0.0643, 0.1448, 0.1562, 0.0673, 0.0667,\n",
       "         0.1424, 0.1404, 0.0600, 0.1107, 0.1426, 0.1025, 0.1071, 0.1280, 0.1199,\n",
       "         0.1454, 0.1223, 0.1208, 0.1149, 0.1743, 0.0519, 0.1352, 0.1053, 0.1627,\n",
       "         0.0767, 0.1405, 0.1556, 0.1339, 0.0856, 0.1804, 0.1357, 0.1266, 0.0942,\n",
       "         0.1364, 0.1443, 0.1600, 0.1188, 0.1406, 0.1190, 0.0764, 0.1758, 0.1530,\n",
       "         0.1056, 0.0518, 0.1197, 0.1324, 0.1393, 0.1599, 0.1670, 0.1252, 0.0645,\n",
       "         0.0944, 0.1653, 0.1328, 0.1440, 0.1313, 0.0951, 0.1450, 0.1146, 0.2029,\n",
       "         0.1548, 0.1380, 0.1451, 0.1366, 0.1372, 0.0674, 0.1932, 0.1762, 0.1223,\n",
       "         0.1439, 0.1272, 0.1390, 0.1094, 0.1595, 0.0933, 0.1285, 0.1065, 0.1510,\n",
       "         0.1562, 0.1359, 0.1516, 0.1103, 0.1094, 0.1219, 0.0823, 0.1507, 0.3013,\n",
       "         0.1331, 0.1151, 0.0895, 0.1586, 0.0895, 0.1673, 0.0414, 0.0994, 0.1650,\n",
       "         0.1227, 0.1495, 0.1317, 0.1131, 0.0867, 0.1232, 0.1342, 0.1176, 0.1972,\n",
       "         0.1547, 0.1026, 0.1574, 0.1823, 0.1333, 0.1684, 0.1455, 0.1608, 0.1688,\n",
       "         0.1444, 0.1161, 0.1244, 0.1195, 0.1336, 0.1402, 0.1029, 0.1304, 0.1983,\n",
       "         0.1444, 0.1495, 0.1247, 0.1587, 0.1421, 0.1212, 0.1448, 0.0998, 0.0792,\n",
       "         0.1388, 0.1412, 0.1068, 0.0966, 0.1393, 0.1235, 0.0558, 0.1712, 0.1128,\n",
       "         0.1425, 0.1307, 0.1374, 0.1329, 0.1536, 0.0328, 0.1270, 0.1303, 0.0785,\n",
       "         0.1286, 0.1132, 0.1227, 0.1313, 0.1214, 0.1647, 0.1132, 0.1206, 0.1616,\n",
       "         0.1190, 0.1121, 0.1258, 0.1690, 0.1531, 0.1329, 0.1263, 0.1216, 0.0945,\n",
       "         0.1813, 0.0950, 0.1317, 0.1574, 0.1106, 0.0949, 0.0813, 0.1105, 0.1472,\n",
       "         0.1298, 0.1689, 0.2280, 0.2584, 0.0695, 0.1333, 0.1261, 0.1581, 0.1210,\n",
       "         0.1546, 0.1272, 0.0822, 0.1267, 0.1462, 0.1262, 0.1300, 0.1153, 0.1519,\n",
       "         0.1122, 0.1538, 0.1348, 0.1320, 0.1450, 0.1247, 0.1536, 0.1209, 0.1904,\n",
       "         0.1463, 0.1959, 0.1417, 0.1485, 0.1425, 0.1361, 0.2464, 0.1504, 0.1425,\n",
       "         0.1807, 0.1039, 0.1516, 0.1150, 0.1256, 0.1242, 0.1218, 0.1502, 0.1200,\n",
       "         0.1303, 0.0980, 0.1396, 0.1758, 0.1441, 0.1421, 0.1227, 0.0557, 0.0999,\n",
       "         0.1538, 0.1905, 0.1935, 0.0647, 0.1174, 0.0464, 0.0903, 0.1256, 0.1080,\n",
       "         0.1141, 0.0848, 0.0337, 0.1725, 0.1775, 0.1009, 0.1541, 0.1155, 0.1455,\n",
       "         0.1649, 0.1110, 0.1293, 0.1431, 0.1819, 0.0763, 0.0798, 0.1393, 0.1338,\n",
       "         0.1412, 0.1382, 0.0908, 0.1853, 0.1480, 0.1427, 0.1369, 0.0756, 0.0978,\n",
       "         0.1161, 0.2041, 0.1373, 0.0633, 0.1495, 0.0698, 0.1451, 0.1381, 0.1341,\n",
       "         0.1194, 0.0717, 0.1073, 0.1472, 0.1417, 0.1387, 0.1333, 0.1632, 0.1788,\n",
       "         0.1267, 0.1425, 0.1086, 0.1339, 0.1135, 0.1288, 0.2867, 0.1198, 0.1737,\n",
       "         0.1367, 0.1576, 0.0776, 0.1740, 0.1473, 0.2890, 0.1384, 0.0612, 0.1377,\n",
       "         0.1042, 0.0711, 0.1952, 0.1044, 0.1523, 0.1259, 0.1010, 0.1631, 0.1433,\n",
       "         0.1138, 0.1172, 0.1468, 0.0964, 0.1264, 0.1420, 0.1443, 0.1264, 0.0690,\n",
       "         0.1059, 0.1398, 0.1526, 0.1397, 0.1587, 0.0772, 0.1287, 0.1756, 0.1128,\n",
       "         0.1424, 0.1229, 0.1544, 0.1407, 0.1582, 0.1075, 0.1641, 0.0816, 0.1119,\n",
       "         0.1663, 0.0676, 0.1239, 0.0692, 0.1288, 0.1147, 0.0893, 0.1901, 0.1990,\n",
       "         0.1267, 0.1315, 0.1269, 0.1358, 0.1209, 0.1104, 0.1539, 0.1152, 0.1445,\n",
       "         0.1754, 0.1369, 0.1276, 0.0648, 0.1403, 0.1362, 0.1391, 0.0420, 0.1151,\n",
       "         0.1441, 0.1354, 0.1685, 0.1609, 0.1241, 0.1610, 0.1466, 0.1487, 0.1148,\n",
       "         0.1274, 0.0745, 0.1631, 0.0836, 0.1353, 0.1405, 0.1390, 0.1379, 0.0869,\n",
       "         0.1280, 0.1075, 0.1495, 0.1390, 0.1354, 0.0600, 0.1543, 0.0946, 0.1422,\n",
       "         0.1036, 0.1443, 0.1512, 0.1885, 0.1023, 0.0660, 0.0746, 0.0326, 0.1319,\n",
       "         0.1122, 0.1620, 0.1304, 0.1230, 0.1197, 0.0891, 0.0801, 0.0961, 0.0849,\n",
       "         0.1285, 0.1644, 0.1396, 0.0666, 0.1533, 0.1520, 0.1352, 0.1450, 0.1384,\n",
       "         0.1075, 0.1655, 0.1380, 0.1266, 0.1743, 0.1276, 0.1987, 0.1169, 0.0984,\n",
       "         0.1142, 0.1321, 0.1285, 0.1128, 0.1232, 0.1288, 0.1131, 0.1494, 0.1284,\n",
       "         0.1553, 0.1453, 0.1288, 0.1639, 0.1318, 0.1203, 0.1688, 0.1572, 0.1671,\n",
       "         0.1140, 0.1361, 0.0621, 0.0749, 0.1657, 0.1601, 0.1359, 0.1575, 0.1248,\n",
       "         0.1327, 0.1318, 0.0797, 0.1301, 0.1420, 0.1599, 0.1329, 0.1163, 0.0799,\n",
       "         0.1277, 0.1253, 0.1531, 0.1174, 0.1313, 0.1478, 0.1384, 0.0812, 0.1287,\n",
       "         0.0477, 0.1529, 0.1018, 0.1239, 0.1533, 0.1121, 0.1352, 0.1296, 0.1251,\n",
       "         0.1672, 0.1240, 0.1360, 0.1217, 0.2598, 0.1384, 0.1457, 0.1453, 0.1372,\n",
       "         0.1613, 0.1308, 0.0840, 0.1086, 0.1546, 0.1889, 0.0662, 0.1542, 0.1396,\n",
       "         0.1259, 0.1713, 0.1542, 0.0916, 0.2047, 0.1280, 0.1707, 0.0756],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0152,  0.0994,  0.0792,  ..., -0.0501,  0.0616,  0.0792],\n",
       "         [-0.0015,  0.0680,  0.0241,  ..., -0.0346, -0.0443, -0.1205],\n",
       "         [-0.0970,  0.0172,  0.0303,  ..., -0.0044,  0.0593, -0.0256],\n",
       "         ...,\n",
       "         [-0.1306,  0.0171,  0.0427,  ..., -0.0007,  0.0470, -0.0680],\n",
       "         [ 0.0107, -0.0011, -0.0867,  ...,  0.1429,  0.0816, -0.2077],\n",
       "         [ 0.0764, -0.0559, -0.0339,  ...,  0.0376, -0.0586,  0.0323]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.2574,  0.0591,  0.3238,  ..., -0.2205,  0.2933, -0.1980],\n",
       "         [-0.4273,  0.5320, -0.0810,  ...,  0.3331,  0.5280,  0.2656],\n",
       "         [-0.1896,  0.0638, -0.1948,  ..., -0.1093, -0.1733, -0.5588],\n",
       "         ...,\n",
       "         [-0.4990, -0.1448,  0.2195,  ...,  0.0947,  0.1118,  0.0109],\n",
       "         [ 0.1565, -0.2580,  0.1049,  ..., -0.1390, -0.4217,  0.0538],\n",
       "         [ 0.0193, -0.5680,  0.2138,  ...,  0.1031, -0.4581,  0.3135]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0117,  0.4345,  0.8025,  ...,  0.4978,  0.2031, -0.1593],\n",
       "         [-0.0959,  0.2362, -0.4451,  ..., -0.6783,  0.1898,  0.3004],\n",
       "         [-0.5390, -0.7548, -0.3996,  ...,  0.5407,  0.2343, -0.3807],\n",
       "         ...,\n",
       "         [-0.5580, -0.3472, -0.4906,  ...,  0.2332,  0.4995,  0.0462],\n",
       "         [-0.2394, -0.0141, -0.3357,  ...,  1.0062,  0.4937, -0.3227],\n",
       "         [-0.2846, -1.2084,  0.1202,  ...,  0.0122,  0.3795, -0.2273]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 9.5903e-02,  5.2210e-01, -1.0104e+00,  ...,  1.2517e-01,\n",
       "          -2.2583e-01, -5.0405e-01],\n",
       "         [ 2.3476e-01, -6.4562e-01, -3.2118e-02,  ..., -1.5272e-01,\n",
       "          -2.3944e-01, -5.5530e-01],\n",
       "         [ 2.5562e-02, -2.2718e-01, -1.3475e+00,  ..., -8.2130e-02,\n",
       "          -6.6902e-02,  7.9991e-02],\n",
       "         ...,\n",
       "         [-1.2474e-01, -4.5467e-01,  1.3144e+00,  ...,  5.3012e-01,\n",
       "          -2.3645e-01, -4.7506e-01],\n",
       "         [ 6.4761e-01,  2.4441e-01,  2.1300e-01,  ..., -1.3515e-01,\n",
       "          -1.1814e-01, -1.4487e-01],\n",
       "         [-8.1010e-01,  7.4952e-01,  4.4022e-01,  ...,  9.1024e-04,\n",
       "          -5.4687e-01, -1.6448e-01]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0781,  0.1949,  0.0699,  0.1121,  0.0598,  0.0934,  0.1112,  0.0636,\n",
       "          0.0637,  0.1150,  0.0941,  0.0379,  0.0935,  0.0964,  0.0775,  0.0786,\n",
       "          0.1027,  0.0802,  0.1064,  0.0841,  0.0825,  0.0879,  0.1090,  0.0333,\n",
       "          0.1090,  0.0750,  0.1139,  0.0503,  0.1040,  0.0967,  0.0864,  0.0759,\n",
       "          0.1062,  0.0976,  0.0851,  0.0616,  0.0933,  0.0971,  0.1063,  0.0736,\n",
       "          0.0878,  0.0894,  0.0559,  0.1159,  0.0882,  0.0824,  0.0245,  0.0723,\n",
       "          0.0978,  0.0911,  0.0975,  0.1108,  0.0854,  0.0489,  0.0745,  0.1083,\n",
       "          0.0922,  0.1075,  0.0937,  0.0891,  0.1251,  0.0808,  0.1327,  0.1021,\n",
       "          0.1343,  0.1011,  0.0903,  0.1078,  0.0533,  0.1101,  0.1311,  0.0778,\n",
       "          0.0983,  0.0893,  0.0837,  0.0942,  0.1041,  0.0728,  0.0966,  0.0717,\n",
       "          0.0917,  0.1054,  0.0887,  0.1059,  0.0824,  0.0808,  0.0831,  0.0629,\n",
       "          0.1014,  0.1570,  0.0926,  0.0755,  0.0847,  0.1145,  0.0985,  0.1002,\n",
       "          0.1587,  0.0788,  0.0932,  0.0850,  0.1086,  0.1054,  0.0769,  0.0714,\n",
       "          0.0986,  0.0958,  0.0770,  0.1208,  0.0981,  0.0759,  0.1104,  0.1320,\n",
       "          0.1052,  0.1212,  0.1086,  0.1050,  0.1382,  0.1118,  0.0827,  0.0889,\n",
       "          0.0804,  0.0927,  0.1056,  0.0862,  0.0929,  0.0584,  0.1011,  0.1077,\n",
       "          0.0981,  0.1080,  0.0908,  0.1015,  0.1012,  0.0769,  0.0679,  0.1048,\n",
       "          0.0899,  0.0734,  0.0683,  0.1047,  0.0930,  0.0564,  0.1240,  0.0809,\n",
       "          0.0991,  0.0801,  0.1214,  0.1123,  0.1074,  0.0487,  0.0934,  0.0960,\n",
       "          0.0527,  0.0886,  0.0801,  0.1035,  0.0988,  0.0792,  0.1237,  0.1067,\n",
       "          0.1072,  0.1029,  0.0892,  0.0789,  0.0932,  0.1183,  0.1031,  0.0946,\n",
       "          0.0936,  0.0912,  0.0699,  0.1324,  0.0877,  0.1178,  0.1148,  0.0711,\n",
       "          0.1043,  0.0620,  0.0983,  0.1122,  0.0906,  0.1089,  0.0746,  0.1986,\n",
       "          0.0670,  0.0780,  0.0890,  0.1007,  0.0885,  0.1077,  0.0983,  0.0608,\n",
       "          0.0956,  0.1100,  0.0916,  0.1011,  0.0888,  0.1049,  0.0849,  0.0973,\n",
       "          0.0942,  0.0991,  0.0993,  0.0858,  0.0934,  0.0849,  0.2000,  0.1050,\n",
       "          0.1249,  0.0962,  0.1032,  0.1127,  0.0948,  0.1513,  0.1094,  0.1025,\n",
       "          0.1216,  0.0740,  0.1031,  0.0837,  0.0975,  0.0807,  0.0865,  0.1004,\n",
       "          0.0863,  0.0863,  0.0854,  0.0934,  0.1348,  0.0930,  0.1065,  0.0867,\n",
       "          0.0516,  0.0758,  0.1728,  0.1224,  0.1034,  0.0514,  0.0805,  0.1005,\n",
       "          0.0681,  0.0810,  0.1010,  0.0813,  0.0736,  0.0679,  0.1127,  0.1119,\n",
       "          0.0556,  0.1041,  0.0959,  0.1030,  0.1126,  0.0864,  0.0956,  0.0856,\n",
       "          0.1247,  0.0728,  0.0507,  0.0898,  0.0835,  0.1018,  0.0980,  0.0549,\n",
       "          0.1066,  0.1172,  0.0967,  0.1018, -0.0815,  0.0737,  0.0821,  0.1196,\n",
       "          0.0825,  0.0379,  0.0971,  0.0527,  0.1090,  0.1012,  0.0881,  0.0803,\n",
       "          0.0738,  0.0608,  0.1048,  0.1027,  0.0983,  0.0873,  0.1311,  0.1305,\n",
       "          0.0947,  0.0994,  0.0836,  0.0965,  0.0811,  0.0849,  0.1156,  0.0888,\n",
       "          0.1091,  0.0938,  0.1196,  0.0555,  0.1112,  0.0881,  0.1627,  0.0976,\n",
       "          0.0470,  0.1042,  0.0706,  0.0462,  0.1718,  0.1292,  0.1156,  0.0826,\n",
       "          0.0711,  0.1211,  0.0919,  0.0958,  0.0863,  0.1118,  0.0759,  0.0858,\n",
       "          0.0942,  0.1109,  0.1029,  0.0704,  0.1017,  0.1059,  0.0934,  0.1037,\n",
       "          0.1111, -0.0669,  0.0925,  0.1086,  0.0818,  0.1024,  0.0919,  0.1134,\n",
       "          0.0915,  0.1017,  0.0690,  0.1070,  0.0562,  0.0883,  0.1102,  0.0620,\n",
       "          0.0821,  0.0507,  0.1026,  0.0688,  0.0624,  0.1290,  0.1424,  0.0894,\n",
       "          0.0904,  0.1007,  0.0855,  0.0871,  0.0893,  0.1091,  0.0728,  0.1098,\n",
       "          0.1047,  0.0921,  0.1035,  0.0614,  0.0966,  0.0840,  0.0979,  0.0424,\n",
       "          0.0885,  0.1045,  0.1070,  0.1081,  0.1124,  0.1002,  0.0819,  0.1010,\n",
       "          0.1106,  0.0729,  0.1009,  0.0500,  0.1108,  0.0666,  0.0997,  0.1072,\n",
       "          0.1070,  0.0877,  0.0735,  0.1031,  0.0793,  0.1103,  0.0984,  0.0965,\n",
       "          0.0879,  0.1060,  0.0803,  0.1017,  0.0700,  0.0970,  0.1068,  0.1396,\n",
       "          0.0653,  0.0539,  0.0970,  0.0321,  0.0955,  0.0958,  0.1109,  0.0928,\n",
       "          0.0917,  0.0942,  0.0920,  0.0606,  0.0721,  0.0527,  0.1006,  0.1193,\n",
       "          0.0812,  0.0660,  0.1013,  0.1148,  0.0960,  0.0998,  0.0785,  0.1209,\n",
       "          0.1048,  0.1029,  0.0928,  0.1319,  0.0932,  0.1363,  0.0999,  0.0698,\n",
       "          0.0916,  0.0968,  0.1079,  0.0747,  0.0797,  0.0876,  0.0813,  0.0974,\n",
       "          0.1065,  0.1225,  0.0991,  0.0876,  0.1189,  0.1052,  0.0858,  0.1020,\n",
       "          0.1050,  0.1073,  0.0811,  0.1007,  0.0321,  0.0642,  0.1138,  0.1116,\n",
       "          0.0965,  0.0979,  0.0762,  0.0946,  0.1111,  0.0430,  0.0983,  0.0934,\n",
       "          0.0996,  0.1053,  0.0901,  0.1668,  0.1031,  0.0935,  0.1203,  0.0957,\n",
       "          0.1005,  0.1012,  0.0922,  0.0516,  0.0944,  0.1079,  0.1162,  0.1149,\n",
       "          0.0897,  0.1217,  0.0904,  0.1006,  0.1094,  0.0964,  0.0987,  0.0828,\n",
       "          0.0848,  0.0958,  0.1911,  0.1023,  0.0886,  0.1021,  0.1067,  0.1158,\n",
       "          0.0974,  0.0899,  0.0901,  0.1029,  0.1212,  0.0524,  0.1000,  0.1056,\n",
       "          0.0881,  0.1120,  0.0990,  0.0967,  0.1363,  0.0844,  0.0888,  0.0569],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.1864, -0.5147,  0.5251,  ...,  0.1722, -0.0465,  0.3794],\n",
       "         [-0.4747, -0.5052,  0.3627,  ..., -0.0068, -1.2258, -0.1105],\n",
       "         [-0.1326,  0.0745, -0.5728,  ..., -0.1979, -0.4902,  0.6817],\n",
       "         ...,\n",
       "         [ 1.3795,  1.5477, -0.3892,  ...,  0.3582, -0.0587,  0.4528],\n",
       "         [-0.1778,  0.0676,  0.8260,  ..., -0.5271, -0.0423,  0.6617],\n",
       "         [-0.4912, -0.5908, -0.6865,  ..., -0.2238, -0.1258, -0.7785]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.2134, -0.0306,  0.9831,  ...,  0.2809,  0.0902, -0.2202],\n",
       "         [ 0.0821,  0.0962,  0.1364,  ..., -0.5791, -0.2901, -0.3665],\n",
       "         [ 0.0137, -0.1800,  0.0855,  ..., -0.1781,  0.1637,  0.1839],\n",
       "         ...,\n",
       "         [ 0.2381,  0.0105, -0.2301,  ...,  0.2532, -0.5382, -0.0207],\n",
       "         [-0.0555,  0.4326, -0.0590,  ...,  0.0344,  0.0443, -0.0157],\n",
       "         [-0.5288, -0.3219, -0.0370,  ...,  0.3506, -0.5138, -1.1922]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9338, 1.1422, 1.1518, 1.5268, 0.4217, 1.1625, 1.1123, 1.1807, 1.0990,\n",
       "         1.4022, 1.1971, 0.7708, 1.1667, 1.3702, 1.2087, 1.2746, 1.0959, 1.1049,\n",
       "         1.1005, 1.0682, 1.0094, 0.9810, 1.2706, 0.6848, 1.2605, 0.8752, 1.3013,\n",
       "         0.7907, 1.1531, 1.2661, 1.0759, 0.8528, 1.5913, 1.1607, 1.1283, 1.0142,\n",
       "         1.1348, 1.2501, 1.4071, 0.9036, 1.1369, 1.2447, 0.5619, 1.4333, 1.3770,\n",
       "         0.9138, 0.4548, 0.9516, 1.1038, 1.1371, 1.1115, 1.3285, 0.9612, 1.0750,\n",
       "         0.8941, 1.1142, 1.0286, 1.1648, 1.1572, 1.0030, 1.6464, 0.9886, 1.2809,\n",
       "         1.2718, 1.8965, 1.1656, 1.1253, 1.3337, 0.9991, 1.3169, 1.4157, 1.0247,\n",
       "         1.1609, 1.0699, 1.2126, 1.0882, 1.3581, 0.8837, 0.9968, 0.7943, 1.1748,\n",
       "         1.3881, 1.2197, 1.1169, 1.0823, 0.9741, 1.1464, 0.8476, 1.1948, 0.6806,\n",
       "         0.9824, 0.9677, 1.1132, 1.2973, 1.2118, 1.4112, 1.7257, 0.9308, 1.6770,\n",
       "         1.0635, 1.5065, 1.2338, 0.9867, 0.7620, 1.0823, 1.1597, 0.9604, 1.5515,\n",
       "         1.2432, 0.9195, 1.2317, 1.2174, 1.0899, 1.5894, 1.3633, 1.2680, 1.7599,\n",
       "         1.2536, 1.0049, 1.1602, 0.9958, 1.1390, 1.3720, 0.9769, 1.1635, 1.5320,\n",
       "         1.1466, 1.2482, 1.1367, 1.3200, 1.2071, 1.1102, 1.2987, 0.8429, 0.9853,\n",
       "         1.2797, 1.1641, 0.6726, 0.8558, 1.0440, 1.0384, 0.8862, 1.5113, 0.9453,\n",
       "         1.3662, 1.1711, 1.1517, 1.8147, 1.1848, 0.3853, 1.1256, 1.1812, 0.9014,\n",
       "         1.2566, 0.9519, 1.0803, 1.1734, 0.7816, 1.2042, 1.6158, 1.1529, 1.1059,\n",
       "         1.0099, 0.8991, 1.2434, 1.2327, 1.1810, 1.0878, 1.1251, 1.1790, 0.9061,\n",
       "         1.4636, 1.1600, 1.2467, 1.4975, 0.9799, 1.3118, 0.7549, 1.0547, 1.2562,\n",
       "         1.2031, 1.2530, 0.6566, 1.2963, 0.8764, 1.1179, 1.1221, 1.3334, 1.1587,\n",
       "         1.1587, 1.1034, 0.7491, 1.1374, 1.2180, 1.1674, 1.2558, 0.9859, 1.3537,\n",
       "         0.9778, 1.2430, 1.1955, 1.2841, 1.2058, 1.0781, 1.2813, 1.0889, 0.8403,\n",
       "         1.1193, 1.7144, 1.2293, 1.3795, 1.3296, 1.2162, 0.7992, 1.2845, 1.2477,\n",
       "         1.4092, 0.9644, 1.0651, 1.0651, 1.1129, 0.9874, 1.1527, 1.1458, 0.9049,\n",
       "         1.1378, 1.0104, 1.2022, 1.3758, 1.2674, 1.2024, 1.0911, 0.9302, 1.0877,\n",
       "         1.2567, 1.7531, 1.4166, 0.8725, 0.9364, 1.3838, 0.8643, 1.1317, 1.0022,\n",
       "         1.0807, 1.2709, 1.4093, 1.2821, 1.5426, 0.5736, 1.1275, 1.0378, 1.3910,\n",
       "         1.2788, 0.9429, 1.1434, 1.1021, 1.4159, 1.0506, 0.7926, 1.1721, 1.0482,\n",
       "         1.2287, 1.2719, 0.7896, 1.5398, 1.2932, 1.1131, 1.0269, 1.3270, 0.9244,\n",
       "         0.8889, 1.3966, 1.1657, 0.7810, 1.2455, 1.2459, 1.2307, 1.3913, 1.0428,\n",
       "         1.0032, 1.0825, 0.9247, 1.5104, 1.1763, 1.2053, 1.1039, 1.2130, 1.3554,\n",
       "         1.2493, 1.1734, 1.0070, 1.2519, 1.0680, 1.1966, 0.9187, 0.8515, 1.4172,\n",
       "         1.2883, 1.4152, 0.9443, 1.2843, 1.1734, 1.2886, 1.3305, 0.7415, 1.1617,\n",
       "         0.9509, 0.7909, 1.2927, 1.8112, 1.4492, 1.0115, 0.8070, 1.2204, 1.0690,\n",
       "         1.0439, 1.0456, 1.2443, 0.9203, 1.1033, 1.0891, 1.3071, 1.3707, 1.2720,\n",
       "         1.7641, 1.2851, 1.1363, 1.5990, 1.2709, 1.2547, 1.0962, 1.4434, 0.9453,\n",
       "         1.4593, 1.1132, 1.2932, 1.1465, 1.2205, 0.8724, 1.5467, 1.0016, 1.0885,\n",
       "         1.2939, 0.8148, 1.0952, 0.7678, 1.2097, 0.9002, 0.7867, 1.6836, 1.2495,\n",
       "         1.0645, 1.1468, 1.1454, 1.1550, 1.0513, 0.9244, 1.3196, 0.9395, 1.2450,\n",
       "         1.5284, 1.1414, 1.1609, 1.1491, 1.1749, 1.0800, 1.3997, 0.6084, 0.9563,\n",
       "         1.2211, 1.2776, 1.2845, 1.2996, 1.2161, 0.9852, 1.2266, 1.3098, 0.9674,\n",
       "         1.3064, 0.9075, 1.4220, 0.8920, 1.1165, 1.2345, 1.1047, 1.0909, 1.1079,\n",
       "         1.1558, 0.8619, 1.3018, 1.1295, 1.3352, 0.8341, 1.1433, 1.0346, 1.0532,\n",
       "         0.8677, 1.3502, 1.1852, 1.6031, 0.8384, 0.6629, 1.3524, 0.2986, 1.2649,\n",
       "         1.2269, 1.6688, 1.1816, 1.1315, 1.0815, 1.4233, 0.9323, 0.8571, 0.7155,\n",
       "         1.1714, 1.4995, 1.2057, 1.1925, 1.3338, 1.4660, 1.1300, 1.3099, 1.0412,\n",
       "         1.9923, 1.2214, 1.2262, 1.0486, 1.4295, 1.2273, 1.4811, 1.1888, 0.9654,\n",
       "         1.1480, 1.1563, 1.2647, 0.9345, 1.0126, 1.1079, 0.9719, 1.0904, 1.0929,\n",
       "         1.4732, 1.3810, 1.2251, 1.3942, 1.2515, 1.1911, 1.3698, 1.3916, 1.4402,\n",
       "         0.9149, 1.2497, 0.7704, 0.7916, 1.2774, 1.4058, 1.2560, 1.2515, 1.1170,\n",
       "         1.3890, 1.2051, 1.3263, 1.1742, 1.2340, 1.2088, 1.3294, 1.1652, 1.4906,\n",
       "         1.0745, 1.1556, 1.4191, 1.2227, 1.1457, 1.0772, 1.1361, 0.6536, 1.0061,\n",
       "         1.3901, 1.2117, 1.2852, 0.9577, 1.4707, 1.0604, 1.2993, 1.3965, 1.1537,\n",
       "         1.3001, 1.0547, 1.0230, 1.0319, 2.2678, 1.3134, 0.9256, 1.2708, 1.2092,\n",
       "         1.2898, 1.1807, 1.2192, 1.0491, 1.2286, 1.5860, 0.9359, 1.2613, 1.2697,\n",
       "         1.1999, 1.2245, 1.2918, 1.0082, 1.4839, 0.9992, 1.1902, 0.8546],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0381,  0.0135,  0.0063,  ..., -0.0082, -0.0013, -0.0379],\n",
       "         [-0.0592, -0.0248,  0.0049,  ...,  0.0098, -0.0769,  0.0232],\n",
       "         [-0.0325, -0.0129, -0.0515,  ...,  0.0275,  0.0087, -0.0550],\n",
       "         ...,\n",
       "         [ 0.1575, -0.0431,  0.0060,  ...,  0.0265, -0.0653,  0.0537],\n",
       "         [ 0.0834,  0.0543, -0.0199,  ...,  0.0571, -0.0037, -0.0262],\n",
       "         [ 0.0275,  0.0293,  0.1142,  ..., -0.0608, -0.0422,  0.0249]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.8830,  0.2445, -0.2481,  ...,  0.1229, -0.1314,  0.1369],\n",
       "         [-0.0291, -0.1432, -0.4395,  ..., -0.5399, -0.8537,  1.1336],\n",
       "         [-0.0038,  0.0102, -0.3050,  ...,  0.3375,  0.3733,  0.4407],\n",
       "         ...,\n",
       "         [ 0.0020, -0.4680, -0.1855,  ...,  0.0925,  0.7542, -0.6008],\n",
       "         [ 0.3358,  0.1737, -0.6947,  ...,  0.0960, -0.4907,  0.1209],\n",
       "         [-0.1295, -0.2197, -0.4233,  ..., -0.4888,  0.3656,  0.4023]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.3164, -0.3801, -0.5962,  ..., -0.3492, -0.1850, -1.6669],\n",
       "         [ 0.6272, -1.0993, -0.0054,  ..., -0.5489,  0.0567,  0.9844],\n",
       "         [ 0.5908,  1.5061, -0.7911,  ..., -0.5531,  0.4200,  1.1657],\n",
       "         ...,\n",
       "         [ 1.1624, -0.2729,  0.1763,  ..., -1.5284,  1.2585, -0.1235],\n",
       "         [-0.4512,  0.5697,  0.9684,  ...,  0.3474, -0.2109, -0.1014],\n",
       "         [-1.1109,  0.1638,  0.2143,  ...,  0.2157,  0.2058,  0.3304]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.8330,  0.0892,  0.1654,  ...,  0.5516,  0.3319,  1.0269],\n",
       "         [-0.0761, -1.0546, -0.1244,  ...,  0.1333,  0.3899, -0.3137],\n",
       "         [-0.0118, -0.6679, -0.9190,  ...,  0.0043,  0.5833, -1.4179],\n",
       "         ...,\n",
       "         [ 0.4702, -0.6323,  0.4614,  ..., -0.2804,  0.2958, -1.8200],\n",
       "         [ 0.8646,  0.9543,  0.0721,  ..., -0.2025, -0.4504, -0.5794],\n",
       "         [-0.1018,  0.7448,  0.3941,  ..., -0.5211, -0.8610,  0.0608]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1253,  0.1779,  0.1140,  0.1863,  0.0688,  0.1619,  0.1671,  0.0889,\n",
       "          0.0719,  0.1655,  0.1640,  0.0682,  0.1492,  0.1714,  0.0982,  0.1221,\n",
       "          0.1423,  0.1379,  0.1535,  0.1482,  0.1475,  0.1300,  0.1809,  0.0553,\n",
       "          0.1615,  0.1189,  0.1824,  0.0874,  0.1529,  0.1764,  0.1495,  0.0989,\n",
       "          0.1951,  0.1534,  0.1314,  0.1005,  0.1603,  0.1643,  0.1725,  0.1191,\n",
       "          0.1524,  0.1588,  0.0843,  0.2059,  0.1603,  0.1322,  0.0382,  0.1354,\n",
       "          0.1612,  0.1574,  0.1612,  0.1605,  0.1277,  0.0698,  0.0953,  0.1755,\n",
       "          0.1467,  0.1500,  0.1482,  0.1010,  0.1790,  0.1298,  0.2025,  0.1650,\n",
       "          0.1566,  0.1552,  0.1594,  0.1780,  0.0793,  0.1799,  0.1888,  0.1357,\n",
       "          0.1475,  0.1398,  0.1467,  0.1283,  0.1767,  0.1241,  0.1334,  0.1048,\n",
       "          0.1233,  0.1670,  0.1506,  0.1553,  0.1384,  0.1171,  0.1346,  0.1205,\n",
       "          0.1606,  0.1738,  0.1269,  0.1252,  0.1045,  0.1569,  0.1093,  0.1772,\n",
       "          0.0381,  0.1246,  0.1933,  0.1207,  0.1774,  0.1538,  0.1151,  0.0988,\n",
       "          0.1545,  0.1492,  0.1366,  0.1998,  0.1752,  0.1144,  0.1680,  0.1834,\n",
       "          0.1540,  0.1731,  0.1766,  0.1593,  0.2111,  0.1728,  0.1264,  0.1489,\n",
       "          0.1404,  0.1569,  0.1679,  0.1004,  0.1488,  0.1207,  0.1599,  0.1581,\n",
       "          0.1421,  0.1751,  0.1487,  0.1553,  0.1566,  0.1344,  0.1042,  0.1637,\n",
       "          0.1567,  0.1087,  0.1197,  0.1492,  0.1400,  0.0750,  0.1932,  0.1359,\n",
       "          0.1715,  0.1307,  0.1518,  0.1115,  0.1612,  0.0299,  0.1495,  0.1549,\n",
       "          0.0865,  0.1601,  0.1251,  0.1493,  0.1605,  0.1203,  0.1622,  0.1335,\n",
       "          0.1565,  0.1617,  0.1317,  0.1157,  0.1390,  0.1863,  0.1365,  0.1336,\n",
       "          0.1533,  0.1394,  0.1160,  0.1774,  0.1102,  0.1492,  0.1774,  0.1308,\n",
       "          0.1012,  0.0949,  0.1402,  0.1655,  0.1416,  0.1690,  0.2821,  0.2159,\n",
       "          0.1073,  0.1478,  0.1512,  0.1643,  0.1411,  0.1729,  0.1196,  0.0962,\n",
       "          0.1546,  0.1484,  0.1453,  0.1569,  0.1440,  0.1731,  0.1345,  0.1738,\n",
       "          0.1584,  0.1372,  0.1505,  0.1430,  0.1719,  0.1272,  0.1896,  0.1749,\n",
       "          0.1897,  0.1671,  0.1882,  0.1795,  0.1301,  0.1338,  0.1634,  0.1670,\n",
       "          0.1844,  0.1089,  0.1436,  0.1408,  0.1620,  0.1350,  0.1392,  0.1617,\n",
       "          0.1206,  0.1452,  0.1185,  0.1464,  0.1787,  0.1595,  0.1582,  0.1460,\n",
       "          0.0589,  0.1112,  0.1516,  0.2049,  0.1837,  0.0775,  0.1389,  0.0449,\n",
       "          0.1038,  0.1258,  0.1163,  0.1438,  0.0930,  0.0341,  0.1790,  0.2139,\n",
       "          0.0973,  0.1570,  0.1379,  0.1549,  0.1654,  0.1392,  0.1572,  0.1511,\n",
       "          0.1940,  0.0903,  0.0942,  0.1419,  0.1444,  0.1642,  0.1734,  0.1002,\n",
       "          0.1988,  0.1637,  0.1526,  0.1504,  0.0848,  0.1183,  0.1366,  0.1897,\n",
       "          0.1372,  0.0522,  0.1588,  0.1094,  0.1704,  0.1714,  0.1436,  0.1250,\n",
       "          0.0843,  0.1182,  0.1789,  0.1634,  0.1661,  0.1536,  0.1642,  0.1689,\n",
       "          0.1513,  0.1566,  0.1295,  0.1727,  0.1370,  0.1397,  0.1787,  0.1216,\n",
       "          0.1906,  0.1612,  0.2057,  0.0791,  0.1856,  0.1631,  0.1677,  0.1717,\n",
       "          0.0704,  0.1615,  0.1222,  0.0729,  0.1355,  0.0998,  0.1809,  0.1479,\n",
       "          0.1006,  0.1531,  0.1576,  0.1370,  0.1407,  0.1605,  0.1072,  0.1328,\n",
       "          0.1405,  0.1642,  0.1804,  0.0834,  0.1087,  0.1662,  0.1436,  0.1818,\n",
       "          0.1895,  0.0892,  0.1487,  0.1731,  0.1225,  0.1728,  0.1416,  0.1474,\n",
       "          0.1357,  0.1793,  0.1282,  0.1887,  0.0807,  0.1247,  0.1735,  0.0890,\n",
       "          0.1381,  0.0795,  0.1547,  0.1234,  0.1084,  0.2158,  0.1985,  0.1294,\n",
       "          0.1392,  0.1472,  0.1414,  0.1198,  0.1185,  0.1694,  0.1222,  0.1570,\n",
       "          0.2001,  0.1592,  0.1490,  0.0807,  0.1668,  0.1196,  0.1579,  0.0344,\n",
       "          0.1286,  0.1687,  0.1612,  0.1668,  0.1684,  0.1432,  0.1285,  0.1733,\n",
       "          0.1671,  0.1202,  0.1578,  0.0845,  0.1896,  0.0692,  0.1573,  0.1548,\n",
       "          0.1496,  0.1505,  0.0885,  0.1475,  0.1204,  0.1560,  0.1571,  0.1586,\n",
       "          0.0671,  0.1690,  0.1096,  0.1440,  0.1211,  0.1353,  0.1518,  0.1940,\n",
       "          0.0989,  0.0694,  0.0853,  0.0298,  0.1383,  0.1280,  0.1940,  0.1509,\n",
       "          0.1403,  0.1374,  0.0856,  0.0922,  0.1176,  0.0838,  0.1593,  0.1725,\n",
       "          0.1455,  0.0742,  0.1643,  0.1793,  0.1705,  0.1647,  0.1360,  0.1183,\n",
       "          0.1773,  0.1552,  0.1431,  0.1828,  0.1518,  0.2022,  0.1436,  0.1115,\n",
       "          0.1447,  0.1560,  0.1588,  0.1280,  0.1447,  0.1544,  0.1217,  0.1458,\n",
       "          0.1539,  0.1845,  0.1733,  0.1561,  0.1932,  0.1470,  0.1460,  0.1680,\n",
       "          0.1882,  0.1815,  0.1336,  0.1712,  0.0838,  0.0808,  0.1774,  0.1822,\n",
       "          0.1558,  0.1686,  0.1410,  0.1723,  0.1392,  0.1028,  0.1478,  0.1569,\n",
       "          0.1588,  0.1622,  0.1324,  0.0915,  0.1240,  0.1554,  0.1780,  0.1396,\n",
       "          0.1399,  0.1682,  0.1457,  0.0934,  0.1529,  0.0452,  0.1838,  0.1108,\n",
       "          0.1330,  0.1729,  0.1266,  0.1606,  0.1718,  0.1508,  0.1719,  0.1375,\n",
       "          0.1435,  0.1337,  0.0801,  0.1671,  0.1605,  0.1451,  0.1686,  0.1750,\n",
       "          0.1491, -0.0887,  0.1239,  0.1588,  0.2045,  0.0762,  0.1574,  0.1609,\n",
       "          0.1502,  0.1848,  0.1574,  0.1007,  0.1984,  0.1444,  0.1916,  0.0930],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.8093e-02, -7.4260e-04,  2.1715e-02,  ...,  1.1666e-01,\n",
       "           7.8033e-02,  2.3393e-02],\n",
       "         [-2.2344e-02,  1.4892e-02,  1.8487e-02,  ..., -1.1681e-02,\n",
       "          -5.7427e-02,  5.3024e-02],\n",
       "         [ 1.0194e-01,  1.1181e-02, -4.0528e-02,  ..., -1.8579e-02,\n",
       "           8.8261e-02, -1.2203e-02],\n",
       "         ...,\n",
       "         [ 1.0562e-01,  4.9667e-02, -4.3529e-02,  ..., -9.8635e-02,\n",
       "           3.7456e-02, -1.3849e-02],\n",
       "         [ 7.3353e-02,  1.8106e-01, -6.4208e-02,  ..., -7.4889e-02,\n",
       "           2.3860e-02, -1.3118e-01],\n",
       "         [-2.1452e-02, -8.5814e-02, -9.9268e-05,  ..., -7.3112e-02,\n",
       "          -1.2369e-01, -2.3819e-03]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0926, -0.2554, -0.3489,  ..., -0.2913, -0.1103, -0.4599],\n",
       "         [ 0.0829,  0.4990, -0.4899,  ..., -0.0651,  0.0016, -0.7366],\n",
       "         [ 0.2306,  0.5125,  0.4556,  ...,  0.3039,  0.1364, -0.0243],\n",
       "         ...,\n",
       "         [-0.4255,  0.3397, -0.0771,  ..., -0.2576, -0.4437, -0.1317],\n",
       "         [ 0.2152, -0.1116, -0.0620,  ...,  0.2318, -0.6592,  0.4399],\n",
       "         [-0.4389,  0.0245, -0.3713,  ..., -0.0397,  0.1465, -0.0310]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.4163,  0.9435,  0.5419,  ..., -1.0272,  0.3591, -0.1086],\n",
       "         [-0.3851, -1.1703, -1.3298,  ..., -1.1803,  0.5468,  0.6054],\n",
       "         [ 1.7142, -1.1546,  0.7388,  ...,  0.3977, -1.9067, -0.7888],\n",
       "         ...,\n",
       "         [-0.5252,  0.1894, -0.1986,  ..., -0.7276, -1.2671,  0.4541],\n",
       "         [ 0.7595, -1.3545,  0.2573,  ...,  0.5327, -0.1587,  1.5547],\n",
       "         [ 0.9795,  0.7202, -0.8217,  ...,  0.7160, -0.7372,  0.4420]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1737, -1.6255, -0.5182,  ..., -0.2070, -1.3844, -0.0249],\n",
       "         [ 0.5707, -0.6563, -2.0284,  ...,  0.8154,  0.1197, -0.3568],\n",
       "         [-0.2443, -0.5871,  0.4864,  ...,  0.9264, -0.9809,  0.3376],\n",
       "         ...,\n",
       "         [-0.2868,  1.0631,  0.3117,  ..., -0.2751,  0.8127, -0.1665],\n",
       "         [-0.3964, -0.2761, -0.3205,  ..., -0.2826, -0.1654, -0.2009],\n",
       "         [ 0.3297,  0.3863,  0.1468,  ..., -1.0025, -0.5463,  0.7142]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0756,  0.1222,  0.0660,  0.1047,  0.0491,  0.0850,  0.1002, -0.0662,\n",
       "          0.0743,  0.0862,  0.0801,  0.0337,  0.0877,  0.0924,  0.0647,  0.0823,\n",
       "          0.0839,  0.0859,  0.0897,  0.0913,  0.0787,  0.0798,  0.0926,  0.0379,\n",
       "          0.0955,  0.0729,  0.0925,  0.0622,  0.0956,  0.0973,  0.0865,  0.0661,\n",
       "          0.1044,  0.0834,  0.0886,  0.0652,  0.0840,  0.0858,  0.1061,  0.0785,\n",
       "          0.0781,  0.0877,  0.0772,  0.1092,  0.0802,  0.0788,  0.0271,  0.0751,\n",
       "          0.0892,  0.0818,  0.0997,  0.0931,  0.0735,  0.0388,  0.0719,  0.0962,\n",
       "          0.0806,  0.0978,  0.0821,  0.0824,  0.1093,  0.0615,  0.1155,  0.0963,\n",
       "          0.1158,  0.0876,  0.0880,  0.1006,  0.0594,  0.1096,  0.1207,  0.0721,\n",
       "          0.0807,  0.0905,  0.0831,  0.0885,  0.1057,  0.0666,  0.0742,  0.0664,\n",
       "          0.0876,  0.0995,  0.0821,  0.1044,  0.0742,  0.0782,  0.0753,  0.0651,\n",
       "          0.0971,  0.0698,  0.0766,  0.0790,  0.0688,  0.0976,  0.0809,  0.0939,\n",
       "          0.1229,  0.0667,  0.1200,  0.0658,  0.0918,  0.0896,  0.0788,  0.0786,\n",
       "          0.0872,  0.0856,  0.0764,  0.1116,  0.0954,  0.0736,  0.1088,  0.1032,\n",
       "          0.0919,  0.1035,  0.1034,  0.1086,  0.1093,  0.1120,  0.0734,  0.0669,\n",
       "          0.0729,  0.0834,  0.0886,  0.0746,  0.0751,  0.0945,  0.0975,  0.0929,\n",
       "          0.0949,  0.1020,  0.0789,  0.0926,  0.0911,  0.0748,  0.0708,  0.0985,\n",
       "          0.0918,  0.1171,  0.0721,  0.0884,  0.0847,  0.0671,  0.1161,  0.0765,\n",
       "          0.0973,  0.0805,  0.1116,  0.0875,  0.0983,  0.0335,  0.0911,  0.0968,\n",
       "          0.0707,  0.0777,  0.0688,  0.0844,  0.0933,  0.0808,  0.1093,  0.1337,\n",
       "          0.0951,  0.0944,  0.0678,  0.0749,  0.0911,  0.1071,  0.0846,  0.0758,\n",
       "          0.0875,  0.0817,  0.0702,  0.1224,  0.0802,  0.1078,  0.1019,  0.0747,\n",
       "          0.0931,  0.0628,  0.0849,  0.0940,  0.0837,  0.1107,  0.1637,  0.1363,\n",
       "          0.0806,  0.0698,  0.0819,  0.0989,  0.0921,  0.0961,  0.0910,  0.0764,\n",
       "          0.0841,  0.0839,  0.0867,  0.1091,  0.0758,  0.0881,  0.0704,  0.0964,\n",
       "          0.0870,  0.0841,  0.0942,  0.0806,  0.0886,  0.0799,  0.2671,  0.0962,\n",
       "          0.1077,  0.0812,  0.0998,  0.1116,  0.0840,  0.0676,  0.0953,  0.0979,\n",
       "          0.1007,  0.0735,  0.0910,  0.0734,  0.0926,  0.0826,  0.0794,  0.0998,\n",
       "          0.0943,  0.0876,  0.0777,  0.0903,  0.1130,  0.0867,  0.0837,  0.0817,\n",
       "          0.0438,  0.0710,  0.1134,  0.1219,  0.1138,  0.0432,  0.0827,  0.0706,\n",
       "          0.0666,  0.0799,  0.0834,  0.0730,  0.0778, -0.0575,  0.0919,  0.1112,\n",
       "          0.0729,  0.0819,  0.0761,  0.1014,  0.0990,  0.0772,  0.0849,  0.0860,\n",
       "          0.1274,  0.0718,  0.0483,  0.0883,  0.0945,  0.0917,  0.0966,  0.0571,\n",
       "          0.1057,  0.0991,  0.0867,  0.0897,  0.0715,  0.0715,  0.0936,  0.0968,\n",
       "          0.0765,  0.0351,  0.0843,  0.0684,  0.0906,  0.0864,  0.0974,  0.0770,\n",
       "          0.0587,  0.0655,  0.1029,  0.0869,  0.0944,  0.0774,  0.0981,  0.1039,\n",
       "          0.0877,  0.0911,  0.0636,  0.0919,  0.0913,  0.0900,  0.1170,  0.0899,\n",
       "          0.1254,  0.0966,  0.1075,  0.0614,  0.0997,  0.0783,  0.0811,  0.0939,\n",
       "          0.0475,  0.0956,  0.0766,  0.0427,  0.0927,  0.1028,  0.1162,  0.0797,\n",
       "          0.1276,  0.0962,  0.0935,  0.0686,  0.0784,  0.0944,  0.0688,  0.0835,\n",
       "          0.0856,  0.1046,  0.0930,  0.0725,  0.1364,  0.0976,  0.0771,  0.1076,\n",
       "          0.0944,  0.0758,  0.0818,  0.1000,  0.0817,  0.0979,  0.0717,  0.0909,\n",
       "          0.0807,  0.1036,  0.0653,  0.1130,  0.0553,  0.0775,  0.1059,  0.0493,\n",
       "          0.0744,  0.0567,  0.0901,  0.0760,  0.0650,  0.1129,  0.1115,  0.0741,\n",
       "          0.0855,  0.0901,  0.0881,  0.0799,  0.0655,  0.1021,  0.0810,  0.0983,\n",
       "          0.1175,  0.0740,  0.0863,  0.0694,  0.0961,  0.0731,  0.0889,  0.0357,\n",
       "          0.0868,  0.0885,  0.0938,  0.1087,  0.1058,  0.0895,  0.0980,  0.0950,\n",
       "          0.1054,  0.0751,  0.1085,  0.0561,  0.1025,  0.1132,  0.0914,  0.0855,\n",
       "          0.0927,  0.0837,  0.0772,  0.0907,  0.0715,  0.0935,  0.0899,  0.0911,\n",
       "          0.0800,  0.0975,  0.0747,  0.0950,  0.0742,  0.0788,  0.0806,  0.1154,\n",
       "          0.0738,  0.0518,  0.0703,  0.0256,  0.0836,  0.0851,  0.1045,  0.0845,\n",
       "          0.0840,  0.0822,  0.0748,  0.0500,  0.0752,  0.0551,  0.0827,  0.1039,\n",
       "          0.0787,  0.0522,  0.0979,  0.1018,  0.0976,  0.0934,  0.0818,  0.1010,\n",
       "          0.0972,  0.1073,  0.0856,  0.1166,  0.0898,  0.1230,  0.0852,  0.0724,\n",
       "          0.0822,  0.0931,  0.0929,  0.0780,  0.0727,  0.0919,  0.0687,  0.0804,\n",
       "          0.0999,  0.1096,  0.0965,  0.0917,  0.1064,  0.0897,  0.0889,  0.0922,\n",
       "          0.0970,  0.1046,  0.0729,  0.0927,  0.0369,  0.0549,  0.1054,  0.1007,\n",
       "          0.0990,  0.1020,  0.0690,  0.0862,  0.0830,  0.0591,  0.0870,  0.0886,\n",
       "          0.0921,  0.0930,  0.0782,  0.0908,  0.0761,  0.0744,  0.0929,  0.0913,\n",
       "          0.0870,  0.0986,  0.0838,  0.1068,  0.0866,  0.0825,  0.0957,  0.0931,\n",
       "          0.0793,  0.1113,  0.0811,  0.0880,  0.0941,  0.0963,  0.0885,  0.0949,\n",
       "          0.0851,  0.0801,  0.0384,  0.0951,  0.0856,  0.0887,  0.1022,  0.1023,\n",
       "          0.0870,  0.0794,  0.0830,  0.0877,  0.1245,  0.0561,  0.0896,  0.0913,\n",
       "          0.0908,  0.1026,  0.0757,  0.0742,  0.1204,  0.0833,  0.1059,  0.0428],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.4880, -0.1870, -0.3688,  ...,  0.3408,  0.1496, -0.0357],\n",
       "         [ 0.6440,  0.1476,  0.1169,  ...,  1.2714, -0.6020,  0.8481],\n",
       "         [-0.3987, -0.1629, -1.8859,  ..., -1.0472, -1.2513,  0.7699],\n",
       "         ...,\n",
       "         [-0.9934, -0.5349, -0.3242,  ..., -0.4162, -0.0703, -0.2288],\n",
       "         [-0.4434, -1.1355, -0.8343,  ..., -0.2698, -0.1928,  0.2601],\n",
       "         [ 0.3505, -0.3060,  0.6841,  ..., -0.1693, -1.9822, -0.2140]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0746, -0.3198, -0.0250,  ...,  0.7018, -0.4546, -0.3742],\n",
       "         [-0.3289, -0.9275, -0.1039,  ..., -0.2260,  0.0233, -0.1321],\n",
       "         [-0.1417, -0.7856, -0.2621,  ...,  0.2562,  0.0905, -0.8666],\n",
       "         ...,\n",
       "         [ 0.0197, -0.2023, -0.6507,  ...,  0.7308,  0.1770, -0.1513],\n",
       "         [ 0.0562,  0.1147,  0.2182,  ...,  0.1019,  0.0863, -0.4947],\n",
       "         [ 0.0374,  0.5955, -0.1906,  ...,  0.2102,  0.3121, -0.1674]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.2831, 1.4016, 1.4795, 1.7703, 0.4585, 1.3836, 1.5261, 1.5867, 1.7140,\n",
       "         1.6249, 1.4908, 1.2166, 1.4993, 1.6007, 1.7757, 1.4611, 1.3741, 1.3885,\n",
       "         1.4493, 1.4485, 1.2627, 1.2634, 1.5410, 0.8512, 1.6168, 1.1598, 1.4480,\n",
       "         1.2557, 1.4112, 1.6204, 1.4839, 1.1271, 1.8717, 1.5371, 1.5271, 1.4058,\n",
       "         1.4496, 1.5048, 1.5527, 1.1775, 1.4143, 1.5042, 0.8178, 1.7286, 1.5963,\n",
       "         1.1569, 0.5857, 1.2939, 1.4163, 1.4406, 1.3588, 1.5634, 1.3033, 1.6463,\n",
       "         1.2144, 1.4174, 1.3615, 1.4649, 1.3834, 1.3630, 1.8461, 1.2868, 1.5052,\n",
       "         1.4742, 1.7937, 1.4392, 1.4067, 1.7424, 1.5174, 1.4925, 1.6233, 1.2968,\n",
       "         1.4201, 1.4357, 1.4991, 1.4491, 1.6830, 1.1567, 1.3592, 1.1626, 1.2996,\n",
       "         1.6952, 1.3865, 1.3737, 1.4306, 1.3475, 1.4159, 1.1818, 1.4335, 0.9634,\n",
       "         1.2774, 1.2951, 1.4571, 1.4549, 1.4552, 1.6006, 2.1726, 1.3457, 1.9214,\n",
       "         1.2934, 1.7120, 1.5777, 1.1993, 1.2100, 1.4230, 1.4671, 1.2237, 1.6331,\n",
       "         1.5490, 1.2527, 1.5205, 1.4488, 1.4265, 1.6467, 1.5909, 1.5418, 1.9973,\n",
       "         1.4719, 1.3040, 1.3909, 1.3371, 1.4860, 1.6789, 1.2772, 1.3967, 2.0991,\n",
       "         1.4948, 1.4995, 1.4347, 1.5682, 1.4230, 1.4627, 1.6459, 1.2575, 1.3648,\n",
       "         1.5807, 1.4441, 1.0362, 1.1807, 1.3965, 1.3092, 1.5525, 1.6717, 1.3281,\n",
       "         1.6239, 1.4237, 1.4010, 1.8179, 1.4776, 0.3772, 1.4003, 1.4875, 1.3446,\n",
       "         1.5652, 1.2630, 1.4998, 1.4542, 1.1160, 1.3136, 2.7256, 1.4306, 1.4054,\n",
       "         1.2888, 1.1097, 1.5386, 1.6032, 1.3277, 1.4332, 1.4789, 1.4545, 1.2080,\n",
       "         1.6154, 1.3190, 1.6140, 1.7543, 1.2893, 1.5921, 1.1105, 1.4599, 1.5774,\n",
       "         1.5401, 1.5484, 1.1270, 1.4210, 1.7249, 1.3836, 1.4489, 1.5398, 1.4495,\n",
       "         1.4557, 1.6666, 1.1038, 1.4446, 1.3989, 1.3488, 1.5869, 1.3103, 1.5590,\n",
       "         1.3125, 1.4915, 1.4561, 1.5345, 1.6916, 1.2816, 1.5525, 1.2990, 1.4208,\n",
       "         1.4288, 1.8337, 1.5726, 1.6329, 1.6328, 1.4508, 0.7409, 1.5477, 1.6141,\n",
       "         1.5731, 1.2793, 1.3080, 1.3239, 1.4656, 1.3538, 1.4003, 1.4044, 1.2297,\n",
       "         1.4641, 1.5656, 1.4973, 1.5415, 1.4107, 1.4015, 1.4310, 1.3556, 1.3243,\n",
       "         1.3976, 1.9638, 1.6421, 1.3752, 1.2709, 1.7236, 1.1189, 1.4938, 1.2351,\n",
       "         1.4442, 1.9514, 1.5700, 1.5308, 1.9398, 0.9243, 1.3942, 1.3816, 1.5606,\n",
       "         1.5070, 1.2265, 1.3804, 1.3376, 1.5580, 1.5347, 1.2483, 1.4125, 1.3408,\n",
       "         1.4932, 1.5689, 1.2286, 1.7254, 1.6218, 1.4166, 1.3902, 1.6487, 1.1160,\n",
       "         1.1952, 1.4432, 1.4294, 1.3886, 1.4194, 2.0039, 1.6014, 1.7398, 1.4309,\n",
       "         1.2444, 1.4045, 1.1973, 1.8724, 1.4081, 1.5039, 1.4000, 1.3592, 1.4287,\n",
       "         1.5083, 1.3555, 1.2984, 1.6309, 1.3622, 1.4329, 1.4115, 1.1650, 1.7921,\n",
       "         1.5329, 1.7134, 1.5194, 1.5716, 1.4482, 0.8604, 1.6858, 1.1352, 1.4304,\n",
       "         1.3008, 1.3164, 1.4323, 1.9879, 1.8223, 1.3514, 1.2596, 1.3282, 1.2740,\n",
       "         1.3532, 1.4456, 1.4587, 1.3169, 1.4421, 1.3878, 1.6544, 1.9084, 2.3023,\n",
       "         3.1054, 1.6221, 1.3390, 2.0786, 1.7425, 1.6186, 1.3952, 1.6048, 1.1757,\n",
       "         1.7240, 1.3599, 1.4132, 1.4643, 1.4510, 1.1988, 1.7287, 1.4276, 1.3348,\n",
       "         1.5927, 1.2109, 1.3738, 1.1259, 1.4129, 1.1824, 1.0364, 1.9323, 1.5785,\n",
       "         1.3188, 1.3679, 1.4799, 1.3927, 1.3762, 1.1648, 1.5896, 1.2312, 1.3901,\n",
       "         1.7145, 1.3800, 1.3651, 1.5971, 1.4549, 1.3453, 1.6490, 0.6678, 1.2898,\n",
       "         1.5100, 1.5012, 1.4983, 1.3957, 1.4469, 1.4658, 1.5599, 1.4209, 1.2630,\n",
       "         1.6884, 1.5057, 1.6490, 1.3765, 1.4064, 1.4896, 1.3435, 1.4269, 1.6293,\n",
       "         1.5279, 1.1797, 1.5179, 1.3519, 1.5105, 1.1576, 1.4260, 1.3223, 1.2908,\n",
       "         1.2466, 1.5911, 1.4724, 1.8180, 1.1582, 1.0118, 1.6087, 0.3077, 1.4626,\n",
       "         1.3432, 1.7818, 1.4779, 1.4247, 1.3743, 1.6346, 1.2549, 1.2538, 1.0785,\n",
       "         1.4235, 1.7508, 1.4140, 1.8287, 1.6183, 1.6935, 1.3922, 1.5743, 1.2970,\n",
       "         1.9381, 1.4643, 1.5525, 1.3838, 1.4889, 1.5598, 1.5690, 1.5120, 1.2417,\n",
       "         1.4685, 1.4908, 1.5440, 1.2861, 1.3162, 1.4194, 1.2382, 1.3181, 1.3206,\n",
       "         1.6773, 1.5938, 1.5420, 1.6830, 1.4756, 1.5102, 1.5489, 1.7196, 1.6888,\n",
       "         1.2546, 1.4311, 1.2928, 1.1805, 1.5638, 1.7542, 1.6286, 1.5297, 1.4351,\n",
       "         1.7558, 1.4251, 1.7970, 1.4511, 1.4729, 1.5119, 1.5337, 1.3137, 1.6668,\n",
       "         1.2329, 1.4726, 1.6152, 1.4904, 1.3100, 1.2710, 1.4303, 0.9927, 1.3378,\n",
       "         1.9141, 1.5580, 1.5239, 1.2168, 1.7156, 1.2784, 1.6041, 1.7135, 1.4636,\n",
       "         1.4470, 1.3324, 1.3111, 1.3263, 0.4668, 1.6258, 1.2660, 1.4841, 1.5840,\n",
       "         1.5672, 1.5596, 1.5753, 1.2777, 1.5614, 1.9572, 1.5341, 1.3977, 1.4959,\n",
       "         1.5439, 1.5146, 1.4650, 1.2497, 1.5857, 1.2399, 1.4922, 1.2953],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0210,  0.0569, -0.0014,  ...,  0.0395,  0.0272,  0.0008],\n",
       "         [ 0.0674,  0.0998, -0.0141,  ..., -0.0068,  0.0289,  0.0542],\n",
       "         [ 0.0276,  0.0453,  0.0394,  ...,  0.0920,  0.1255, -0.0167],\n",
       "         ...,\n",
       "         [ 0.0370, -0.0561,  0.0071,  ..., -0.0225,  0.0017, -0.0918],\n",
       "         [ 0.0452,  0.0448, -0.0036,  ..., -0.0239,  0.0601,  0.0648],\n",
       "         [ 0.0180, -0.0275, -0.0592,  ..., -0.0401,  0.0387,  0.0896]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.4593,  0.8859,  0.2047,  ...,  0.0243,  0.9244,  0.7828],\n",
       "         [ 0.6188, -0.1097,  0.1824,  ...,  0.2778,  0.3821,  0.2505],\n",
       "         [-0.3677,  0.4838,  0.6396,  ..., -0.9671,  0.8468, -0.3771],\n",
       "         ...,\n",
       "         [-0.4822, -0.1203,  0.5549,  ..., -0.3513,  0.7232,  0.0869],\n",
       "         [-0.6773, -0.2780, -0.3209,  ...,  0.0792, -0.4990,  0.1133],\n",
       "         [ 0.1658,  0.3074,  0.4446,  ..., -1.1762, -0.0540,  0.0347]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.4518, -0.0520, -0.5761,  ..., -0.1017, -1.2997,  0.0676],\n",
       "         [ 0.2270,  0.2033, -1.1330,  ..., -0.9144, -0.7506, -0.9492],\n",
       "         [-0.1278,  0.4444, -0.0553,  ..., -1.0965,  0.4122, -0.4350],\n",
       "         ...,\n",
       "         [ 0.1665, -0.8903, -0.0921,  ...,  2.7645,  0.0988,  0.6108],\n",
       "         [-0.5088,  0.0493, -0.2256,  ..., -1.8439,  0.2862,  0.3571],\n",
       "         [-0.7943, -0.3654, -1.2516,  ...,  0.4039, -0.5345,  1.1698]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2403,  1.3456,  0.3776,  ...,  0.9039, -0.5450,  0.7503],\n",
       "         [ 0.7910,  0.3756,  0.4096,  ..., -1.1107,  0.8939, -1.4526],\n",
       "         [-0.3292,  0.0665, -0.4719,  ..., -1.1953, -0.8362, -1.1273],\n",
       "         ...,\n",
       "         [-0.5524, -0.1696,  0.1530,  ...,  1.4998, -1.9309,  0.5257],\n",
       "         [ 0.8060,  1.4516, -1.0054,  ..., -0.1477,  0.6144, -0.7033],\n",
       "         [ 0.4353, -0.4492,  0.8304,  ...,  0.4620,  0.5377,  0.0122]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1312,  0.1629,  0.1370,  0.2045,  0.0963,  0.1617,  0.1800,  0.1096,\n",
       "          0.0842,  0.1678,  0.1665,  0.0810,  0.1635,  0.1795,  0.1144,  0.1333,\n",
       "          0.1507,  0.1610,  0.1654,  0.1530,  0.1441,  0.1498,  0.1692,  0.0768,\n",
       "          0.1746,  0.1345,  0.1622,  0.1143,  0.1480,  0.1939,  0.1602,  0.1108,\n",
       "          0.1958,  0.1718,  0.1593,  0.1165,  0.1811,  0.1894,  0.1727,  0.1291,\n",
       "          0.1635,  0.1817,  0.1117,  0.2046,  0.1690,  0.1414,  0.0555,  0.1411,\n",
       "          0.1802,  0.1609,  0.1779,  0.1713,  0.1405,  0.0942,  0.1112,  0.1756,\n",
       "          0.1549,  0.1616,  0.1477,  0.1264,  0.1962,  0.1365,  0.2179,  0.1552,\n",
       "          0.1652,  0.1632,  0.1703,  0.1793,  0.0990,  0.1811,  0.1919,  0.1412,\n",
       "          0.1555,  0.1506,  0.1584,  0.1448,  0.1862,  0.1250,  0.1450,  0.1301,\n",
       "          0.1384,  0.1631,  0.1553,  0.1718,  0.1487,  0.1192,  0.1487,  0.1285,\n",
       "          0.1726,  0.1430,  0.1444,  0.1488,  0.1225,  0.1642,  0.1395,  0.1763,\n",
       "          0.0470,  0.1495,  0.2433,  0.1360,  0.1846,  0.1749,  0.1293,  0.1167,\n",
       "          0.1769,  0.1548,  0.1493,  0.1962,  0.1734,  0.1276,  0.1886,  0.1744,\n",
       "          0.1646,  0.1865,  0.1735,  0.1678,  0.2169,  0.1862,  0.1341,  0.1604,\n",
       "          0.1512,  0.1726,  0.1777,  0.1055,  0.1529,  0.2217,  0.1832,  0.1560,\n",
       "          0.1525,  0.1709,  0.1351,  0.1709,  0.1563,  0.1370,  0.1136,  0.1610,\n",
       "          0.1628,  0.1337,  0.1317,  0.1709,  0.1435,  0.0920,  0.1994,  0.1570,\n",
       "          0.1813,  0.1432,  0.1744,  0.1458,  0.1664,  0.0333,  0.1574,  0.1707,\n",
       "          0.1109,  0.1776,  0.1467,  0.1736,  0.1714,  0.1465,  0.1542,  0.1993,\n",
       "          0.1508,  0.1743,  0.1460,  0.1308,  0.1465,  0.1911,  0.1317,  0.1466,\n",
       "          0.1623,  0.1479,  0.1311,  0.1781,  0.1234,  0.1644,  0.1895,  0.1445,\n",
       "          0.1290,  0.1048,  0.1716,  0.1803,  0.1581,  0.2019,  0.3653,  0.1982,\n",
       "          0.1677,  0.1472,  0.1727,  0.1680,  0.1571,  0.1768,  0.1527,  0.1133,\n",
       "          0.1818,  0.1499,  0.1527,  0.1726,  0.1585,  0.1728,  0.1471,  0.1804,\n",
       "          0.1623,  0.1670,  0.1690,  0.1431,  0.1947,  0.1405,  0.2750,  0.1771,\n",
       "          0.2102,  0.1718,  0.1799,  0.2001,  0.1444,  0.0872,  0.1717,  0.1769,\n",
       "          0.1828,  0.1314,  0.1580,  0.1437,  0.1808,  0.1360,  0.1452,  0.1995,\n",
       "          0.1289,  0.1584,  0.1499,  0.1622,  0.1814,  0.1553,  0.1645,  0.1529,\n",
       "          0.0696,  0.1249,  0.1385,  0.2146,  0.2030,  0.0897,  0.1615,  0.0605,\n",
       "          0.1332,  0.1408,  0.1367,  0.1698,  0.1134,  0.0389,  0.1669,  0.2202,\n",
       "          0.1081,  0.1638,  0.1495,  0.1594,  0.1762,  0.1481,  0.1485,  0.1547,\n",
       "          0.1948,  0.0968,  0.1127,  0.1565,  0.1662,  0.1591,  0.1999,  0.1196,\n",
       "          0.2000,  0.1808,  0.1546,  0.1743,  0.1125,  0.1277,  0.1567,  0.1664,\n",
       "          0.1372,  0.0700,  0.1796,  0.1211,  0.1801,  0.1832,  0.1536,  0.1405,\n",
       "          0.1008,  0.1224,  0.1831,  0.1661,  0.1835,  0.1691,  0.1609,  0.1755,\n",
       "          0.1713,  0.1608,  0.1281,  0.1915,  0.1555,  0.1432,  0.1697,  0.1306,\n",
       "          0.2031,  0.1746,  0.2083,  0.1100,  0.1801,  0.1535,  0.1129,  0.1846,\n",
       "          0.0914,  0.1717,  0.1472,  0.0968,  0.1156,  0.1316,  0.2050,  0.1558,\n",
       "          0.1455,  0.1514,  0.1611,  0.1416,  0.1711,  0.1588,  0.1135,  0.1431,\n",
       "          0.1691,  0.1757,  0.2018,  0.1126,  0.1713,  0.1835,  0.1471,  0.2108,\n",
       "          0.2053, -0.1092,  0.1536,  0.1790,  0.1335,  0.1767,  0.1592,  0.1619,\n",
       "          0.1542,  0.1688,  0.1398,  0.2049,  0.1012,  0.1205,  0.1926,  0.1104,\n",
       "          0.1566,  0.0987,  0.1614,  0.1308,  0.1106,  0.2298,  0.2052,  0.1389,\n",
       "          0.1562,  0.1634,  0.1318,  0.1482,  0.1283,  0.1831,  0.1436,  0.1655,\n",
       "          0.2008,  0.1645,  0.1617,  0.1061,  0.1723,  0.1311,  0.1808,  0.0528,\n",
       "          0.1318,  0.1802,  0.1598,  0.1791,  0.1706,  0.1494,  0.1443,  0.1854,\n",
       "          0.1544,  0.1338,  0.1709,  0.1039,  0.2080,  0.0998,  0.1677,  0.1713,\n",
       "          0.1582,  0.1685,  0.0940,  0.1646,  0.1325,  0.1701,  0.1637,  0.1707,\n",
       "          0.0787,  0.1737,  0.1271,  0.1538,  0.1331,  0.1423,  0.1569,  0.1988,\n",
       "          0.1184,  0.0945,  0.1071,  0.0332,  0.1440,  0.1350,  0.1870,  0.1734,\n",
       "          0.1535,  0.1523,  0.1167,  0.1214,  0.1414,  0.1129,  0.1601,  0.1903,\n",
       "          0.1426,  0.1022,  0.1800,  0.1916,  0.1789,  0.1755,  0.1527,  0.1261,\n",
       "          0.1823,  0.1794,  0.1616,  0.1859,  0.1678,  0.2017,  0.1692,  0.1197,\n",
       "          0.1574,  0.1708,  0.1591,  0.1494,  0.1421,  0.1585,  0.1308,  0.1405,\n",
       "          0.1747,  0.1912,  0.1709,  0.1728,  0.1978,  0.1614,  0.1574,  0.1658,\n",
       "          0.1958,  0.2114,  0.1507,  0.1690,  0.1084,  0.1035,  0.1867,  0.1978,\n",
       "          0.1628,  0.1726,  0.1577,  0.1832,  0.1423,  0.1317,  0.1628,  0.1582,\n",
       "          0.1754,  0.1613,  0.1459,  0.1016,  0.1316,  0.1625,  0.1820,  0.1572,\n",
       "          0.1617,  0.1749,  0.1541,  0.1473,  0.1756,  0.0605,  0.1952,  0.1141,\n",
       "          0.1287,  0.1874,  0.1509,  0.1726,  0.1995,  0.1587,  0.1787,  0.1479,\n",
       "          0.1517,  0.1447,  0.0601,  0.1823,  0.1772,  0.1575,  0.1767,  0.1673,\n",
       "          0.1631,  0.1281,  0.1390,  0.1690,  0.2078,  0.0919,  0.1491,  0.1693,\n",
       "          0.1625,  0.1865,  0.1612,  0.1135,  0.1765,  0.1526,  0.2750,  0.1059],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0515,  0.0687,  0.0005,  ..., -0.0137,  0.0742, -0.0003],\n",
       "         [-0.0497, -0.0807,  0.0027,  ...,  0.0440, -0.0291, -0.0722],\n",
       "         [ 0.0211, -0.0672,  0.0349,  ..., -0.0199,  0.0136,  0.0229],\n",
       "         ...,\n",
       "         [ 0.0377,  0.0083, -0.0626,  ...,  0.0346, -0.0182, -0.0295],\n",
       "         [-0.0986,  0.0766,  0.0092,  ...,  0.0267, -0.0379, -0.0504],\n",
       "         [-0.0151, -0.0211, -0.0443,  ...,  0.0034, -0.0560, -0.0251]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.5160,  0.4044, -0.3720,  ..., -0.3654, -0.9703, -1.3531],\n",
       "         [-0.2847,  0.3214,  0.3276,  ...,  0.2155, -0.3423, -1.0012],\n",
       "         [-0.0333,  0.4527,  0.6339,  ..., -0.4386,  0.0865, -0.3754],\n",
       "         ...,\n",
       "         [-0.4058,  0.1027, -0.5986,  ...,  0.0590, -0.1237,  0.1371],\n",
       "         [ 0.0195, -0.0311,  0.2284,  ...,  0.4343,  0.4991, -1.0926],\n",
       "         [ 0.2396,  1.2757, -0.1752,  ...,  0.2940,  0.2301,  0.1030]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.6310,  0.5664,  0.0411,  ...,  0.6902, -0.7437,  0.2819],\n",
       "         [ 0.7506,  1.0553, -0.3418,  ..., -1.1543,  1.5588,  1.3129],\n",
       "         [-0.2978, -0.9944,  0.0031,  ...,  0.6016, -1.5312,  0.1430],\n",
       "         ...,\n",
       "         [-1.0794,  0.0701,  2.5746,  ..., -0.0945,  0.3166, -0.0839],\n",
       "         [ 2.7248,  0.8389, -0.7439,  ..., -0.4688, -1.5897, -0.8242],\n",
       "         [-0.1790, -0.0872,  2.1707,  ..., -1.1378,  0.2670, -0.1691]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.4206,  0.3040,  0.1042,  ..., -0.8783,  2.1973, -0.0149],\n",
       "         [ 0.3762,  1.1150,  0.8232,  ...,  2.1169,  0.8326,  0.3368],\n",
       "         [ 0.2213, -0.6598,  0.9906,  ...,  0.6655, -0.6340,  1.8840],\n",
       "         ...,\n",
       "         [-0.3691, -0.9777,  0.3052,  ...,  0.4614,  1.5346, -0.1433],\n",
       "         [ 0.6552,  0.0022, -0.7226,  ..., -0.4146, -0.9404,  0.2548],\n",
       "         [ 0.1845,  1.1051,  0.1907,  ...,  1.1257, -0.4489,  0.8518]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1064,  0.1382,  0.0860,  0.1473,  0.0634,  0.1326,  0.1422,  0.0780,\n",
       "          0.0853,  0.1260,  0.1253,  0.0735,  0.1191,  0.1447,  0.0733,  0.0999,\n",
       "          0.1488,  0.1294,  0.1368,  0.1135,  0.1048,  0.0965,  0.1284, -0.0335,\n",
       "          0.1401,  0.1097,  0.1280,  0.0796,  0.1276,  0.1411,  0.1149,  0.0826,\n",
       "          0.1569,  0.1236,  0.0930,  0.0788,  0.1287,  0.1265,  0.1596,  0.0982,\n",
       "          0.1200,  0.1242,  0.1021,  0.1456,  0.1185,  0.1019, -0.0278,  0.1062,\n",
       "          0.1334,  0.1125,  0.1566,  0.1263,  0.1093,  0.0577,  0.0805,  0.1470,\n",
       "          0.1110,  0.1297,  0.1170,  0.0827,  0.1460,  0.1014,  0.1602,  0.1179,\n",
       "          0.1288,  0.1240,  0.1164,  0.1465,  0.0628,  0.1556,  0.1533,  0.1189,\n",
       "          0.1229,  0.1271,  0.1192,  0.1079,  0.1641,  0.0974,  0.1233,  0.0884,\n",
       "          0.0961,  0.1307,  0.1056,  0.1794,  0.1158,  0.0994,  0.1083,  0.1028,\n",
       "          0.1370,  0.0853,  0.1102,  0.1088,  0.0923,  0.1373,  0.0903,  0.1403,\n",
       "          0.0838,  0.1230,  0.1626,  0.0936,  0.1222,  0.1235,  0.0937,  0.0949,\n",
       "          0.1338,  0.1325,  0.1117,  0.1417,  0.1516,  0.1009,  0.1592,  0.1458,\n",
       "          0.1268,  0.1362,  0.1373,  0.1402,  0.1579,  0.1379,  0.0968,  0.1223,\n",
       "          0.1163,  0.1269,  0.1344,  0.0795,  0.1176,  0.1165,  0.1362,  0.1125,\n",
       "          0.1274,  0.1298,  0.1092,  0.1243,  0.1061,  0.1083,  0.0765,  0.1361,\n",
       "          0.1334,  0.1663,  0.1068,  0.1383,  0.1156,  0.0858,  0.1583,  0.1208,\n",
       "          0.1384,  0.1010,  0.1396,  0.0828,  0.1341, -0.0232,  0.1264,  0.1320,\n",
       "          0.0861,  0.1139,  0.1023,  0.1264,  0.1200,  0.1399,  0.1028,  0.1221,\n",
       "          0.1277,  0.1385,  0.1158,  0.0901,  0.1186,  0.1504,  0.1141,  0.1098,\n",
       "          0.1333,  0.1089,  0.0973,  0.1411,  0.0756,  0.1212,  0.1481,  0.1262,\n",
       "          0.0770,  0.0955,  0.1167,  0.1327,  0.1175,  0.1626,  0.4338,  0.1417,\n",
       "          0.1099,  0.1254,  0.1377,  0.1389,  0.1269,  0.1398,  0.1122,  0.1089,\n",
       "          0.1253,  0.1232,  0.1025,  0.1334,  0.1309,  0.1423,  0.1121,  0.1261,\n",
       "          0.1266,  0.1120,  0.1353,  0.1039,  0.1481,  0.1086,  0.4340,  0.1485,\n",
       "          0.1413,  0.1223,  0.1480,  0.1769,  0.1043,  0.0614,  0.1280,  0.1309,\n",
       "          0.1401,  0.0981,  0.1043,  0.1187,  0.1376,  0.1069,  0.1110,  0.1461,\n",
       "          0.1052,  0.1315,  0.0933,  0.1254,  0.1522,  0.1139,  0.1226,  0.1272,\n",
       "          0.0647,  0.0915,  0.1175,  0.1719,  0.1702,  0.0653,  0.1290, -0.0658,\n",
       "          0.0953,  0.0977,  0.0949,  0.1101,  0.1285, -0.0431,  0.1334,  0.1810,\n",
       "          0.0967,  0.1242,  0.1329,  0.1146,  0.1302,  0.1375,  0.1152,  0.1488,\n",
       "          0.1661,  0.0867,  0.0957,  0.1137,  0.1546,  0.1428,  0.1364,  0.0825,\n",
       "          0.1621,  0.1409,  0.1064,  0.1402, -0.0716,  0.1023,  0.1110,  0.1296,\n",
       "          0.1085,  0.0586,  0.1140,  0.1132,  0.1365,  0.1505,  0.1205,  0.1008,\n",
       "          0.0838,  0.0912,  0.1400,  0.1218,  0.1435,  0.1314,  0.1246,  0.1381,\n",
       "          0.1248,  0.1247,  0.1018,  0.1367,  0.1265,  0.0971,  0.1714,  0.1254,\n",
       "          0.1720,  0.1296,  0.1570,  0.1051,  0.1299,  0.0970,  0.0505,  0.1364,\n",
       "          0.0606,  0.1357,  0.1222,  0.0648,  0.0884,  0.0924,  0.1706,  0.1245,\n",
       "          0.1477,  0.1295,  0.1271,  0.1067,  0.1131,  0.1334,  0.0816,  0.1033,\n",
       "          0.1270,  0.1358,  0.1376,  0.0721,  0.1038,  0.1496,  0.1047,  0.1663,\n",
       "          0.1500,  0.0727,  0.1190,  0.1266,  0.1120,  0.1477,  0.1175,  0.1122,\n",
       "          0.1186,  0.1139,  0.1085,  0.1719,  0.0657,  0.0884,  0.1460,  0.0764,\n",
       "          0.1254,  0.0724,  0.1204,  0.1049,  0.0968,  0.1756,  0.1704,  0.1133,\n",
       "          0.1209,  0.1211,  0.1056,  0.1032,  0.1080,  0.1353,  0.1160,  0.1286,\n",
       "          0.1728,  0.1181,  0.1361,  0.0877,  0.1369,  0.1032,  0.1170,  0.0348,\n",
       "          0.0999,  0.1333,  0.1195,  0.1375,  0.1478,  0.1138,  0.1321,  0.1545,\n",
       "          0.1281,  0.0972,  0.1441,  0.0920,  0.1652,  0.1059,  0.1305,  0.1244,\n",
       "          0.1314,  0.1190,  0.0798,  0.1164,  0.0978,  0.1258,  0.1216,  0.1165,\n",
       "          0.0768,  0.1336,  0.1032,  0.1310,  0.0995,  0.1000,  0.1074,  0.1637,\n",
       "          0.1036,  0.0767,  0.0842,  0.0217,  0.1165,  0.0885,  0.1382,  0.1238,\n",
       "          0.1189,  0.1133, -0.0703,  0.0767,  0.1012,  0.0896,  0.1275,  0.1476,\n",
       "          0.1366,  0.0811,  0.1286,  0.1441,  0.1485,  0.1324,  0.0990,  0.0763,\n",
       "          0.1257,  0.1477,  0.1254,  0.1349,  0.1214,  0.1503,  0.1127,  0.0939,\n",
       "          0.1116,  0.1516,  0.1070,  0.1104,  0.1009,  0.1255,  0.0968,  0.1164,\n",
       "          0.1541,  0.1386,  0.1400,  0.1159,  0.1622,  0.1237,  0.1220,  0.1077,\n",
       "          0.1359,  0.1737,  0.1181,  0.1397,  0.0730,  0.0781,  0.1382,  0.1413,\n",
       "          0.1377,  0.1373,  0.1200,  0.1372,  0.1152,  0.1156,  0.1130,  0.1388,\n",
       "          0.1281,  0.1179,  0.1049,  0.1234,  0.0973,  0.1187,  0.1249,  0.1182,\n",
       "          0.1066,  0.1419,  0.1132,  0.1306,  0.1364,  0.0722,  0.1610,  0.0894,\n",
       "          0.1011,  0.1434,  0.0996,  0.1263,  0.1464,  0.1339,  0.1219,  0.1402,\n",
       "          0.1195,  0.1175,  0.0388,  0.1393,  0.1514,  0.1241,  0.1501,  0.1302,\n",
       "          0.1202,  0.0726,  0.1081,  0.1120,  0.1842,  0.0578,  0.1129,  0.1328,\n",
       "          0.1264,  0.1468,  0.1180,  0.0849,  0.1492,  0.1138,  0.1863,  0.0783],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1497, -0.2574, -0.5078,  ...,  0.2358,  0.3450, -0.6021],\n",
       "         [-0.1154, -0.1603,  0.5909,  ...,  0.2260, -0.0161,  0.3863],\n",
       "         [-0.7621, -0.9499,  0.6589,  ...,  0.2835, -0.7579, -0.3285],\n",
       "         ...,\n",
       "         [ 0.0363, -1.0105,  1.0293,  ..., -1.5152, -1.2679, -0.2389],\n",
       "         [-1.1179, -0.2655, -1.3469,  ...,  0.8865, -0.1496,  0.2984],\n",
       "         [ 0.7656,  0.3874, -0.4698,  ...,  0.0166,  0.0504,  0.0342]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1011, -0.1837, -0.0411,  ..., -0.4036, -0.1707, -0.3674],\n",
       "         [ 0.0808,  0.0924,  0.2740,  ..., -0.0657,  0.2187, -0.2546],\n",
       "         [-0.1357,  0.1164, -0.0534,  ...,  0.4991,  0.3647,  0.2639],\n",
       "         ...,\n",
       "         [ 0.0643,  0.2265,  0.4011,  ...,  0.6426,  0.2887, -0.0889],\n",
       "         [-0.1111, -0.0251,  0.0678,  ...,  0.0640,  0.0834,  0.0680],\n",
       "         [-0.0608, -0.4983, -0.0502,  ...,  0.1540, -0.3841,  0.0995]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.7839, 1.9004, 1.9093, 2.2172, 0.4306, 1.7831, 2.0977, 1.9081, 2.4503,\n",
       "         1.9128, 1.8922, 2.0440, 1.9136, 1.9725, 2.4910, 1.9282, 1.8708, 1.9783,\n",
       "         1.8222, 1.9437, 1.6685, 1.5069, 1.8484, 1.0711, 2.1035, 1.7234, 1.7670,\n",
       "         1.9221, 1.7062, 2.2361, 1.8978, 1.5153, 2.3305, 1.9319, 2.4063, 2.0410,\n",
       "         1.8363, 1.8208, 1.9733, 1.6945, 1.8419, 1.9017, 1.4269, 2.1736, 2.0328,\n",
       "         1.5599, 0.8008, 1.7369, 1.8151, 1.8151, 1.9186, 1.9760, 1.7524, 2.8318,\n",
       "         1.6111, 1.7839, 1.8187, 1.8500, 1.7512, 1.8267, 2.1847, 1.8003, 2.0396,\n",
       "         1.8197, 2.0581, 1.8358, 1.8282, 2.1799, 2.0423, 1.9077, 2.0264, 1.8433,\n",
       "         1.9441, 1.8505, 1.8443, 1.9033, 2.1633, 1.5671, 1.8145, 1.8186, 1.8183,\n",
       "         2.2157, 1.7764, 2.0972, 1.8727, 1.8581, 1.8460, 1.7837, 1.8146, 1.3891,\n",
       "         1.7416, 1.7929, 1.8702, 1.8932, 1.8829, 1.9256, 1.9644, 1.9327, 2.6914,\n",
       "         1.6811, 2.1882, 2.0201, 1.6952, 1.7858, 2.0057, 1.8185, 1.7159, 1.9071,\n",
       "         2.1627, 1.8095, 2.1202, 1.9526, 1.8128, 1.8905, 1.8191, 2.0354, 2.4939,\n",
       "         1.9027, 1.5877, 1.7836, 1.7991, 1.9105, 2.1243, 1.7594, 1.7323, 3.6851,\n",
       "         2.0939, 1.7600, 1.8940, 1.9399, 1.7546, 1.8708, 2.1435, 1.8251, 2.0057,\n",
       "         1.9957, 2.0133, 1.8818, 1.5785, 1.7841, 1.6741, 2.1025, 2.1188, 1.7851,\n",
       "         1.9965, 1.8700, 1.8452, 1.9695, 2.0352, 0.3877, 1.6904, 1.9106, 1.9370,\n",
       "         1.8596, 1.7118, 1.9054, 1.7879, 1.6879, 1.6778, 3.9577, 1.7099, 1.8420,\n",
       "         1.8110, 1.5192, 1.9361, 2.0844, 1.8372, 2.0331, 1.8978, 1.7736, 1.6483,\n",
       "         1.8255, 1.5941, 1.9683, 2.2963, 1.8241, 1.8811, 1.8110, 1.9559, 1.9330,\n",
       "         2.0063, 1.9209, 2.1723, 1.8698, 3.0129, 1.8274, 1.8507, 1.9291, 1.8578,\n",
       "         1.9255, 2.6042, 1.8168, 2.0057, 1.9139, 1.6918, 2.1049, 1.8147, 1.9880,\n",
       "         1.6317, 1.8008, 1.9516, 2.0554, 2.2671, 1.7984, 1.9940, 1.7185, 1.9500,\n",
       "         1.9460, 2.1651, 1.9538, 2.1849, 2.1682, 1.8534, 0.8496, 1.9722, 1.8937,\n",
       "         1.9324, 1.6556, 1.8904, 1.7041, 1.9652, 1.7635, 1.8244, 1.8070, 1.7614,\n",
       "         1.9051, 2.5286, 1.9077, 1.9315, 1.8728, 1.9277, 1.8920, 2.3247, 1.8061,\n",
       "         1.9538, 2.4142, 2.2025, 2.1253, 1.7561, 1.9598, 1.6838, 2.1608, 1.6963,\n",
       "         1.8732, 3.2820, 1.4086, 2.0637, 2.4371, 1.5424, 1.8139, 1.7773, 1.9201,\n",
       "         2.0515, 1.7106, 1.7899, 1.8486, 1.9061, 2.1721, 2.0339, 1.9675, 1.8844,\n",
       "         1.7800, 1.8557, 1.9539, 2.2467, 2.0222, 1.8183, 1.7616, 2.1274, 1.6205,\n",
       "         1.6382, 1.8909, 1.8245, 2.3775, 1.6182, 3.5763, 1.8960, 2.1847, 2.0820,\n",
       "         1.7340, 2.1573, 1.6645, 2.2313, 1.7986, 1.9698, 1.8168, 1.7280, 1.8807,\n",
       "         1.9779, 1.8078, 1.8781, 2.0782, 1.9720, 1.8275, 2.6283, 1.7719, 2.1875,\n",
       "         1.8709, 2.0555, 2.5058, 2.0355, 1.8728, 0.7151, 2.1101, 1.7566, 1.7411,\n",
       "         1.8315, 1.9855, 1.8271, 2.2744, 2.4427, 1.7957, 2.1473, 1.6372, 1.8248,\n",
       "         1.7115, 1.8978, 1.8473, 1.9580, 2.0343, 1.7914, 2.0206, 2.2914, 3.0549,\n",
       "         4.7308, 2.0453, 1.7777, 2.5578, 2.2328, 2.1016, 1.8715, 2.0022, 1.6521,\n",
       "         2.1699, 1.8123, 1.8818, 1.9698, 1.8122, 1.5570, 2.2394, 2.0480, 1.8240,\n",
       "         2.0306, 1.7473, 1.8520, 1.7184, 1.8442, 1.6493, 1.4311, 2.3223, 2.0465,\n",
       "         1.6447, 1.8002, 1.9254, 1.7733, 2.2506, 1.5983, 1.9644, 1.6188, 1.8576,\n",
       "         2.2502, 1.8983, 1.8191, 2.2234, 1.8922, 1.7542, 2.0661, 0.5176, 1.7369,\n",
       "         1.8468, 1.8380, 2.0767, 1.7779, 1.8242, 2.1569, 1.9803, 1.7873, 1.6000,\n",
       "         2.0606, 2.1409, 2.0873, 2.0757, 1.9412, 1.8715, 1.8515, 1.9469, 2.4723,\n",
       "         2.0452, 1.8007, 1.9881, 1.6975, 1.8696, 1.6052, 1.8531, 1.7527, 1.7660,\n",
       "         1.8860, 2.0143, 1.8243, 2.2764, 1.7382, 1.6426, 1.8956, 0.3291, 1.7493,\n",
       "         1.7731, 2.1461, 1.8922, 1.7782, 1.8130, 2.1632, 1.8078, 1.6428, 1.7377,\n",
       "         1.8521, 2.2933, 1.8891, 2.3656, 1.8887, 2.0507, 1.8459, 2.0362, 1.8460,\n",
       "         1.8890, 1.9467, 1.9882, 1.7926, 1.7839, 1.8165, 1.9465, 1.9504, 1.6444,\n",
       "         1.9062, 2.2830, 1.8085, 1.7776, 1.7635, 1.9728, 1.8293, 1.8096, 1.7668,\n",
       "         1.9736, 1.8939, 1.9542, 2.1443, 1.8073, 1.8017, 2.0316, 2.2159, 2.0897,\n",
       "         1.7406, 1.8384, 2.0547, 1.8611, 2.0514, 2.1183, 1.9933, 1.9774, 1.9988,\n",
       "         2.1755, 1.8897, 3.7346, 1.7875, 1.8750, 1.9909, 1.9578, 1.6699, 1.3664,\n",
       "         1.6854, 1.8463, 1.8435, 1.8553, 1.7961, 1.6215, 1.9247, 2.0119, 1.9417,\n",
       "         2.3212, 2.0257, 1.6781, 1.7752, 2.0266, 1.6741, 1.8956, 2.1080, 1.9283,\n",
       "         1.9036, 1.9323, 1.7454, 1.7944, 0.4364, 2.0175, 1.9338, 1.7725, 2.0437,\n",
       "         1.9008, 1.9926, 2.3580, 1.8088, 2.1252, 2.5061, 2.2518, 1.8589, 1.7454,\n",
       "         1.9953, 1.9477, 2.0426, 1.6382, 1.9286, 1.6900, 2.5136, 2.3153],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0808,  0.0031, -0.0053,  ...,  0.0714,  0.0156,  0.0117],\n",
       "         [ 0.0194,  0.0034,  0.0042,  ...,  0.0788, -0.1300, -0.0407],\n",
       "         [-0.0796, -0.0390,  0.0259,  ..., -0.0035,  0.0700, -0.0440],\n",
       "         ...,\n",
       "         [ 0.0360, -0.0811, -0.0108,  ..., -0.0541,  0.0766, -0.0077],\n",
       "         [-0.0782,  0.0109, -0.0002,  ...,  0.0113,  0.0383,  0.0162],\n",
       "         [ 0.0243, -0.0393, -0.0271,  ...,  0.0456, -0.0221,  0.0533]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2189, -0.0431, -0.0785,  ..., -0.1006,  0.2099, -0.2321],\n",
       "         [ 1.1314,  0.0918,  0.1182,  ..., -0.6343, -0.5981,  0.1248],\n",
       "         [ 0.2193,  0.2777,  0.3141,  ..., -0.3251, -0.4043,  0.2689],\n",
       "         ...,\n",
       "         [ 0.2634,  0.3939, -0.0357,  ..., -0.1453,  0.3227,  0.5001],\n",
       "         [-0.3813, -0.2436,  1.0186,  ..., -0.0320, -0.2730, -0.2316],\n",
       "         [ 0.1350,  0.0544,  0.3805,  ..., -1.1644, -0.0533, -0.1968]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.1950,  0.2631, -0.0758,  ...,  0.6700,  0.6630, -0.4726],\n",
       "         [ 0.1304,  0.2007, -0.1871,  ..., -0.8946, -0.8743,  0.4376],\n",
       "         [ 0.6323,  0.4839,  0.6452,  ...,  0.8658, -0.4834,  0.6864],\n",
       "         ...,\n",
       "         [ 0.2038, -0.1178,  1.0178,  ...,  0.1270, -0.2074,  0.3628],\n",
       "         [ 0.0133, -0.4721, -0.2321,  ..., -0.3032,  0.6053, -1.1244],\n",
       "         [ 0.5516, -0.5350,  1.4712,  ...,  0.1164, -0.1830, -0.8255]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.5986,  1.2413,  1.2899,  ...,  1.4475, -1.1638,  0.5822],\n",
       "         [ 1.7642, -0.4001,  0.6228,  ..., -0.4306, -0.4564,  1.4171],\n",
       "         [-1.5075, -1.7333,  0.0400,  ...,  1.6065, -0.3073,  1.7609],\n",
       "         ...,\n",
       "         [ 0.2904, -1.6922,  1.9145,  ..., -0.8189,  0.9698,  1.3503],\n",
       "         [-0.6115, -2.4779,  0.1185,  ..., -0.3881, -0.0301, -0.1664],\n",
       "         [-0.2783,  1.1324,  0.2504,  ...,  0.5038, -1.6225, -0.1231]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1681,  0.1491,  0.1675,  0.2072,  0.1315,  0.1704,  0.2046,  0.1404,\n",
       "          0.1260,  0.1695,  0.1863,  0.1376,  0.1967,  0.1806,  0.1299,  0.1458,\n",
       "          0.1719,  0.1749,  0.1853,  0.1880,  0.1616,  0.1462,  0.1678,  0.0856,\n",
       "          0.2187,  0.1721,  0.1724,  0.1409,  0.1714,  0.2029,  0.1868,  0.1305,\n",
       "          0.2077,  0.2029,  0.1924,  0.1432,  0.1843,  0.1856,  0.1952,  0.1451,\n",
       "          0.1691,  0.1910,  0.1194,  0.1901,  0.1622,  0.1560,  0.0631,  0.1724,\n",
       "          0.1769,  0.1807,  0.1807,  0.1935,  0.1558,  0.1450,  0.1504,  0.1712,\n",
       "          0.1820,  0.1748,  0.1676,  0.1428,  0.1946,  0.1597,  0.1815,  0.1660,\n",
       "          0.1843,  0.1748,  0.1711,  0.2144,  0.1509,  0.1891,  0.1921,  0.1801,\n",
       "          0.1903,  0.1826,  0.1661,  0.1647,  0.2234,  0.1534,  0.1527,  0.1567,\n",
       "          0.1509,  0.1699,  0.1581,  0.1934,  0.1754,  0.1411,  0.1696,  0.1552,\n",
       "          0.1850,  0.1309,  0.1606,  0.1688,  0.1418,  0.1753,  0.1655,  0.1809,\n",
       "          0.0562,  0.2165,  0.2539,  0.1460,  0.1874,  0.1940,  0.1558,  0.1553,\n",
       "          0.2087,  0.1777,  0.1743,  0.1818,  0.1946,  0.1672,  0.2006,  0.1749,\n",
       "          0.1935,  0.1806,  0.1840,  0.1896,  0.2182,  0.1880,  0.1588,  0.1884,\n",
       "          0.1730,  0.1753,  0.1957,  0.1266,  0.1515,  0.2379,  0.2049,  0.1534,\n",
       "          0.1761,  0.1715,  0.1506,  0.1840,  0.1719,  0.1607,  0.1316,  0.1910,\n",
       "          0.1804,  0.1452,  0.1737,  0.1845,  0.1683,  0.1447,  0.1970,  0.1961,\n",
       "          0.1961,  0.1477,  0.1722,  0.1565,  0.1746,  0.0402,  0.1702,  0.1922,\n",
       "          0.1364,  0.1844,  0.1583,  0.2017,  0.1886,  0.1775,  0.1554,  0.3312,\n",
       "          0.1712,  0.1829,  0.1707,  0.1496,  0.1778,  0.1969,  0.1388,  0.1622,\n",
       "          0.1674,  0.1699,  0.1607,  0.1721,  0.1517,  0.1672,  0.1916,  0.1758,\n",
       "          0.1500,  0.1443,  0.1871,  0.1908,  0.1923,  0.2030,  0.5186,  0.1651,\n",
       "          0.2377,  0.1586,  0.1818,  0.1817,  0.1751,  0.2017,  0.1895,  0.1559,\n",
       "          0.1931,  0.1673,  0.1487,  0.2169,  0.1828,  0.2064,  0.1631,  0.1735,\n",
       "          0.1902,  0.1710,  0.1990,  0.1570,  0.1824,  0.1560,  0.3096,  0.1786,\n",
       "          0.1865,  0.2030,  0.2062,  0.2229,  0.1485,  0.0682,  0.1873,  0.1989,\n",
       "          0.1927,  0.1556,  0.1706,  0.1758,  0.1955,  0.1644,  0.1592,  0.2018,\n",
       "          0.1383,  0.1806,  0.1748,  0.1987,  0.1766,  0.1471,  0.1727,  0.1897,\n",
       "          0.1238,  0.1516,  0.1381,  0.2281,  0.2137,  0.1351,  0.1931,  0.0616,\n",
       "          0.1635,  0.1672,  0.1671,  0.1997,  0.1390,  0.0450,  0.1963,  0.2451,\n",
       "          0.1067,  0.1766,  0.1845,  0.1642,  0.1890,  0.1896,  0.1779,  0.1799,\n",
       "          0.1782,  0.1311,  0.1612,  0.1725,  0.1785,  0.1704,  0.1913,  0.1642,\n",
       "          0.2056,  0.1893,  0.1462,  0.1907,  0.1491,  0.1494,  0.1723,  0.1682,\n",
       "          0.1508,  0.0967,  0.1665,  0.1327,  0.1778,  0.2144,  0.2090,  0.1620,\n",
       "          0.1329,  0.1422,  0.1941,  0.1825,  0.1930,  0.1746,  0.1586,  0.1804,\n",
       "          0.1833,  0.1712,  0.1552,  0.2014,  0.1722,  0.1494,  0.1359,  0.1591,\n",
       "          0.2114,  0.1787,  0.2113,  0.1490,  0.1922,  0.1656,  0.0546,  0.1989,\n",
       "          0.1221,  0.1882,  0.1676,  0.1331,  0.0915,  0.1481,  0.2360,  0.1896,\n",
       "          0.1390,  0.1542,  0.1793,  0.1644,  0.1942,  0.1812,  0.1582,  0.1563,\n",
       "          0.1598,  0.2165,  0.2188,  0.1713,  0.2571,  0.2025,  0.1742,  0.2311,\n",
       "          0.2251,  0.1418,  0.1790,  0.1875,  0.1555,  0.1916,  0.1738,  0.1540,\n",
       "          0.1886,  0.1726,  0.1620,  0.2169,  0.1415,  0.1331,  0.2048,  0.1548,\n",
       "          0.1777,  0.1378,  0.1708,  0.1557,  0.1485,  0.2145,  0.1896,  0.1584,\n",
       "          0.1738,  0.1737,  0.1518,  0.1792,  0.1502,  0.1850,  0.1468,  0.1782,\n",
       "          0.2109,  0.1679,  0.1781,  0.1618,  0.1779,  0.1486,  0.1874, -0.0658,\n",
       "          0.1598,  0.1673,  0.1685,  0.1758,  0.1734,  0.1652,  0.1577,  0.2035,\n",
       "          0.1683,  0.1570,  0.1863,  0.1648,  0.2008,  0.1304,  0.1743,  0.1912,\n",
       "          0.1693,  0.1878,  0.1246,  0.2083,  0.1703,  0.1884,  0.1700,  0.1779,\n",
       "          0.1213,  0.1754,  0.1562,  0.1695,  0.1579,  0.1402,  0.1587,  0.2029,\n",
       "          0.1464,  0.1244,  0.1304,  0.0416,  0.1546,  0.1657,  0.1927,  0.1991,\n",
       "          0.1681,  0.1666,  0.1458,  0.1572,  0.1715,  0.1573,  0.1849,  0.2028,\n",
       "          0.1702,  0.1515,  0.1922,  0.2020,  0.1917,  0.1967,  0.1671,  0.1536,\n",
       "          0.1990,  0.1946,  0.1905,  0.1643,  0.1819,  0.1831,  0.1955,  0.1508,\n",
       "          0.1877,  0.2159,  0.1595,  0.1780,  0.1672,  0.1874,  0.1522,  0.1597,\n",
       "          0.1798,  0.2020,  0.1815,  0.1917,  0.2253,  0.1683,  0.1878,  0.1711,\n",
       "          0.2144,  0.2013,  0.1735,  0.1758,  0.1712,  0.1442,  0.1974,  0.1835,\n",
       "          0.1931,  0.1787,  0.1818,  0.1964,  0.1680,  0.2591,  0.1739,  0.1717,\n",
       "          0.1822,  0.1759,  0.1607,  0.1087,  0.1357,  0.1783,  0.1756,  0.1659,\n",
       "          0.1540,  0.1741,  0.1843,  0.1526,  0.2032,  0.0703,  0.2224,  0.1286,\n",
       "          0.1533,  0.1994,  0.1519,  0.1903,  0.2009,  0.1874,  0.1786,  0.1785,\n",
       "          0.1694,  0.1651,  0.0472,  0.2081,  0.1958,  0.1548,  0.2122,  0.1774,\n",
       "          0.1840,  0.1554,  0.1651,  0.1821,  0.2173,  0.1396,  0.1456,  0.1667,\n",
       "          0.1789,  0.2038,  0.1752,  0.1479,  0.1815,  0.1779,  0.3156,  0.1506],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0919, -0.0314,  0.0266,  ...,  0.1082, -0.0134,  0.0450],\n",
       "         [-0.0528, -0.0083,  0.0453,  ..., -0.0253, -0.0089, -0.0102],\n",
       "         [ 0.0152,  0.0139,  0.0269,  ..., -0.0501,  0.1086,  0.0262],\n",
       "         ...,\n",
       "         [-0.0005, -0.0714,  0.0596,  ..., -0.0717,  0.0564, -0.0658],\n",
       "         [-0.0914, -0.0372,  0.0346,  ..., -0.0386, -0.0033, -0.0170],\n",
       "         [-0.0259, -0.0198, -0.0494,  ...,  0.0648,  0.0304, -0.0555]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.9448, -0.2116,  0.0644,  ...,  0.1082,  0.0210, -0.2294],\n",
       "         [-0.5002, -0.1125,  0.2631,  ..., -0.0807, -0.0226,  0.0277],\n",
       "         [-0.1105,  0.0062,  0.0084,  ..., -0.2947, -0.3465,  0.3041],\n",
       "         ...,\n",
       "         [ 0.3103,  0.4538,  0.0409,  ..., -0.4323,  0.4578, -0.8438],\n",
       "         [-1.0573,  0.0498, -0.1459,  ..., -1.0222, -0.1578,  1.0314],\n",
       "         [ 0.2527,  0.0380, -0.6444,  ...,  0.2238,  0.1465, -0.0263]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.3064,  2.3844, -1.4517,  ...,  1.6760, -1.0640,  0.6503],\n",
       "         [ 2.1648, -0.5104,  0.6707,  ...,  1.8130,  0.9510, -1.6072],\n",
       "         [ 0.0959,  0.6832,  1.2595,  ...,  0.3903,  2.3627,  0.2072],\n",
       "         ...,\n",
       "         [ 0.3251, -0.6376, -0.8889,  ..., -0.4064, -2.1447, -0.6971],\n",
       "         [-1.3499,  2.1007,  1.0411,  ..., -1.0742, -1.1698,  0.1081],\n",
       "         [-0.2103, -2.0341, -1.4566,  ...,  0.3167,  0.7022,  2.5791]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1629,  1.4034,  0.1843,  ..., -0.0165,  1.0992,  0.4597],\n",
       "         [ 0.8289,  0.8716, -0.2426,  ..., -1.2052, -1.0951, -0.3535],\n",
       "         [-0.8435, -0.0105,  0.1556,  ..., -1.9507,  0.0909,  1.2834],\n",
       "         ...,\n",
       "         [ 2.0954, -2.2247, -0.9697,  ...,  1.8037,  0.4072, -1.1364],\n",
       "         [ 0.0974,  2.2133,  0.9143,  ..., -0.6170,  0.4613, -1.2787],\n",
       "         [-0.9321,  0.1052, -0.9151,  ..., -0.5582,  0.5072, -0.4765]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0657,  0.0842,  0.0582,  0.0824,  0.0510,  0.0718,  0.1004,  0.0687,\n",
       "          0.0591,  0.0572,  0.0731,  0.0544,  0.0628,  0.0717,  0.0603,  0.0615,\n",
       "          0.0728,  0.0603,  0.0686,  0.0775,  0.0587,  0.0542,  0.0582,  0.0391,\n",
       "          0.0775,  0.0618,  0.0605,  0.0738,  0.0703,  0.0904,  0.0619, -0.0513,\n",
       "          0.0905,  0.0775,  0.1003,  0.0620,  0.0637,  0.0642,  0.0761,  0.0647,\n",
       "          0.0581,  0.0688,  0.0657,  0.0811,  0.0626,  0.0572,  0.0286,  0.0587,\n",
       "          0.0657,  0.0588,  0.0787,  0.0807,  0.0513,  0.0708,  0.0644,  0.0690,\n",
       "          0.0646,  0.0698,  0.0570, -0.0626,  0.0663,  0.0583,  0.0814,  0.0630,\n",
       "          0.0695,  0.0706,  0.0747,  0.0761,  0.0738,  0.0784,  0.0768,  0.0679,\n",
       "          0.0615,  0.0676,  0.0514,  0.0539,  0.0838,  0.0542,  0.0588,  0.0909,\n",
       "          0.0733,  0.0804,  0.0584,  0.0909,  0.0663,  0.0688,  0.0614,  0.0624,\n",
       "          0.0752,  0.0682,  0.0574,  0.0736,  0.0599,  0.0573,  0.0672,  0.0605,\n",
       "          0.1165,  0.0768,  0.1834,  0.0549,  0.0684,  0.0606, -0.0614,  0.0585,\n",
       "          0.0707,  0.0711,  0.0685,  0.0668,  0.0821,  0.0639,  0.0781,  0.0609,\n",
       "          0.0921,  0.0604,  0.0611,  0.0682,  0.0697,  0.0731,  0.0532,  0.0623,\n",
       "          0.0640,  0.0674,  0.0546,  0.0531,  0.0537,  0.2777,  0.0822,  0.0572,\n",
       "          0.0862,  0.0617,  0.0550,  0.0667,  0.0659,  0.0805,  0.0732,  0.0616,\n",
       "          0.0725,  0.1669,  0.0602,  0.0753,  0.0627,  0.0700,  0.0860,  0.0800,\n",
       "          0.0764,  0.0585,  0.0752,  0.0673,  0.0633, -0.0201,  0.0667,  0.0634,\n",
       "          0.0658,  0.0549,  0.0489,  0.0722,  0.0758,  0.0830,  0.0695,  0.2155,\n",
       "          0.0581,  0.0841,  0.0525,  0.0616,  0.0678,  0.0840,  0.0681,  0.0876,\n",
       "          0.0711,  0.0579,  0.0628,  0.0708,  0.0763,  0.0561,  0.0775,  0.0634,\n",
       "          0.0775,  0.0593,  0.0777,  0.0626,  0.0632,  0.0821,  0.3076,  0.0684,\n",
       "          0.1378,  0.0544,  0.0691,  0.0649,  0.0590,  0.0708,  0.0762,  0.0727,\n",
       "          0.0568,  0.1023,  0.0554,  0.0811,  0.0721,  0.0690,  0.0581,  0.0680,\n",
       "          0.0679,  0.0652,  0.0766,  0.0526,  0.0676,  0.0562,  0.4631,  0.0877,\n",
       "          0.0797,  0.0681,  0.0632,  0.0833,  0.0631,  0.0367,  0.0627,  0.0684,\n",
       "          0.0704,  0.0555,  0.0720,  0.0622,  0.0681,  0.0609,  0.0601,  0.0751,\n",
       "          0.0632,  0.0642,  0.1040,  0.0530,  0.0767,  0.0659,  0.0655,  0.0675,\n",
       "          0.0668,  0.0531,  0.0573,  0.0774,  0.1005,  0.0673,  0.0775,  0.0607,\n",
       "          0.0902,  0.0747,  0.0583,  0.0578,  0.0642,  0.0414,  0.0592,  0.0845,\n",
       "          0.0966,  0.0623,  0.0720,  0.0594,  0.0719,  0.0752,  0.0619,  0.0740,\n",
       "          0.0897,  0.0587,  0.0631,  0.0698,  0.0752,  0.0561,  0.0539,  0.0918,\n",
       "          0.0810,  0.0633,  0.0662,  0.0774,  0.0596,  0.0543,  0.0739,  0.0665,\n",
       "          0.0619,  0.0453,  0.0560,  0.0662,  0.0637,  0.0688,  0.0857,  0.0702,\n",
       "          0.0698,  0.0535,  0.0712,  0.0694,  0.0616,  0.0719,  0.0619,  0.0756,\n",
       "          0.0659,  0.0612,  0.0606,  0.0704,  0.0776,  0.0510,  0.1630,  0.0747,\n",
       "          0.0904,  0.0600,  0.0736,  0.0674,  0.0687,  0.0640,  0.0164,  0.0704,\n",
       "          0.0575,  0.0675,  0.0664,  0.0597,  0.0465,  0.0599,  0.0901,  0.0738,\n",
       "          0.1643,  0.0515,  0.0806,  0.0526,  0.0662,  0.0649,  0.0904,  0.0684,\n",
       "          0.0633,  0.0560,  0.0626,  0.0680,  0.1619,  0.0777,  0.0549,  0.1001,\n",
       "          0.0696,  0.0647,  0.0768,  0.0691,  0.0661,  0.0658,  0.0649,  0.0508,\n",
       "          0.0842,  0.0533,  0.0671,  0.0910,  0.0678,  0.0740,  0.0750,  0.0510,\n",
       "          0.0690,  0.0552,  0.0511,  0.0532,  0.0552,  0.0958,  0.0834,  0.0640,\n",
       "          0.0652,  0.0634,  0.0535,  0.1304,  0.0604,  0.0609,  0.0558,  0.0697,\n",
       "          0.0879,  0.0572,  0.0689,  0.0598,  0.0731,  0.0583,  0.0517,  0.0319,\n",
       "          0.0549,  0.0604,  0.0627,  0.0947,  0.0818,  0.0555,  0.0735,  0.0752,\n",
       "          0.0723,  0.0575,  0.0803,  0.0827,  0.0836,  0.1442,  0.0718,  0.0646,\n",
       "          0.0737,  0.0581,  0.0508,  0.0644,  0.0884,  0.0613,  0.0614,  0.0660,\n",
       "          0.0714,  0.0638,  0.0616,  0.0721,  0.0642, -0.0495,  0.0529,  0.0778,\n",
       "          0.0738,  0.0503,  0.0577,  0.0213,  0.0547,  0.1077,  0.0750,  0.0684,\n",
       "          0.0622,  0.0703,  0.0666,  0.0688,  0.0577,  0.0918,  0.0795,  0.0721,\n",
       "          0.0725, -0.0788,  0.0617,  0.0752,  0.0628,  0.0566,  0.0598,  0.0668,\n",
       "          0.0732,  0.0836,  0.0654,  0.0627,  0.0539,  0.0738,  0.0613,  0.0557,\n",
       "          0.0684,  0.1044,  0.0714,  0.0580,  0.0589,  0.0632,  0.0610,  0.0611,\n",
       "          0.0888,  0.0680,  0.0660,  0.0625,  0.0808,  0.0646,  0.0592,  0.0716,\n",
       "          0.0627,  0.0906,  0.0647,  0.0639,  0.0652,  0.0541,  0.0653,  0.0636,\n",
       "          0.0708,  0.0752,  0.0659,  0.0677,  0.0612,  0.1008,  0.0694,  0.0621,\n",
       "          0.0663,  0.0573,  0.0625,  0.0486,  0.0670,  0.0710,  0.0604,  0.0611,\n",
       "          0.0777,  0.0605,  0.0776,  0.2264,  0.0642,  0.0689,  0.0752,  0.0477,\n",
       "          0.0551,  0.0674,  0.0553,  0.0597,  0.0656,  0.0759,  0.0657,  0.0753,\n",
       "          0.0844,  0.0575, -0.0162,  0.0641,  0.0728,  0.0573,  0.0738,  0.0726,\n",
       "          0.0645,  0.0681,  0.0583,  0.0545,  0.0802,  0.0865,  0.0646,  0.0661,\n",
       "          0.0693,  0.0854,  0.0834,  0.0505,  0.0757,  0.0620,  0.1481,  0.0718],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.2365, -0.7261,  0.6803,  ..., -0.5170,  1.0180,  0.6581],\n",
       "         [ 0.3828, -1.1631, -0.2686,  ..., -1.2235,  0.1205,  0.9521],\n",
       "         [ 0.1983,  0.5458,  0.8462,  ...,  0.2081,  0.0919,  0.0496],\n",
       "         ...,\n",
       "         [-0.0406, -0.7786, -1.2690,  ..., -0.3790, -1.3515, -0.3421],\n",
       "         [-2.2154, -0.3381,  0.3206,  ...,  0.7009, -0.5655, -0.1608],\n",
       "         [-1.1768, -0.4463,  0.2766,  ..., -0.8461, -0.7476,  0.5644]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.0536, -0.2496,  0.1147,  ..., -0.2675,  0.3659,  0.1910],\n",
       "         [ 0.1327, -0.1665, -0.5374,  ..., -0.1923,  0.2104, -0.1830],\n",
       "         [-0.0884, -0.0911,  0.1286,  ...,  0.7157, -0.3686,  0.2933],\n",
       "         ...,\n",
       "         [-0.3388,  0.9014,  0.1017,  ...,  0.0908,  0.8275,  0.1551],\n",
       "         [-0.1467, -0.3875,  0.1540,  ..., -0.0972, -0.1337, -0.1115],\n",
       "         [ 0.4521,  0.1145, -0.0185,  ...,  0.1739, -0.1970, -0.1555]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([2.5221, 2.7497, 2.3036, 2.6598, 0.5792, 2.1587, 2.9897, 2.6496, 3.6946,\n",
       "         2.0740, 2.2641, 2.8862, 2.3327, 2.1497, 3.6983, 2.2000, 2.2531, 2.6053,\n",
       "         2.3922, 2.4092, 2.0676, 1.7132, 2.3424, 1.6702, 2.2977, 2.3568, 2.0750,\n",
       "         2.9784, 2.0163, 2.9494, 2.0540, 2.2705, 2.9354, 2.3471, 3.3914, 2.6562,\n",
       "         2.2222, 2.1781, 2.7174, 2.1906, 2.1503, 1.9474, 2.3154, 2.6810, 2.4192,\n",
       "         1.8831, 1.3022, 2.2222, 2.2802, 1.9793, 2.6178, 2.5925, 2.5602, 4.5179,\n",
       "         2.4193, 2.2047, 2.4216, 2.2771, 1.9973, 2.6244, 2.1549, 2.4640, 2.6497,\n",
       "         2.2524, 2.6708, 2.2553, 2.3176, 2.0892, 2.9327, 2.7067, 2.6999, 2.5569,\n",
       "         2.3043, 1.9932, 2.0956, 2.2073, 2.4164, 2.2265, 2.4198, 2.5924, 2.3463,\n",
       "         2.5749, 2.0562, 2.6057, 2.2651, 2.2930, 2.0754, 2.6630, 1.9595, 2.2001,\n",
       "         2.0490, 2.3976, 2.2297, 2.2967, 2.4707, 2.2671, 2.0989, 2.3619, 4.4444,\n",
       "         1.9350, 2.2023, 1.9676, 2.2574, 2.6100, 2.2472, 2.2390, 2.1311, 2.3016,\n",
       "         2.8540, 2.4243, 2.6884, 2.5312, 2.0690, 2.2004, 1.9890, 2.5471, 2.6636,\n",
       "         2.2289, 1.8463, 1.9669, 2.1130, 2.3206, 2.1153, 2.2332, 1.8342, 6.3079,\n",
       "         2.8842, 2.0572, 2.7212, 2.3292, 1.9783, 1.9699, 2.6107, 2.4428, 2.5965,\n",
       "         2.1780, 2.7867, 3.9250, 1.9464, 1.9713, 2.0642, 2.8206, 2.6865, 2.3035,\n",
       "         2.2246, 2.2189, 2.3210, 2.4493, 2.5129, 0.4819, 1.8933, 2.2766, 2.6923,\n",
       "         2.0197, 2.0301, 2.1573, 2.0199, 2.3697, 2.2568, 6.5224, 1.9422, 2.4859,\n",
       "         2.1477, 1.9101, 2.5055, 2.5467, 2.6155, 2.7194, 2.0907, 1.9739, 2.2919,\n",
       "         2.0666, 2.0880, 1.9608, 2.4779, 2.3112, 2.1253, 2.7242, 2.1300, 2.2052,\n",
       "         2.0513, 2.2028, 2.4980, 2.1640, 4.0248, 2.2141, 2.2457, 2.2946, 2.1275,\n",
       "         2.4129, 3.9943, 3.0649, 2.3472, 2.7285, 2.0566, 2.3178, 2.1636, 2.5778,\n",
       "         1.9878, 2.0203, 2.6193, 2.4730, 2.9224, 2.5418, 2.3123, 2.2454, 2.0516,\n",
       "         2.4247, 2.4151, 2.0574, 2.4971, 2.5551, 2.4407, 1.0141, 2.0401, 2.0927,\n",
       "         2.4123, 2.2201, 2.4840, 2.1743, 2.2428, 1.9187, 2.4384, 2.1616, 2.7892,\n",
       "         2.1839, 3.6899, 1.9918, 2.5202, 2.2457, 2.2766, 2.1738, 3.6605, 2.4880,\n",
       "         2.8479, 3.0183, 2.8733, 3.3761, 2.0020, 2.3989, 2.6048, 3.0364, 2.1866,\n",
       "         2.3181, 5.4268, 1.6619, 2.7694, 3.1583, 2.9665, 2.3198, 2.1452, 2.1408,\n",
       "         2.4369, 2.0295, 2.0217, 2.4618, 2.3438, 2.5879, 3.0384, 2.8189, 2.6355,\n",
       "         2.0958, 2.0389, 2.9257, 2.9099, 2.2610, 2.2642, 2.0912, 2.4346, 2.3255,\n",
       "         2.0970, 2.3175, 2.1791, 3.4808, 1.7843, 6.7163, 1.9749, 2.2396, 2.9516,\n",
       "         2.2306, 3.4165, 2.2098, 2.4333, 2.1519, 2.3605, 2.2321, 2.0795, 2.2860,\n",
       "         2.0230, 2.5763, 2.7945, 2.3458, 2.3520, 1.9837, 5.1599, 2.5136, 2.7009,\n",
       "         2.0539, 2.2556, 4.8358, 2.5464, 2.3344, 0.5649, 2.3362, 2.7072, 1.8993,\n",
       "         2.3268, 3.0035, 3.0298, 2.6048, 2.9432, 2.4675, 4.5772, 2.0659, 2.4432,\n",
       "         1.8380, 2.0803, 1.9897, 3.0269, 2.8235, 2.0974, 2.1215, 2.3328, 4.0384,\n",
       "         7.6443, 1.9090, 2.4348, 2.8755, 2.4078, 2.9772, 2.6338, 2.4658, 2.0673,\n",
       "         2.2346, 2.0680, 2.4192, 2.6848, 2.0205, 2.0759, 2.9793, 2.6965, 2.2333,\n",
       "         2.3439, 2.5166, 2.1304, 2.4089, 2.3716, 2.0516, 1.9769, 2.7856, 2.4122,\n",
       "         1.9820, 2.1284, 1.9368, 2.0964, 3.2542, 2.0131, 2.0436, 2.0332, 2.4444,\n",
       "         2.9093, 1.8104, 2.0655, 2.9608, 2.3689, 2.2217, 1.9287, 0.5006, 2.0166,\n",
       "         2.0771, 2.2997, 2.8789, 2.2318, 2.0985, 3.9007, 2.2303, 2.0671, 1.9555,\n",
       "         2.1947, 3.1725, 2.2588, 3.4723, 2.4979, 2.1265, 2.3866, 2.1368, 4.2778,\n",
       "         2.4248, 2.8376, 2.1754, 2.0092, 2.0974, 2.3379, 2.2443, 2.3763, 2.2409,\n",
       "         2.6501, 2.6272, 2.1220, 2.6881, 2.4532, 2.7169, 2.3319, 0.3952, 1.8393,\n",
       "         2.3964, 2.5374, 2.1905, 1.9416, 2.1001, 2.7837, 2.5259, 2.2330, 2.7938,\n",
       "         2.0535, 2.9807, 2.5333, 3.1914, 2.2847, 2.1215, 2.3525, 2.4152, 2.5936,\n",
       "         2.1636, 2.9034, 2.2416, 2.1961, 2.1006, 1.9807, 2.3496, 2.1616, 1.9037,\n",
       "         2.0487, 3.0771, 2.0793, 2.3052, 2.4698, 2.2161, 2.6033, 2.4078, 2.0915,\n",
       "         2.1008, 2.0461, 2.4570, 2.5963, 2.0358, 1.9979, 2.5305, 2.3466, 2.3921,\n",
       "         1.9244, 1.9896, 3.1783, 2.5243, 2.4446, 2.4749, 2.1154, 2.3124, 2.6869,\n",
       "         2.3514, 2.0018, 6.9089, 1.9679, 2.1929, 2.3182, 1.9121, 1.8761, 1.6004,\n",
       "         2.3747, 2.2670, 1.9643, 2.1088, 2.1765, 1.9735, 2.3045, 4.4698, 2.1788,\n",
       "         3.5136, 2.4036, 1.6076, 2.5083, 2.2999, 1.9082, 1.9806, 2.0477, 2.1655,\n",
       "         2.3277, 2.5141, 2.3015, 2.2316, 0.5157, 2.0544, 2.7221, 1.9844, 2.3098,\n",
       "         2.5003, 2.2894, 3.2401, 2.3600, 2.8101, 3.2302, 3.4497, 2.3571, 1.9396,\n",
       "         2.3946, 2.6905, 2.6506, 2.0928, 2.4324, 1.9526, 4.3582, 3.7641],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0409, -0.0054, -0.0616,  ..., -0.0695,  0.0623, -0.0257],\n",
       "         [ 0.0374,  0.0344, -0.0020,  ...,  0.0743, -0.0776,  0.0352],\n",
       "         [-0.0546,  0.0114, -0.0273,  ...,  0.0021, -0.0052,  0.0121],\n",
       "         ...,\n",
       "         [ 0.0045,  0.0020,  0.0022,  ..., -0.0017, -0.0179, -0.0189],\n",
       "         [ 0.0201,  0.0601,  0.0140,  ...,  0.0091,  0.0735, -0.0027],\n",
       "         [-0.0206, -0.0278,  0.0402,  ..., -0.0458, -0.0328,  0.0148]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.6373, -0.0333, -0.9232,  ..., -0.2592,  0.4978,  0.8061],\n",
       "         [ 0.5421, -1.0308, -0.7741,  ..., -0.2094, -0.1860,  0.7277],\n",
       "         [-0.2224,  0.3320,  0.5312,  ..., -0.6891, -0.1723,  0.2163],\n",
       "         ...,\n",
       "         [-0.4235, -0.2071, -0.2087,  ...,  0.0850, -0.7621, -0.1923],\n",
       "         [-0.3180, -0.1914,  0.0824,  ...,  0.0142, -0.8993,  0.3229],\n",
       "         [ 0.1444, -0.7258, -0.0576,  ...,  0.3291, -0.0097, -0.2347]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.2522,  1.4758,  1.3589,  ...,  1.1716,  0.1911,  1.2620],\n",
       "         [ 0.0857,  0.1111,  1.0705,  ...,  0.6487, -0.3026, -0.6757],\n",
       "         [ 0.2551,  0.9350, -0.1407,  ...,  1.1461, -0.6164,  0.5534],\n",
       "         ...,\n",
       "         [-0.3071, -1.2373, -0.3215,  ..., -2.3351,  0.0928, -1.0183],\n",
       "         [ 0.7474, -0.0610,  0.6662,  ...,  0.4156, -0.4050, -0.8435],\n",
       "         [-0.3090,  0.4268,  1.9516,  ..., -0.4899,  0.1054,  0.2097]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1674,  0.5213,  1.7799,  ...,  0.3929, -2.2305, -0.6930],\n",
       "         [-0.2940, -0.9568, -1.4105,  ..., -0.7483, -0.0297, -0.6038],\n",
       "         [-0.0511,  0.6103,  0.1177,  ...,  0.5122, -0.5970, -2.1416],\n",
       "         ...,\n",
       "         [ 0.4544, -1.7956,  0.9433,  ...,  1.5579, -0.6078,  2.4429],\n",
       "         [ 0.0789,  0.1512,  0.3889,  ...,  0.8148,  0.3462,  0.4596],\n",
       "         [-0.9727,  0.2203, -0.0306,  ..., -0.2097,  1.2610, -0.1943]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2296,  0.2506,  0.2045,  0.2522,  0.2190,  0.1816,  0.3025,  0.2603,\n",
       "          0.2563,  0.1836,  0.2177,  0.2587,  0.2228,  0.2086,  0.1904,  0.1855,\n",
       "          0.2059,  0.2401,  0.2384,  0.2344,  0.2006,  0.1714,  0.2235, -0.1606,\n",
       "          0.2198,  0.2228,  0.1854,  0.2931,  0.2012,  0.2825,  0.1945,  0.2186,\n",
       "          0.2728,  0.2198,  0.3430,  0.2288,  0.2046,  0.2012,  0.2562,  0.2075,\n",
       "          0.2206,  0.1691,  0.2722,  0.2263,  0.2110,  0.1928,  0.1571,  0.2099,\n",
       "          0.2290,  0.1803,  0.2505,  0.2254,  0.2213, -0.3483,  0.2365,  0.2094,\n",
       "          0.2050,  0.2202,  0.1862,  0.2537,  0.2148,  0.2351,  0.2412,  0.1934,\n",
       "          0.2735,  0.1990,  0.2310,  0.1823,  0.2795,  0.2866,  0.2398,  0.2526,\n",
       "          0.1894,  0.1876,  0.1935,  0.2135,  0.2465,  0.2135,  0.2346,  0.2719,\n",
       "          0.2556,  0.2335,  0.1730,  0.2547,  0.2032,  0.1993,  0.1962,  0.2442,\n",
       "          0.1823,  0.1938,  0.1712,  0.2261,  0.2140,  0.2139,  0.2681,  0.1940,\n",
       "          0.1000,  0.2309,  0.6029,  0.2018,  0.2037,  0.1775,  0.2229,  0.2461,\n",
       "          0.2347,  0.2073,  0.1977,  0.2141,  0.2668,  0.2402,  0.2521,  0.2132,\n",
       "          0.2058,  0.2197,  0.1925,  0.2412,  0.2421,  0.2104,  0.1765,  0.2013,\n",
       "          0.2062,  0.2037,  0.1886,  0.1909,  0.1800,  0.6189,  0.2701,  0.1886,\n",
       "          0.2742,  0.2283,  0.1805,  0.1813,  0.2034,  0.2825,  0.2506,  0.2029,\n",
       "          0.2879,  0.4003,  0.1884,  0.1923,  0.1941,  0.2548,  0.2304,  0.2178,\n",
       "          0.1959,  0.2170,  0.2409,  0.2363,  0.2365, -0.0795,  0.1724,  0.2130,\n",
       "          0.2494,  0.1912,  0.1808,  0.2002,  0.1930,  0.2548,  0.1988,  0.5411,\n",
       "          0.1819,  0.2680,  0.1930,  0.1870,  0.2517,  0.2433,  0.2226,  0.2702,\n",
       "          0.2077,  0.1821,  0.2293,  0.1855,  0.2195,  0.1708,  0.2238,  0.2167,\n",
       "          0.2122,  0.2318,  0.2072,  0.2015,  0.1926,  0.2162,  0.7826,  0.2385,\n",
       "          0.3320,  0.2122,  0.2075,  0.2343,  0.1841,  0.2214,  0.3537,  0.2551,\n",
       "          0.2041,  0.2914,  0.1750,  0.2197,  0.2026,  0.2621,  0.1924,  0.1943,\n",
       "          0.2566,  0.2410,  0.2475,  0.2266,  0.2171,  0.2143,  0.4528,  0.2461,\n",
       "          0.2385,  0.1985,  0.2199,  0.2363,  0.2393,  0.0973,  0.1990,  0.2105,\n",
       "          0.2071,  0.2306,  0.2587,  0.2042,  0.2097,  0.1871,  0.2092,  0.2271,\n",
       "          0.2600,  0.1995,  0.3309,  0.1696,  0.2269,  0.2188,  0.1963,  0.2056,\n",
       "         -0.2608,  0.2150,  0.2367,  0.2808,  0.2815,  0.2706,  0.2136,  0.1260,\n",
       "          0.2757,  0.2713,  0.2301,  0.2141,  0.2302, -0.0655,  0.2476,  0.2823,\n",
       "          0.2815,  0.2335,  0.2118,  0.2132,  0.2268,  0.1945,  0.1952,  0.2280,\n",
       "          0.2401,  0.2061,  0.2891,  0.2460,  0.2482,  0.1826,  0.1934,  0.3321,\n",
       "          0.2948,  0.2171,  0.1926,  0.2167,  0.2521,  0.2114,  0.2203,  0.2318,\n",
       "          0.2087,  0.2336,  0.1907,  0.2603,  0.1729,  0.2078,  0.2854,  0.2221,\n",
       "          0.2506,  0.1779,  0.2026,  0.1972,  0.2073,  0.2129,  0.1953,  0.2377,\n",
       "          0.1885,  0.2461,  0.2593,  0.2059,  0.2106,  0.1543,  0.1537,  0.2779,\n",
       "          0.2646,  0.1893,  0.2086,  0.3160,  0.2589,  0.2205,  0.0775,  0.2256,\n",
       "          0.2472,  0.1785,  0.2065,  0.2412,  0.1535,  0.2404,  0.2771,  0.2401,\n",
       "          0.3288,  0.1905,  0.2429,  0.1857,  0.1810,  0.1754,  0.2745,  0.2389,\n",
       "          0.1911,  0.1996,  0.1950,  0.3192,  0.4744,  0.1795,  0.2095,  0.2825,\n",
       "          0.2161,  0.2270,  0.2538,  0.2388,  0.1960,  0.2020,  0.1880,  0.2347,\n",
       "          0.2710,  0.1992,  0.2056,  0.2853,  0.2508,  0.2030,  0.2323,  0.2427,\n",
       "          0.2123,  0.2237,  0.2130,  0.1775,  0.1842,  0.2598,  0.2708,  0.1826,\n",
       "          0.2058,  0.1834,  0.1967,  0.3560,  0.1985,  0.1812,  0.1888,  0.2149,\n",
       "          0.2686,  0.1823,  0.1923,  0.2729,  0.2229,  0.2301,  0.1732, -0.1020,\n",
       "          0.2173,  0.1911,  0.2084,  0.2546,  0.2565,  0.1970,  0.2739,  0.2000,\n",
       "          0.1940,  0.1687,  0.2159,  0.2787,  0.2401,  0.2928,  0.2289,  0.2112,\n",
       "          0.2767,  0.1997,  0.1982,  0.2210,  0.3043,  0.1812,  0.1930,  0.1984,\n",
       "          0.1998,  0.1931,  0.2247,  0.2473,  0.2282,  0.2416,  0.1867,  0.2623,\n",
       "          0.2347,  0.2031,  0.2168, -0.0801,  0.1554,  0.2618,  0.2175,  0.2169,\n",
       "          0.1946,  0.1985,  0.2363,  0.2703,  0.2268,  0.2835,  0.2004,  0.2728,\n",
       "          0.2231,  0.2582,  0.2001,  0.2079,  0.2146,  0.2232,  0.2668,  0.2117,\n",
       "          0.2554,  0.2210,  0.2090,  0.1849,  0.1801,  0.2269,  0.2070,  0.2037,\n",
       "          0.1858,  0.3109,  0.2034,  0.2132,  0.2296,  0.2141,  0.2513,  0.2326,\n",
       "          0.1937,  0.1921,  0.1795,  0.2479,  0.2233,  0.2100,  0.1836,  0.2269,\n",
       "          0.2116,  0.2420,  0.1857,  0.1791,  0.3261,  0.2322,  0.2462,  0.1990,\n",
       "          0.2046,  0.2289,  0.2387,  0.2341,  0.1810,  0.5316,  0.2086,  0.1987,\n",
       "          0.2315,  0.1739,  0.1744,  0.1590,  0.2064,  0.1961,  0.1811,  0.1835,\n",
       "          0.2141,  0.1982,  0.2064,  0.4937,  0.2095, -0.1544,  0.2335,  0.1523,\n",
       "          0.2204,  0.2006,  0.1837,  0.2030,  0.1920,  0.2038,  0.2147,  0.2328,\n",
       "          0.2576,  0.2092,  0.0933,  0.2006,  0.2627,  0.1947,  0.2337,  0.2427,\n",
       "          0.2004,  0.2772,  0.2293,  0.2226,  0.3021,  0.2810,  0.2067,  0.1869,\n",
       "          0.2185,  0.2485,  0.2576,  0.2057,  0.2104,  0.2036,  0.7887,  0.2881],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0842, -0.0834,  0.0186,  ..., -0.0181,  0.0273, -0.1000],\n",
       "         [ 0.0018,  0.0817, -0.0040,  ...,  0.0006,  0.0328, -0.0263],\n",
       "         [-0.0728, -0.0451,  0.0443,  ..., -0.0083,  0.0942, -0.0182],\n",
       "         ...,\n",
       "         [ 0.0187, -0.0282,  0.0534,  ..., -0.0376, -0.0328,  0.0470],\n",
       "         [ 0.0064,  0.0276,  0.0746,  ...,  0.0242,  0.0230,  0.0631],\n",
       "         [ 0.0310, -0.0590, -0.0335,  ..., -0.0355, -0.0098, -0.0498]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1677,  0.2845, -0.0427,  ...,  0.2071,  0.1617, -0.0327],\n",
       "         [ 0.7800,  0.6862, -0.2837,  ...,  0.0458, -0.0225,  0.2775],\n",
       "         [ 0.0309, -0.6053, -0.0256,  ..., -0.4777,  0.4834, -0.3681],\n",
       "         ...,\n",
       "         [ 0.3675,  0.4468, -0.0153,  ..., -0.1615, -0.3496, -0.1227],\n",
       "         [-0.0720, -0.4455, -0.2193,  ...,  0.1047,  0.1088, -0.5558],\n",
       "         [-0.3676, -0.0973,  0.4503,  ...,  0.1112, -0.2097, -0.3795]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.1647, -0.5051,  2.8095,  ..., -1.2276,  0.4151, -0.2789],\n",
       "         [-1.9720, -0.8686, -1.2445,  ...,  0.3614,  0.3333, -1.3894],\n",
       "         [-0.7162, -0.7963,  2.6978,  ..., -1.7670, -0.3750,  0.3093],\n",
       "         ...,\n",
       "         [-1.5111,  1.1997, -0.3621,  ..., -0.5633, -1.9057, -0.0601],\n",
       "         [ 4.1243, -0.6348,  1.7662,  ..., -1.3830, -0.9016,  0.0181],\n",
       "         [-1.3173,  1.4084, -3.0745,  ...,  0.4037,  0.3333, -0.2773]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.4385,  0.2797, -0.2334,  ..., -2.1418,  3.0344,  0.1913],\n",
       "         [-1.9781,  0.6109,  0.4170,  ...,  1.4527,  0.4748, -0.6325],\n",
       "         [ 1.4164,  0.7803, -0.8519,  ..., -1.1618,  2.6020, -1.7750],\n",
       "         ...,\n",
       "         [-0.9064, -1.0524, -1.8084,  ...,  0.1232, -1.6206, -1.6024],\n",
       "         [-0.6115,  0.2021, -0.4600,  ..., -0.6898, -0.6140, -0.1709],\n",
       "         [ 0.4095, -0.3828,  0.2228,  ...,  0.1921,  0.3160, -0.1963]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1607,  0.1120,  0.1324,  0.1889,  0.0358,  0.1578,  0.1928,  0.1110,\n",
       "          0.0985,  0.1201,  0.1488,  0.1419,  0.1357,  0.1583, -0.0678,  0.1141,\n",
       "          0.1519,  0.1587,  0.1480,  0.1463,  0.1360,  0.1103,  0.1269,  0.0405,\n",
       "          0.1613,  0.1571,  0.1287,  0.1126,  0.1451,  0.1871,  0.1351,  0.1062,\n",
       "          0.1863,  0.1850,  0.1860,  0.1018,  0.1348,  0.1322,  0.2052,  0.1220,\n",
       "          0.1431,  0.1238,  0.1815,  0.1747,  0.1451,  0.1254,  0.0381,  0.1320,\n",
       "          0.1528,  0.1154,  0.2192,  0.1629,  0.1439,  0.1744,  0.1095,  0.1892,\n",
       "          0.1299,  0.1534,  0.1217,  0.1249,  0.1483,  0.1193,  0.1797,  0.1395,\n",
       "          0.1889,  0.1597,  0.1482,  0.1344,  0.1050,  0.1692,  0.1650,  0.1851,\n",
       "          0.1585,  0.1398,  0.1156,  0.1174,  0.2131,  0.1171,  0.1295,  0.1273,\n",
       "          0.1359,  0.1335,  0.1228,  0.2441,  0.1325,  0.1240,  0.1135,  0.1477,\n",
       "          0.1517,  0.0748,  0.1318,  0.1203,  0.1096,  0.1714,  0.1225,  0.1282,\n",
       "          0.1160,  0.1579,  0.3439, -0.1114,  0.1198,  0.1182,  0.1248,  0.1351,\n",
       "          0.1649,  0.1731,  0.1360,  0.1479,  0.2096,  0.1425,  0.2153,  0.1557,\n",
       "          0.1553,  0.1447,  0.1368, -0.1582,  0.1544,  0.1700,  0.1090,  0.1350,\n",
       "          0.1302,  0.1411,  0.1318,  0.1093,  0.1120,  0.1980,  0.2034,  0.1090,\n",
       "          0.1752,  0.1277,  0.1041,  0.1409,  0.1205,  0.1190,  0.1303,  0.1419,\n",
       "          0.1767,  0.1647,  0.1181,  0.1737,  0.1481,  0.1250,  0.1798,  0.2260,\n",
       "          0.1492,  0.1269,  0.1629,  0.1197,  0.1539,  0.0372,  0.1357,  0.1581,\n",
       "          0.1591,  0.1280,  0.1235,  0.1523,  0.1297,  0.1716,  0.1312,  0.2705,\n",
       "          0.1204,  0.1683,  0.1430,  0.1058, -0.1243,  0.1682,  0.1306,  0.1742,\n",
       "          0.1574,  0.1207,  0.1420,  0.1170,  0.1180,  0.1165,  0.1458,  0.1404,\n",
       "          0.1242,  0.1003,  0.1330,  0.1587,  0.1153,  0.2082,  1.5046,  0.1256,\n",
       "          0.2116,  0.1107,  0.1585,  0.1499,  0.1317,  0.1779, -0.1124,  0.1376,\n",
       "          0.1372,  0.1510,  0.1062,  0.1524,  0.1518,  0.1726,  0.1385,  0.1395,\n",
       "          0.1651,  0.1297,  0.1456,  0.1217,  0.1638,  0.1244,  1.2549,  0.1942,\n",
       "          0.1471,  0.1160,  0.1733,  0.2131,  0.1075,  0.0535,  0.1366,  0.1654,\n",
       "          0.1534,  0.1049, -0.1409,  0.1446,  0.1734,  0.1065,  0.1265,  0.1850,\n",
       "          0.1370,  0.1389,  0.1703,  0.1213,  0.2226,  0.1227,  0.1466,  0.1342,\n",
       "          0.2341,  0.1015,  0.1171,  0.2198,  0.2674,  0.1394,  0.1390,  0.1007,\n",
       "          0.1553,  0.1423,  0.1243,  0.1266,  0.0998,  0.0426,  0.1234,  0.2583,\n",
       "          0.0847,  0.1317,  0.1657,  0.1510,  0.1468,  0.1629,  0.1268,  0.1587,\n",
       "          0.2194,  0.1270,  0.1501,  0.1283,  0.2482,  0.1504,  0.1253,  0.1531,\n",
       "          0.2282,  0.1490,  0.1232,  0.1595,  0.1342,  0.1172,  0.1307,  0.1212,\n",
       "          0.1114,  0.0543,  0.1207,  0.1248,  0.1296,  0.1718,  0.1629,  0.1374,\n",
       "          0.1060,  0.1167,  0.1307,  0.1522,  0.1623,  0.1374,  0.1355,  0.1612,\n",
       "          0.1324,  0.1543,  0.1059,  0.1652,  0.1619,  0.1018,  0.0498,  0.1431,\n",
       "          0.2662,  0.1298,  0.1645,  0.1206,  0.1599,  0.1233, -0.0374,  0.1280,\n",
       "          0.1007,  0.1535,  0.1422,  0.1021, -0.0671,  0.1125,  0.2172,  0.1661,\n",
       "          0.1354,  0.1123,  0.2012,  0.1051,  0.1430,  0.1357, -0.1218,  0.1316,\n",
       "          0.1295,  0.1515,  0.1491, -0.1428,  0.1913,  0.1329,  0.1323,  0.1983,\n",
       "          0.1657,  0.1002,  0.1791,  0.1945,  0.1219,  0.1516,  0.1453,  0.1332,\n",
       "          0.1387,  0.1182,  0.1366,  0.2217,  0.1053,  0.1079,  0.1761,  0.1106,\n",
       "          0.1488,  0.1204,  0.1430,  0.1136,  0.1281,  0.2001,  0.1476,  0.1282,\n",
       "          0.1251,  0.1438,  0.1220,  0.1988,  0.1109,  0.1258,  0.1070,  0.1478,\n",
       "          0.2243,  0.1212,  0.1579,  0.1619,  0.1525,  0.1215,  0.1125, -0.0493,\n",
       "          0.1190,  0.1370,  0.1542,  0.1973,  0.1596,  0.1220, -0.1018,  0.1843,\n",
       "          0.1335,  0.1208,  0.1606, -0.1254,  0.1828,  0.1318,  0.1462,  0.1430,\n",
       "          0.1431,  0.1386,  0.1017,  0.1403,  0.1430,  0.1211,  0.1593,  0.1344,\n",
       "          0.1798,  0.1683,  0.1260,  0.1612,  0.1340, -0.1023,  0.1144,  0.2050,\n",
       "          0.1596,  0.1016,  0.1213, -0.0231,  0.1131,  0.1261,  0.2142,  0.1607,\n",
       "          0.1274,  0.1409,  0.1221,  0.1125,  0.1298,  0.1796,  0.1482,  0.1953,\n",
       "          0.1559,  0.1203,  0.1586,  0.1622,  0.1788,  0.1446,  0.1453,  0.0970,\n",
       "          0.1787,  0.1716,  0.1399,  0.1414,  0.1295,  0.1620,  0.1288,  0.1098,\n",
       "          0.1261,  0.2035,  0.1168,  0.1574,  0.1325,  0.1336,  0.1254,  0.1403,\n",
       "          0.2263,  0.1369,  0.1322,  0.1334,  0.2304,  0.1548,  0.1274,  0.1339,\n",
       "          0.1491,  0.2052,  0.1221,  0.1190,  0.1880,  0.1118,  0.1680,  0.1548,\n",
       "          0.1491,  0.1573,  0.1679,  0.1703,  0.1163,  0.3959,  0.1038,  0.1497,\n",
       "          0.1652,  0.1155,  0.1143,  0.0848,  0.1139,  0.1527,  0.1286,  0.1352,\n",
       "          0.1319,  0.1371,  0.1485,  0.1577,  0.1858,  0.1024,  0.2129,  0.0956,\n",
       "          0.1327,  0.1518,  0.1120,  0.1236,  0.1271,  0.1484,  0.1417,  0.1767,\n",
       "          0.1650,  0.1412, -0.0304,  0.1433,  0.2225,  0.1300,  0.1521,  0.1438,\n",
       "          0.1382, -0.1219,  0.1379,  0.1488,  0.2423,  0.1043,  0.1295,  0.1199,\n",
       "          0.1369,  0.1966,  0.1401,  0.1112,  0.1979,  0.1227,  0.2660,  0.1745],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0574, -0.3530, -0.1251,  ..., -0.4632,  0.0994, -0.5526],\n",
       "         [ 0.2472, -0.3494, -0.5175,  ...,  0.3277, -0.2968,  1.4243],\n",
       "         [ 0.1092, -1.1694, -0.0398,  ..., -0.3378, -0.6762,  1.8534],\n",
       "         ...,\n",
       "         [-0.8677,  0.5310,  0.3582,  ...,  0.0317, -0.5988,  0.8610],\n",
       "         [ 0.5898,  0.1624, -0.6137,  ..., -0.9941, -1.2106, -0.3031],\n",
       "         [ 0.9331, -0.0368,  0.4407,  ...,  0.3231,  0.1828, -0.2459]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.2439, -0.2498, -0.1237,  ...,  0.0414, -0.6675,  0.3585],\n",
       "         [ 0.0098, -0.0964, -0.1312,  ...,  0.6367, -0.4795, -0.1244],\n",
       "         [-0.3528, -0.0797, -0.4385,  ...,  0.6037,  0.3336, -0.1677],\n",
       "         ...,\n",
       "         [ 0.3299,  0.1997,  0.3709,  ..., -0.0153, -0.0202,  0.1473],\n",
       "         [ 5.2841,  1.9147, -0.7786,  ..., -1.2499,  3.1106, -0.2661],\n",
       "         [ 0.1947,  0.0506, -0.1479,  ..., -0.1032, -0.2225, -0.2557]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 3.3239,  4.7530,  2.9175,  2.9506,  0.5167,  2.3740,  3.9021,  3.8431,\n",
       "          5.3686,  2.2169,  2.7069,  4.9337,  2.7714,  2.3222,  4.2532,  2.5741,\n",
       "          2.4881,  3.0939,  2.7173,  2.8856,  2.5365,  2.1892,  2.8561,  2.1185,\n",
       "          2.6505,  3.0549,  2.2810,  4.5325,  2.4906,  3.6016,  2.4681,  3.1262,\n",
       "          3.1725,  2.7162,  4.8128,  3.4281,  2.3457,  2.4412,  3.0831,  3.0008,\n",
       "          2.3465,  2.3812,  4.6069,  2.8251,  2.6098,  2.5190,  1.7938,  2.7599,\n",
       "          2.9527,  2.2256,  3.2396,  2.6943,  3.4054,  7.5079,  3.7213,  2.4720,\n",
       "          2.6979,  2.8872,  2.3088,  3.8069,  2.2847,  3.3647,  3.0072,  2.4672,\n",
       "          3.5303,  2.8332,  2.8300,  2.3129,  5.0042,  3.2825,  3.0109,  3.0532,\n",
       "          2.5472,  2.3603,  2.4522,  2.7328,  2.4700,  2.6394,  2.9899,  3.9083,\n",
       "          3.7359,  2.6816,  2.4759,  2.9051,  2.6036,  3.0544,  2.5802,  3.4897,\n",
       "          2.4233,  3.6621,  2.4615,  2.9121,  2.9401,  2.4427,  3.4655,  2.4222,\n",
       "          2.8329,  2.8225,  9.1851,  2.8031,  2.4713,  2.3430,  2.9774,  3.5645,\n",
       "          2.5551,  2.5343,  2.4888,  2.7296,  3.2532,  3.2232,  2.9829,  2.7764,\n",
       "          2.4361,  2.5945,  2.3458,  3.0472,  2.7638,  2.4940,  2.3923,  2.4056,\n",
       "          2.6694,  2.6913,  2.5220,  2.9531,  2.4489, 12.1675,  3.5666,  2.3190,\n",
       "          3.4988,  2.6832,  2.3248,  2.4418,  2.9401,  3.4967,  3.6772,  2.3787,\n",
       "          4.2491,  7.0816,  2.4377,  2.3559,  2.5002,  3.6993,  2.7932,  2.8840,\n",
       "          2.4078,  2.8630,  2.9403,  3.5572,  3.0385,  0.5660,  2.2254,  2.4941,\n",
       "          3.8008,  2.4377,  2.5707,  2.4942,  2.2707,  3.7887,  2.8254,  9.8128,\n",
       "          2.3442,  3.4166,  2.4687,  2.3090,  3.1394,  3.0874,  3.6044,  3.2009,\n",
       "          2.3324,  2.2937,  2.9656,  2.2038,  2.8381,  2.3772,  2.5948,  2.8508,\n",
       "          2.9040,  3.6664,  2.5165,  2.5380,  2.5047,  2.4658,  1.4741,  2.9242,\n",
       "          4.4093,  2.4489,  2.6260,  2.8856,  2.4623,  2.7573,  5.1259,  4.1041,\n",
       "          2.6489,  3.8755,  2.3024,  2.5552,  2.5028,  2.7536,  2.4315,  2.2374,\n",
       "          2.9654,  3.1327,  3.2675,  3.0302,  2.4820,  2.5997,  1.2053,  2.8078,\n",
       "          2.6868,  2.5090,  2.6431,  2.6496,  3.1568, -1.0667,  2.3500,  2.2377,\n",
       "          2.6260,  3.1389,  3.6117,  2.4219,  2.5082,  2.4570,  2.7915,  2.5638,\n",
       "          4.4722,  2.5574,  5.4996,  2.3427,  2.6666,  2.7157,  2.5839,  2.6391,\n",
       "          6.5120,  2.9492,  4.0060,  3.1878,  3.0039,  4.0309,  2.7646,  3.5196,\n",
       "          3.7459,  3.9539,  3.0123,  2.6491,  6.2893,  2.3527,  3.3804,  2.9335,\n",
       "          6.5917,  2.8616,  2.5937,  2.7731,  2.8577,  2.4033,  2.3561,  3.0278,\n",
       "          2.5250,  3.3307,  4.1339,  3.5173,  3.1633,  2.3375,  2.3731,  4.3522,\n",
       "          3.2249,  2.3870,  2.4458,  2.5052,  3.3087,  2.8861,  2.6206,  2.6903,\n",
       "          2.8164,  5.2177,  2.1622,  9.4470,  2.4128,  2.4647,  3.7953,  2.8095,\n",
       "          4.7797,  2.6849,  2.5192,  2.6063,  2.5527,  2.7198,  2.6310,  2.5360,\n",
       "          2.4318,  3.2315,  3.5341,  2.4483,  2.7643,  2.4603,  7.2054,  4.0835,\n",
       "          2.5540,  2.4594,  2.3882,  6.3762,  2.7077,  3.0186,  0.4853,  2.6451,\n",
       "          4.5090,  2.2566,  2.6659,  4.2971,  3.7743,  3.4781,  2.7130,  2.7398,\n",
       "          7.8189,  2.2388,  3.0426,  2.3363,  2.5443,  2.3648,  4.6808,  3.1248,\n",
       "          2.5172,  2.3718,  2.5427,  5.7042,  9.9270,  2.2080,  3.1094,  3.1425,\n",
       "          2.4302,  3.8536,  3.3427,  2.6576,  2.6344,  2.4529,  2.3739,  3.1063,\n",
       "          3.3818,  2.2336,  2.4513,  3.6553,  4.0419,  2.9120,  2.4478,  3.6344,\n",
       "          2.4595,  3.4184,  2.8448,  2.4406,  2.4703,  2.7853,  2.9377,  2.3688,\n",
       "          2.5310,  2.3486,  2.6310,  6.0099,  2.6511,  2.4510,  2.4683,  2.7346,\n",
       "          3.1351,  2.3925,  2.3893,  4.2129,  2.7432,  3.3839,  2.1766,  0.6900,\n",
       "          2.8256,  2.3122,  2.4385,  3.1965,  3.0098,  2.4634,  6.1575,  2.4755,\n",
       "          2.6499,  2.4887,  2.5925,  4.4776,  2.6163,  6.5644,  2.8961,  2.4876,\n",
       "          3.3708,  2.5101,  5.8883,  2.7121,  4.1939,  2.3853,  2.3197,  2.2745,\n",
       "          4.1588,  2.5376,  2.7419,  2.8118,  3.0856,  4.3034,  2.4485,  2.6635,\n",
       "          3.2669,  3.5467,  3.5300,  0.5852,  2.1250,  3.5331,  2.5222,  2.6214,\n",
       "          2.3907,  2.5809,  3.8883,  3.8222,  3.3114,  4.3199,  2.4730,  3.1104,\n",
       "          2.8940,  4.7440,  2.3641,  2.2655,  2.5626,  2.6232,  2.9684,  2.8391,\n",
       "          3.7680,  2.3687,  2.4161,  2.2407,  2.2540,  2.5554,  2.4756,  2.3435,\n",
       "          2.5588,  3.7872,  2.2744,  2.6974,  2.9071,  2.3849,  3.0338,  2.8611,\n",
       "          2.3340,  2.1887,  2.2247,  2.7480,  2.6936,  2.4287,  2.4220,  3.1484,\n",
       "          2.4511,  2.5173,  2.4718,  2.3214,  4.2852,  3.4955,  2.6828,  2.5904,\n",
       "          2.5129,  2.5320,  2.9875,  2.6259,  2.2452, 10.6968,  2.5266,  2.5235,\n",
       "          2.6121,  2.3635,  2.3729,  1.6735,  3.2418,  2.5227,  2.2994,  2.4424,\n",
       "          2.7895,  2.2879,  2.4555,  9.0940,  2.6098,  5.0090,  2.5775,  2.0005,\n",
       "          3.0486,  2.5963,  2.4029,  2.4834,  2.3391,  2.5296,  2.6205,  2.9341,\n",
       "          2.8600,  2.5355,  0.5340,  2.3644,  3.0427,  2.2060,  2.6825,  3.1154,\n",
       "          2.5019,  4.8907,  3.0390,  3.2487,  3.4783,  5.8447,  2.7864,  2.2543,\n",
       "          2.5617,  3.0011,  3.1183,  2.7870,  2.5428,  2.7670,  7.5327,  5.4863],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.7669e-01,  3.1569e-01,  1.5851e-01,  1.3945e-01, -1.1896e-03,\n",
       "          1.2375e-01,  2.6496e-01,  2.6096e-01,  6.7586e-01,  1.0710e-01,\n",
       "          1.3908e-01,  3.6746e-01,  1.2955e-01,  1.0825e-01,  1.8656e+00,\n",
       "          1.3614e-01,  1.2036e-01,  2.0247e-01,  1.5323e-01,  1.4658e-01,\n",
       "          1.4754e-01,  9.8654e-02,  1.7681e-01,  7.1238e-01,  1.1900e-01,\n",
       "          1.9566e-01,  1.2726e-01,  6.1056e-01,  1.1509e-01,  2.2295e-01,\n",
       "          1.3008e-01,  2.6659e-01,  1.7230e-01,  1.2620e-01,  4.3300e-01,\n",
       "          2.4866e-01,  1.3588e-01,  1.2479e-01,  1.3519e-01,  1.7984e-01,\n",
       "          1.2980e-01,  1.1112e-01,  3.9435e-01,  1.5743e-01,  1.6659e-01,\n",
       "          1.3434e-01,  3.0724e-01,  1.7729e-01,  1.6042e-01,  1.2445e-01,\n",
       "          1.5695e-01,  1.6848e-01,  2.5557e-01,  4.9402e-01,  2.9853e-01,\n",
       "          1.4160e-01,  1.5070e-01,  1.7100e-01,  1.1371e-01,  2.5740e-01,\n",
       "          1.0917e-01,  2.9968e-01,  1.9934e-01,  1.3113e-01,  1.2515e-01,\n",
       "          1.7921e-01,  1.4381e-01,  1.1156e-01,  9.2785e-01,  2.1932e-01,\n",
       "          1.7154e-01,  1.5185e-01,  1.4333e-01,  1.2572e-01,  1.3751e-01,\n",
       "          1.4353e-01,  1.3306e-01,  1.5518e-01,  2.1693e-01,  2.6120e-01,\n",
       "          2.7520e-01,  1.4240e-01,  1.5346e-01,  1.7442e-01,  1.2060e-01,\n",
       "          2.1769e-01,  1.2767e-01,  1.9945e-01,  1.2654e-01,  3.7560e-01,\n",
       "          1.3163e-01,  1.9038e-01,  1.6629e-01,  1.4120e-01,  1.9816e-01,\n",
       "          1.2034e-01,  2.2829e-01,  1.3297e-01,  2.5555e-02,  1.2920e-01,\n",
       "          1.2247e-01,  1.1188e-01,  2.3178e-01,  5.6918e-01,  1.2584e-01,\n",
       "          1.3525e-01,  1.2034e-01,  1.3558e-01,  1.5314e-01,  1.7487e-01,\n",
       "          1.5446e-01,  1.9486e-01,  1.1577e-01,  1.1803e-01,  1.0913e-01,\n",
       "          2.0005e-01,  1.1842e-01,  1.3095e-01,  1.3894e-01,  1.0409e-01,\n",
       "          1.3664e-01,  1.4672e-01,  1.2266e-01,  1.2406e-01,  1.1618e-01,\n",
       "          6.6038e-01,  1.9070e-01,  1.2206e-01,  1.2779e-01,  1.4225e-01,\n",
       "          1.2577e-01,  1.0800e-01,  1.9290e-01,  2.6078e-01,  2.3882e-01,\n",
       "          1.2349e-01,  2.8724e-01,  3.8703e-01,  1.0867e-01,  1.1841e-01,\n",
       "          1.3595e-01,  4.5416e-01,  1.4524e-01,  1.3668e-01,  1.3060e-01,\n",
       "          1.8470e-01,  1.9113e-01,  2.2969e-01,  1.5366e-01,  8.8663e-01,\n",
       "          1.1399e-01,  1.2137e-01,  3.0361e-01,  1.0493e-01,  1.5259e-01,\n",
       "          1.2225e-01,  1.1048e-01,  2.3966e-01,  1.8536e-01,  9.5539e+00,\n",
       "          1.1223e-01,  2.1324e-01,  1.2885e-01,  1.3823e-01,  1.9867e-01,\n",
       "          1.5247e-01,  3.3047e-01,  2.4393e-01,  1.1846e-01,  1.1159e-01,\n",
       "          2.5466e-01,  1.1032e-01,  1.6060e-01,  1.1237e-01,  1.1485e-01,\n",
       "          1.6701e-01,  1.4611e-01,  3.5747e-01,  1.2626e-01,  1.2912e-01,\n",
       "          1.2003e-01,  1.2516e-01,  3.0555e-03,  1.7673e-01,  5.7460e-01,\n",
       "          1.3738e-01,  1.3218e-01,  1.5023e-01,  1.3607e-01,  1.7762e-01,\n",
       "          4.3953e+00,  3.8840e-01,  1.3300e-01,  3.0694e-01,  1.4609e-01,\n",
       "          1.2331e-01,  1.1599e-01,  1.8813e-01,  1.2713e-01,  1.2646e-01,\n",
       "          1.8851e-01,  2.4172e-01,  1.9391e-01,  1.8609e-01,  1.2393e-01,\n",
       "          1.5281e-01,  6.3194e-02,  2.1338e-01,  1.2912e-01,  1.1695e-01,\n",
       "          1.2684e-01,  1.2660e-01,  2.0792e-01,  4.0938e-02,  1.1683e-01,\n",
       "          1.1105e-01,  1.5619e-01,  1.9240e-01,  3.1713e-01,  1.2357e-01,\n",
       "          1.2471e-01,  1.2329e-01,  2.1922e-01,  1.3051e-01,  4.2400e-01,\n",
       "          1.2632e-01,  2.8054e-01,  1.1220e-01,  1.7938e-01,  1.7574e-01,\n",
       "          1.3729e-01,  1.4326e-01,  3.9974e-01,  1.6896e-01,  3.9021e-01,\n",
       "          1.6281e-01,  1.5416e-01,  3.0459e-01,  1.3405e-01,  2.7158e-01,\n",
       "          3.3305e-01,  3.0065e-01,  1.7738e-01,  1.2675e-01,  1.0607e+00,\n",
       "          4.6354e-01,  2.1645e-01,  1.6814e-01,  2.5263e-01,  1.9882e-01,\n",
       "          1.1764e-01,  1.2411e-01,  1.4772e-01,  1.2268e-01,  1.1638e-01,\n",
       "          1.6822e-01,  1.2764e-01,  2.4943e-01,  2.6309e-01,  2.0334e-01,\n",
       "          1.7798e-01,  1.0987e-01,  1.1723e-01,  1.2949e+00,  1.5775e-01,\n",
       "          1.2035e-01,  1.5183e-01,  1.1433e-01,  1.2039e-01,  1.9369e-01,\n",
       "          1.3896e-01,  1.5797e-01,  1.9250e-01,  1.4096e+00,  1.0120e-01,\n",
       "          7.1609e-01,  1.2173e-01,  1.1885e-01,  2.3540e-01,  2.1990e-01,\n",
       "          5.2832e-01,  1.8659e-01,  1.2035e-01,  1.8140e-01,  1.2608e-01,\n",
       "          1.5225e-01,  1.7418e-01,  1.6582e-01,  1.2084e-01,  2.0357e-01,\n",
       "          5.3457e-01,  1.2939e-01,  1.3603e-01,  1.3277e-01,  1.4290e+00,\n",
       "          3.5953e-01,  1.3739e-01,  1.2281e-01,  1.1085e-01,  1.0123e+00,\n",
       "          1.5581e-01,  2.0024e-01,  1.0840e-02,  1.2280e-01,  4.2177e-01,\n",
       "          1.1262e-01,  1.6653e-01,  4.7453e-01,  7.0726e-01,  2.1575e-01,\n",
       "          1.2694e-01,  1.7432e-01,  8.9152e-01,  1.3576e-01,  1.6738e-01,\n",
       "          1.1607e-01,  1.2664e-01,  1.1347e-01,  3.4283e-01,  2.1936e-01,\n",
       "          1.4251e-01,  1.1954e-01,  1.1632e-01,  4.4251e-01,  4.7672e+00,\n",
       "          1.0415e-01,  1.8929e-01,  1.0258e-01,  1.2120e-01,  4.3891e-01,\n",
       "          2.6794e-01,  1.5512e-01,  2.2858e-01,  1.0941e-01,  1.2325e-01,\n",
       "          2.0318e-01,  1.8892e-01,  1.2848e-01,  1.2436e-01,  2.0258e-01,\n",
       "          2.9767e-01,  1.8475e-01,  1.3621e-01,  3.7397e-01,  1.2872e-01,\n",
       "          2.7165e-01,  1.8115e-01,  1.2872e-01,  1.4066e-01,  1.3422e-01,\n",
       "          1.7403e-01,  1.3885e-01,  1.3135e-01,  1.1406e-01,  1.5181e-01,\n",
       "          3.0187e-01,  1.6965e-01,  1.2185e-01,  1.5636e-01,  1.5726e-01,\n",
       "          1.4443e-01,  1.0973e-01,  1.1683e-01,  1.9739e-01,  1.4848e-01,\n",
       "          2.0384e-01,  1.1049e-01,  5.1745e-02,  1.5581e-01,  1.2250e-01,\n",
       "          1.2644e-01,  1.6869e-01,  1.3104e-01,  1.2786e-01,  4.4260e-01,\n",
       "          1.1773e-01,  1.2056e-01,  1.4177e-01,  1.1166e-01,  3.6691e-01,\n",
       "          1.1507e-01,  1.1276e+00,  1.9049e-01,  1.2761e-01,  2.1755e-01,\n",
       "          1.2054e-01,  3.9245e-01,  1.3859e-01,  3.6192e-01,  1.1713e-01,\n",
       "          1.2237e-01,  1.0842e-01,  2.2176e-01,  1.3976e-01,  1.5152e-01,\n",
       "          1.6673e-01,  1.9992e-01,  2.0333e-01,  1.3445e-01,  1.6155e-01,\n",
       "          2.7845e-01,  4.0024e-01,  1.9509e-01,  2.0803e-02,  1.0168e-01,\n",
       "          1.7851e-01,  1.2012e-01,  1.2832e-01,  1.2368e-01,  1.2281e-01,\n",
       "          2.5017e-01,  3.4902e-01,  2.5196e-01,  3.7938e-01,  1.2483e-01,\n",
       "          1.6371e-01,  1.7889e-01,  4.6451e-01,  1.2815e-01,  1.0569e-01,\n",
       "          1.5513e-01,  1.3193e-01,  1.4546e-01,  1.4634e-01,  2.1221e-01,\n",
       "          1.2269e-01,  1.3209e-01,  1.3009e-01,  1.1306e-01,  1.3483e-01,\n",
       "          1.1912e-01,  1.2499e-01,  1.1382e-01,  2.5669e-01,  1.1326e-01,\n",
       "          1.4758e-01,  1.6963e-01,  1.1584e-01,  1.9148e-01,  1.8985e-01,\n",
       "          1.2671e-01,  1.1540e-01,  9.3833e-02,  1.8283e-01,  1.3283e-01,\n",
       "          1.2751e-01,  1.1122e-01,  1.9747e-01,  1.1650e-01,  1.2190e-01,\n",
       "          1.1282e-01,  1.0937e-01,  4.1441e-01,  2.6152e-01,  1.5891e-01,\n",
       "          1.1826e-01,  1.0840e-01,  1.3116e-01,  1.6830e-01,  1.1906e-01,\n",
       "          1.0435e-01,  7.0746e-01,  1.2819e-01,  1.1875e-01,  1.3401e-01,\n",
       "          9.8375e-02,  1.0133e-01,  1.1564e-01,  3.4167e-01,  1.2239e-01,\n",
       "          1.2218e-01,  1.0667e-01,  1.5428e-01,  1.2153e-01,  1.2975e-01,\n",
       "          5.5875e-01,  1.1022e-01,  6.0293e-01,  1.2107e-01,  1.0041e-01,\n",
       "          2.6823e-01,  1.2840e-01,  1.2393e-01,  1.0615e-01,  1.1952e-01,\n",
       "          1.1000e-01,  1.4635e-01,  1.8354e-01,  1.6082e-01,  1.4056e-01,\n",
       "          1.7342e-03,  1.1145e-01,  1.4938e-01,  1.1448e-01,  1.1725e-01,\n",
       "          2.0656e-01,  1.2242e-01,  2.3177e-01,  1.9990e-01,  2.0634e-01,\n",
       "          1.3984e-01,  9.9994e-01,  1.7912e-01,  1.2369e-01,  1.4078e-01,\n",
       "          2.0234e-01,  2.5300e-01,  1.4696e-01,  1.5247e-01,  1.3817e-01,\n",
       "          1.6715e-03,  4.4172e-01], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen = [\"shared.weight\"]\n",
    "\n",
    "param_influence = []\n",
    "for n, p in param_optimizer:\n",
    "    if not any(fr in n for fr in frozen):\n",
    "        param_influence.append(p)\n",
    "    elif \"shared.weight\" in n:\n",
    "        pass  # need gradients through embedding layer for computing saliency map\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "param_influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameter size = 44057088\n"
     ]
    }
   ],
   "source": [
    "param_shape_tensor = []\n",
    "param_size = 0\n",
    "for p in param_influence:\n",
    "    tmp_p = p.clone().detach()\n",
    "    param_shape_tensor.append(tmp_p)\n",
    "    param_size += torch.numel(tmp_p)\n",
    "\n",
    "print(\"  Parameter size = %d\" % param_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(influence_fn_examples[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'bert_util' from '/home/psz/workspace/AI/My-project/in-fn-for-generation/bert_util.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bert_util\n",
    "import importlib\n",
    "importlib.reload(bert_util) # reload when changes to bert_util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================test example: 0======================================\n",
      " loss: tensor(2.2270e-05, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "len of traindataset 30000\n",
      "######## START COMPUTING IHVP ########\n",
      "Recursion at depth 0: norm is 0.000041\n",
      "Recursion at depth 200: norm is 0.003136\n",
      "Recursion at depth 400: norm is 0.004833\n",
      "Recursion at depth 600: norm is 0.005764\n",
      "Recursion at depth 800: norm is 0.006274\n",
      "Recursion at depth 1000: norm is 0.006554\n",
      "Recursion at depth 1200: norm is 0.006707\n",
      "Recursion at depth 1400: norm is 0.006791\n",
      "Recursion at depth 1600: norm is 0.006837\n",
      "Recursion at depth 1800: norm is 0.006863\n",
      "Recursion at depth 2000: norm is 0.006877\n",
      "Recursion at depth 2200: norm is 0.006884\n",
      "Recursion at depth 2400: norm is 0.006888\n",
      "Recursion at depth 2600: norm is 0.006891\n",
      "Recursion at depth 2800: norm is 0.006892\n",
      "Recursion at depth 3000: norm is 0.006893\n",
      "Recursion at depth 3200: norm is 0.006893\n",
      "Recursion at depth 3400: norm is 0.006893\n",
      "Recursion at depth 3600: norm is 0.006893\n",
      "Recursion at depth 3800: norm is 0.006893\n",
      "Recursion at depth 4000: norm is 0.006893\n",
      "Recursion at depth 4200: norm is 0.006893\n",
      "Recursion at depth 4400: norm is 0.006893\n",
      "Recursion at depth 4600: norm is 0.006893\n",
      "Recursion at depth 4800: norm is 0.006893\n",
      "Recursion at depth 5000: norm is 0.006893\n",
      "Recursion at depth 5200: norm is 0.006893\n",
      "Recursion at depth 5400: norm is 0.006893\n",
      "Recursion at depth 5600: norm is 0.006893\n",
      "Recursion at depth 5800: norm is 0.006893\n",
      "Recursion at depth 6000: norm is 0.006893\n",
      "Recursion at depth 6200: norm is 0.006893\n",
      "Recursion at depth 6400: norm is 0.006893\n",
      "Recursion at depth 6600: norm is 0.006893\n",
      "Recursion at depth 6800: norm is 0.006893\n",
      "Recursion at depth 7000: norm is 0.006893\n",
      "Recursion at depth 7200: norm is 0.006893\n",
      "Recursion at depth 7400: norm is 0.006893\n",
      "Recursion at depth 7499: norm is 0.006893\n",
      "######## FINISHED COMPUTING IHVP ########\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch.autograd as autograd\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_ids = [\n",
    "    torch.tensor(example[\"input_ids\"], dtype=torch.long).unsqueeze(0)\n",
    "    for example in influence_fn_examples\n",
    "]\n",
    "label_ids = [\n",
    "    torch.tensor(example[\"labels\"], dtype=torch.long).unsqueeze(0)\n",
    "    for example in influence_fn_examples\n",
    "]\n",
    "attn_mask = [\n",
    "    torch.tensor(example[\"attention_mask\"], dtype=torch.long).unsqueeze(0)\n",
    "    for example in influence_fn_examples\n",
    "]\n",
    "\n",
    "# for each test sample, we should compute the influence socre on train dataset\n",
    "for idx, (input_id, label_id, attention_mask) in enumerate(\n",
    "    zip(input_ids, label_ids, attn_mask)\n",
    "):\n",
    "    print(\n",
    "        f\"====================test example: {idx}======================================\"\n",
    "    )\n",
    "    input_id = input_id.to(model.device)\n",
    "    label_id = label_id.to(model.device)\n",
    "    attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    \n",
    "    # get test example grad\n",
    "    model.zero_grad()\n",
    "    output = model(input_id, attention_mask=attention_mask, labels=label_id)\n",
    "    # test_loss = output.loss \n",
    "    # *scaled the loss to avoid the grad is too large\n",
    "    scaled_loss = output.loss * 1e-5\n",
    "    print(\" loss:\", scaled_loss)\n",
    "    \n",
    "    # test_grads = autograd.grad(test_loss, param_influence)\n",
    "    test_grads = autograd.grad(scaled_loss, param_influence)\n",
    "    # reload train dataset\n",
    "    train_dataloader_lissa = tokenized_datasets[\"train\"]\n",
    "    print(\"len of traindataset\", len(train_dataloader_lissa))\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    ######## IHVP ########\n",
    "    model.train()\n",
    "    print(\"######## START COMPUTING IHVP ########\")\n",
    "    inverse_hvp = bert_util.get_inverse_hvp_lissa(\n",
    "        test_grads,\n",
    "        model,\n",
    "        device,\n",
    "        param_influence,\n",
    "        train_dataloader_lissa,\n",
    "        damping=args.damping,\n",
    "        num_samples=args.lissa_repeat,\n",
    "        recursion_depth=int(len(train_dataloader_lissa) * args.lissa_depth),\n",
    "    )\n",
    "    print(\"######## FINISHED COMPUTING IHVP ########\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
